// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %cheri_cc1 -emit-llvm -O1 %s -o -  | FileCheck %s -check-prefix HYBRID
// RUN: %cheri_purecap_cc1 -emit-llvm -O1 %s -o -  | FileCheck %s -check-prefix PURECAP

template <class Tp>
__attribute__((always_inline)) void DoNotOptimize(Tp& value) {
#ifdef __CHERI_PURE_CAPABILITY__
  asm volatile("clb $zero, $zero, %0" : "+r,m"(value) : : "memory");
#else
  asm volatile("lb $zero, %0" : "+r,m"(value) : : "memory");
#endif
// asm volatile("" : "+m,r"(value) : : "memory");
}

char DummyData4[4];
char DummyData8[8];
char DummyData16[16];
char DummyData32[32];
char DummyData64[64];

// HYBRID-LABEL: @_Z4testv(
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load i32, i32* bitcast ([4 x i8]* @DummyData4 to i32*), align 1
// HYBRID-NEXT:    tail call void asm sideeffect "lb $$zero, $0", "=*r|m,0,~{memory},~{$1}"([4 x i8]* nonnull @DummyData4, i32 [[TMP0]]) #1, !srcloc !2
// HYBRID-NEXT:    [[TMP1:%.*]] = load i64, i64* bitcast ([8 x i8]* @DummyData8 to i64*), align 1
// HYBRID-NEXT:    tail call void asm sideeffect "lb $$zero, $0", "=*r|m,0,~{memory},~{$1}"([8 x i8]* nonnull @DummyData8, i64 [[TMP1]]) #1, !srcloc !2
// HYBRID-NEXT:    tail call void asm sideeffect "lb $$zero, $0", "=*r|m,*0,~{memory},~{$1}"([16 x i8]* nonnull @DummyData16, [16 x i8]* nonnull @DummyData16) #1, !srcloc !2
// HYBRID-NEXT:    tail call void asm sideeffect "lb $$zero, $0", "=*r|m,*0,~{memory},~{$1}"([32 x i8]* nonnull @DummyData32, [32 x i8]* nonnull @DummyData32) #1, !srcloc !2
// HYBRID-NEXT:    tail call void asm sideeffect "lb $$zero, $0", "=*r|m,*0,~{memory},~{$1}"([64 x i8]* nonnull @DummyData64, [64 x i8]* nonnull @DummyData64) #1, !srcloc !2
// HYBRID-NEXT:    [[TMP2:%.*]] = load i8, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @DummyData4, i64 0, i64 0), align 1, !tbaa !3
// HYBRID-NEXT:    ret i8 [[TMP2]]
//
// PURECAP-LABEL: @_Z4testv(
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load i32, i32 addrspace(200)* bitcast ([4 x i8] addrspace(200)* @DummyData4 to i32 addrspace(200)*), align 1
// PURECAP-NEXT:    tail call void asm sideeffect "clb $$zero, $$zero, $0", "=*r|m,0,~{memory},~{$1}"([4 x i8] addrspace(200)* nonnull @DummyData4, i32 [[TMP0]]) #1, !srcloc !2
// PURECAP-NEXT:    [[TMP1:%.*]] = load i64, i64 addrspace(200)* bitcast ([8 x i8] addrspace(200)* @DummyData8 to i64 addrspace(200)*), align 1
// PURECAP-NEXT:    tail call void asm sideeffect "clb $$zero, $$zero, $0", "=*r|m,0,~{memory},~{$1}"([8 x i8] addrspace(200)* nonnull @DummyData8, i64 [[TMP1]]) #1, !srcloc !2
// PURECAP-NEXT:    tail call void asm sideeffect "clb $$zero, $$zero, $0", "=*r|m,*0,~{memory},~{$1}"([16 x i8] addrspace(200)* nonnull @DummyData16, [16 x i8] addrspace(200)* nonnull @DummyData16) #1, !srcloc !2
// PURECAP-NEXT:    tail call void asm sideeffect "clb $$zero, $$zero, $0", "=*r|m,*0,~{memory},~{$1}"([32 x i8] addrspace(200)* nonnull @DummyData32, [32 x i8] addrspace(200)* nonnull @DummyData32) #1, !srcloc !2
// PURECAP-NEXT:    tail call void asm sideeffect "clb $$zero, $$zero, $0", "=*r|m,*0,~{memory},~{$1}"([64 x i8] addrspace(200)* nonnull @DummyData64, [64 x i8] addrspace(200)* nonnull @DummyData64) #1, !srcloc !2
// PURECAP-NEXT:    [[TMP2:%.*]] = load i8, i8 addrspace(200)* getelementptr inbounds ([4 x i8], [4 x i8] addrspace(200)* @DummyData4, i64 0, i64 0), align 1, !tbaa !3
// PURECAP-NEXT:    ret i8 [[TMP2]]
//
char test() {
  DoNotOptimize(DummyData4);
  DoNotOptimize(DummyData8);
  DoNotOptimize(DummyData16);
  DoNotOptimize(DummyData32);
  DoNotOptimize(DummyData64);
  return DummyData4[0];
}

// HYBRID-LABEL: @_Z8test_ptrPv(
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[P_ADDR:%.*]] = alloca i8*, align 8
// HYBRID-NEXT:    store i8* [[P:%.*]], i8** [[P_ADDR]], align 8, !tbaa !6
// HYBRID-NEXT:    call void asm sideeffect "lb $$zero, $0", "=*r|m,0,~{memory},~{$1}"(i8** nonnull [[P_ADDR]], i8* [[P]]) #1, !srcloc !2
// HYBRID-NEXT:    [[TMP0:%.*]] = load i8*, i8** [[P_ADDR]], align 8, !tbaa !6
// HYBRID-NEXT:    ret i8* [[TMP0]]
//
//
// PURECAP-LABEL: @_Z8test_ptrU3capPv(
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[P_ADDR:%.*]] = alloca i8 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    store i8 addrspace(200)* [[P:%.*]], i8 addrspace(200)* addrspace(200)* [[P_ADDR]], align 16, !tbaa !6
// PURECAP-NEXT:    call void asm sideeffect "clb $$zero, $$zero, $0", "=*r|m,0,~{memory},~{$1}"(i8 addrspace(200)* addrspace(200)* nonnull [[P_ADDR]], i8 addrspace(200)* [[P]]) #1, !srcloc !2
// PURECAP-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[P_ADDR]], align 16, !tbaa !6
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
void* test_ptr(void* p) {
  DoNotOptimize(p);
  return p;
}
