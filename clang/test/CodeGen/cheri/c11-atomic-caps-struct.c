// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature
/// We should be able to use atomic instructions instead of the generic libcall
// RUN: %riscv64_cheri_cc1 -target-feature +a -std=c11 -o - -emit-llvm -disable-O0-optnone %s -verify \
// RUN:   | opt -S --passes=mem2reg | FileCheck --check-prefix=HYBRID %s
// RUN: %riscv64_cheri_purecap_cc1 -target-feature +a -std=c11 -o - -emit-llvm -disable-O0-optnone %s -verify \
// RUN:   | opt -S --passes=mem2reg | FileCheck --check-prefix=PURECAP %s

typedef struct capstruct {
  unsigned __intcap value;
} capstruct;

// HYBRID-LABEL: define {{[^@]+}}@test_init
// HYBRID-SAME: (ptr noundef [[F:%.*]], ptr addrspace(200) [[VALUE_COERCE:%.*]]) #[[ATTR0:[0-9]+]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[VALUE:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16
// HYBRID-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr [[VALUE]], i32 0, i32 0
// HYBRID-NEXT:    store ptr addrspace(200) [[VALUE_COERCE]], ptr [[COERCE_DIVE]], align 16
// HYBRID-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[F]], ptr align 16 [[VALUE]], i64 16, i1 false)
// HYBRID-NEXT:    ret void
//
// PURECAP-LABEL: define {{[^@]+}}@test_init
// PURECAP-SAME: (ptr addrspace(200) noundef [[F:%.*]], ptr addrspace(200) [[VALUE_COERCE:%.*]]) addrspace(200) #[[ATTR0:[0-9]+]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[VALUE:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16, addrspace(200)
// PURECAP-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr addrspace(200) [[VALUE]], i32 0, i32 0
// PURECAP-NEXT:    store ptr addrspace(200) [[VALUE_COERCE]], ptr addrspace(200) [[COERCE_DIVE]], align 16
// PURECAP-NEXT:    call void @llvm.memcpy.p200.p200.i64(ptr addrspace(200) align 16 [[F]], ptr addrspace(200) align 16 [[VALUE]], i64 16, i1 false)
// PURECAP-NEXT:    ret void
//
void test_init(_Atomic(capstruct) *f, capstruct value) {
  __c11_atomic_init(f, value);
}

// HYBRID-LABEL: define {{[^@]+}}@test_load
// HYBRID-SAME: (ptr noundef [[F:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16
// HYBRID-NEXT:    call void @__atomic_load(i64 noundef 16, ptr noundef [[F]], ptr noundef [[ATOMIC_TEMP]], i32 noundef signext 5)
// HYBRID-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[RETVAL]], ptr align 16 [[ATOMIC_TEMP]], i64 16, i1 false)
// HYBRID-NEXT:    [[TMP0:%.*]] = load [[STRUCT_CAPSTRUCT]], ptr [[RETVAL]], align 16
// HYBRID-NEXT:    ret [[STRUCT_CAPSTRUCT]] [[TMP0]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_load
// PURECAP-SAME: (ptr addrspace(200) noundef [[F:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16, addrspace(200)
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16, addrspace(200)
// PURECAP-NEXT:    call void @__atomic_load(i64 noundef 16, ptr addrspace(200) noundef [[F]], ptr addrspace(200) noundef [[ATOMIC_TEMP]], i32 noundef signext 5)
// PURECAP-NEXT:    call void @llvm.memcpy.p200.p200.i64(ptr addrspace(200) align 16 [[RETVAL]], ptr addrspace(200) align 16 [[ATOMIC_TEMP]], i64 16, i1 false)
// PURECAP-NEXT:    [[TMP0:%.*]] = load [[STRUCT_CAPSTRUCT]], ptr addrspace(200) [[RETVAL]], align 16
// PURECAP-NEXT:    ret [[STRUCT_CAPSTRUCT]] [[TMP0]]
//
capstruct test_load(_Atomic(capstruct) *f) {
  return __c11_atomic_load(f, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{large atomic operation may incur significant performance penalty}}
}

// HYBRID-LABEL: define {{[^@]+}}@test_store
// HYBRID-SAME: (ptr noundef [[F:%.*]], ptr addrspace(200) [[VALUE_COERCE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[VALUE:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16
// HYBRID-NEXT:    [[DOTATOMICTMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16
// HYBRID-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr [[VALUE]], i32 0, i32 0
// HYBRID-NEXT:    store ptr addrspace(200) [[VALUE_COERCE]], ptr [[COERCE_DIVE]], align 16
// HYBRID-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[DOTATOMICTMP]], ptr align 16 [[VALUE]], i64 16, i1 false)
// HYBRID-NEXT:    call void @__atomic_store(i64 noundef 16, ptr noundef [[F]], ptr noundef [[DOTATOMICTMP]], i32 noundef signext 5)
// HYBRID-NEXT:    ret void
//
// PURECAP-LABEL: define {{[^@]+}}@test_store
// PURECAP-SAME: (ptr addrspace(200) noundef [[F:%.*]], ptr addrspace(200) [[VALUE_COERCE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[VALUE:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16, addrspace(200)
// PURECAP-NEXT:    [[DOTATOMICTMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16, addrspace(200)
// PURECAP-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr addrspace(200) [[VALUE]], i32 0, i32 0
// PURECAP-NEXT:    store ptr addrspace(200) [[VALUE_COERCE]], ptr addrspace(200) [[COERCE_DIVE]], align 16
// PURECAP-NEXT:    call void @llvm.memcpy.p200.p200.i64(ptr addrspace(200) align 16 [[DOTATOMICTMP]], ptr addrspace(200) align 16 [[VALUE]], i64 16, i1 false)
// PURECAP-NEXT:    call void @__atomic_store(i64 noundef 16, ptr addrspace(200) noundef [[F]], ptr addrspace(200) noundef [[DOTATOMICTMP]], i32 noundef signext 5)
// PURECAP-NEXT:    ret void
//
void test_store(_Atomic(capstruct) *f, capstruct value) {
  __c11_atomic_store(f, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{large atomic operation may incur significant performance penalty}}
}

// HYBRID-LABEL: define {{[^@]+}}@test_xchg
// HYBRID-SAME: (ptr noundef [[F:%.*]], ptr addrspace(200) [[VALUE_COERCE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16
// HYBRID-NEXT:    [[VALUE:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16
// HYBRID-NEXT:    [[DOTATOMICTMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16
// HYBRID-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr [[VALUE]], i32 0, i32 0
// HYBRID-NEXT:    store ptr addrspace(200) [[VALUE_COERCE]], ptr [[COERCE_DIVE]], align 16
// HYBRID-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[DOTATOMICTMP]], ptr align 16 [[VALUE]], i64 16, i1 false)
// HYBRID-NEXT:    call void @__atomic_exchange(i64 noundef 16, ptr noundef [[F]], ptr noundef [[DOTATOMICTMP]], ptr noundef [[ATOMIC_TEMP]], i32 noundef signext 5)
// HYBRID-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[RETVAL]], ptr align 16 [[ATOMIC_TEMP]], i64 16, i1 false)
// HYBRID-NEXT:    [[TMP0:%.*]] = load [[STRUCT_CAPSTRUCT]], ptr [[RETVAL]], align 16
// HYBRID-NEXT:    ret [[STRUCT_CAPSTRUCT]] [[TMP0]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_xchg
// PURECAP-SAME: (ptr addrspace(200) noundef [[F:%.*]], ptr addrspace(200) [[VALUE_COERCE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16, addrspace(200)
// PURECAP-NEXT:    [[VALUE:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16, addrspace(200)
// PURECAP-NEXT:    [[DOTATOMICTMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16, addrspace(200)
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16, addrspace(200)
// PURECAP-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr addrspace(200) [[VALUE]], i32 0, i32 0
// PURECAP-NEXT:    store ptr addrspace(200) [[VALUE_COERCE]], ptr addrspace(200) [[COERCE_DIVE]], align 16
// PURECAP-NEXT:    call void @llvm.memcpy.p200.p200.i64(ptr addrspace(200) align 16 [[DOTATOMICTMP]], ptr addrspace(200) align 16 [[VALUE]], i64 16, i1 false)
// PURECAP-NEXT:    call void @__atomic_exchange(i64 noundef 16, ptr addrspace(200) noundef [[F]], ptr addrspace(200) noundef [[DOTATOMICTMP]], ptr addrspace(200) noundef [[ATOMIC_TEMP]], i32 noundef signext 5)
// PURECAP-NEXT:    call void @llvm.memcpy.p200.p200.i64(ptr addrspace(200) align 16 [[RETVAL]], ptr addrspace(200) align 16 [[ATOMIC_TEMP]], i64 16, i1 false)
// PURECAP-NEXT:    [[TMP0:%.*]] = load [[STRUCT_CAPSTRUCT]], ptr addrspace(200) [[RETVAL]], align 16
// PURECAP-NEXT:    ret [[STRUCT_CAPSTRUCT]] [[TMP0]]
//
capstruct test_xchg(_Atomic(capstruct) *f, capstruct value) {
  return __c11_atomic_exchange(f, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{large atomic operation may incur significant performance penalty}}
}

// HYBRID-LABEL: define {{[^@]+}}@test_cmpxchg_weak
// HYBRID-SAME: (ptr noundef [[F:%.*]], ptr noundef [[EXP:%.*]], ptr addrspace(200) [[NEW_COERCE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[NEW:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16
// HYBRID-NEXT:    [[DOTATOMICTMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16
// HYBRID-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr [[NEW]], i32 0, i32 0
// HYBRID-NEXT:    store ptr addrspace(200) [[NEW_COERCE]], ptr [[COERCE_DIVE]], align 16
// HYBRID-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[DOTATOMICTMP]], ptr align 16 [[NEW]], i64 16, i1 false)
// HYBRID-NEXT:    [[CALL:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 16, ptr noundef [[F]], ptr noundef [[EXP]], ptr noundef [[DOTATOMICTMP]], i32 noundef signext 0, i32 noundef signext 0)
// HYBRID-NEXT:    ret i1 [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_weak
// PURECAP-SAME: (ptr addrspace(200) noundef [[F:%.*]], ptr addrspace(200) noundef [[EXP:%.*]], ptr addrspace(200) [[NEW_COERCE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[NEW:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16, addrspace(200)
// PURECAP-NEXT:    [[DOTATOMICTMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16, addrspace(200)
// PURECAP-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr addrspace(200) [[NEW]], i32 0, i32 0
// PURECAP-NEXT:    store ptr addrspace(200) [[NEW_COERCE]], ptr addrspace(200) [[COERCE_DIVE]], align 16
// PURECAP-NEXT:    call void @llvm.memcpy.p200.p200.i64(ptr addrspace(200) align 16 [[DOTATOMICTMP]], ptr addrspace(200) align 16 [[NEW]], i64 16, i1 false)
// PURECAP-NEXT:    [[CALL:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 16, ptr addrspace(200) noundef [[F]], ptr addrspace(200) noundef [[EXP]], ptr addrspace(200) noundef [[DOTATOMICTMP]], i32 noundef signext 0, i32 noundef signext 0)
// PURECAP-NEXT:    ret i1 [[CALL]]
//
_Bool test_cmpxchg_weak(_Atomic(capstruct) *f, capstruct *exp, capstruct new) {
  return __c11_atomic_compare_exchange_weak(f, exp, new, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
  // expected-warning@-1{{large atomic operation may incur significant performance penalty}}
}

// HYBRID-LABEL: define {{[^@]+}}@test_cmpxchg_strong
// HYBRID-SAME: (ptr noundef [[F:%.*]], ptr noundef [[EXP:%.*]], ptr addrspace(200) [[NEW_COERCE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[NEW:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16
// HYBRID-NEXT:    [[DOTATOMICTMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16
// HYBRID-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr [[NEW]], i32 0, i32 0
// HYBRID-NEXT:    store ptr addrspace(200) [[NEW_COERCE]], ptr [[COERCE_DIVE]], align 16
// HYBRID-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[DOTATOMICTMP]], ptr align 16 [[NEW]], i64 16, i1 false)
// HYBRID-NEXT:    [[CALL:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 16, ptr noundef [[F]], ptr noundef [[EXP]], ptr noundef [[DOTATOMICTMP]], i32 noundef signext 0, i32 noundef signext 0)
// HYBRID-NEXT:    ret i1 [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_strong
// PURECAP-SAME: (ptr addrspace(200) noundef [[F:%.*]], ptr addrspace(200) noundef [[EXP:%.*]], ptr addrspace(200) [[NEW_COERCE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[NEW:%.*]] = alloca [[STRUCT_CAPSTRUCT:%.*]], align 16, addrspace(200)
// PURECAP-NEXT:    [[DOTATOMICTMP:%.*]] = alloca [[STRUCT_CAPSTRUCT]], align 16, addrspace(200)
// PURECAP-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_CAPSTRUCT]], ptr addrspace(200) [[NEW]], i32 0, i32 0
// PURECAP-NEXT:    store ptr addrspace(200) [[NEW_COERCE]], ptr addrspace(200) [[COERCE_DIVE]], align 16
// PURECAP-NEXT:    call void @llvm.memcpy.p200.p200.i64(ptr addrspace(200) align 16 [[DOTATOMICTMP]], ptr addrspace(200) align 16 [[NEW]], i64 16, i1 false)
// PURECAP-NEXT:    [[CALL:%.*]] = call zeroext i1 @__atomic_compare_exchange(i64 noundef 16, ptr addrspace(200) noundef [[F]], ptr addrspace(200) noundef [[EXP]], ptr addrspace(200) noundef [[DOTATOMICTMP]], i32 noundef signext 0, i32 noundef signext 0)
// PURECAP-NEXT:    ret i1 [[CALL]]
//
_Bool test_cmpxchg_strong(_Atomic(capstruct) *f, capstruct *exp, capstruct new) {
  return __c11_atomic_compare_exchange_strong(f, exp, new, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
  // expected-warning@-1{{large atomic operation may incur significant performance penalty}}
}
