// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature
// RUN: %riscv64_cheri_cc1 -std=c11 -o - -emit-llvm -disable-O0-optnone -Wno-atomic-alignment %s \
// RUN:   | opt -S -mem2reg | FileCheck --check-prefix=HYBRID %s
// RUN: %riscv64_cheri_purecap_cc1 -std=c11 -o - -emit-llvm -disable-O0-optnone -Wno-atomic-alignment %s \
// RUN:   | opt -S -mem2reg | FileCheck --check-prefix=PURECAP %s

// HYBRID-LABEL: define {{[^@]+}}@test_load
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]]) #[[ATTR0:[0-9]+]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load atomic i8 addrspace(200)*, i8 addrspace(200)** [[F]] seq_cst, align 16
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_load
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]]) addrspace(200) #[[ATTR0:[0-9]+]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load atomic i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[F]] seq_cst, align 16
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
__uintcap_t test_load(__uintcap_t *f) {
  __uintcap_t ret;
  __atomic_load(f, &ret, __ATOMIC_SEQ_CST);
  return ret;
}

// HYBRID-LABEL: define {{[^@]+}}@test_store
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    store atomic i8 addrspace(200)* [[VALUE]], i8 addrspace(200)** [[F]] seq_cst, align 16
// HYBRID-NEXT:    ret void
//
// PURECAP-LABEL: define {{[^@]+}}@test_store
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    store atomic i8 addrspace(200)* [[VALUE]], i8 addrspace(200)* addrspace(200)* [[F]] seq_cst, align 16
// PURECAP-NEXT:    ret void
//
void test_store(__uintcap_t *f, __uintcap_t value) {
  __atomic_store(f, &value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_xchg
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i8 addrspace(200)** [[F]], i8 addrspace(200)* [[VALUE]] seq_cst, align 16
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_xchg
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[VALUE]] seq_cst, align 16
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
__uintcap_t test_xchg(__uintcap_t *f, __uintcap_t value) {
  __uintcap_t ret;
  __atomic_exchange(f, &value, &ret, __ATOMIC_SEQ_CST);
  return ret;
}

// HYBRID-LABEL: define {{[^@]+}}@test_xchg_long_ptr
// HYBRID-SAME: (i64 addrspace(200)** [[F:%.*]], i64 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i64 addrspace(200)** [[F]], i64 addrspace(200)* [[VALUE]] seq_cst, align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP0]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_xchg_long_ptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[F:%.*]], i64 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i64 addrspace(200)* addrspace(200)* [[F]], i64 addrspace(200)* [[VALUE]] seq_cst, align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP0]]
//
long *__capability test_xchg_long_ptr(long *__capability *f, long *__capability value) {
  long *__capability ret;
  __atomic_exchange(f, &value, &ret, __ATOMIC_SEQ_CST);
  return ret;
}

// HYBRID-LABEL: define {{[^@]+}}@test_cmpxchg_weak
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]], i8 addrspace(200)** [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    [[TMP1:%.*]] = cmpxchg weak i8 addrspace(200)** [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic, align 16
// HYBRID-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// HYBRID-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// HYBRID-NEXT:    br i1 [[TMP3]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// HYBRID:       cmpxchg.store_expected:
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP2]], i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    br label [[CMPXCHG_CONTINUE]]
// HYBRID:       cmpxchg.continue:
// HYBRID-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TMP3]] to i8
// HYBRID-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[FROMBOOL]] to i1
// HYBRID-NEXT:    ret i1 [[TOBOOL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_weak
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]], i8 addrspace(200)* addrspace(200)* [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    [[TMP1:%.*]] = cmpxchg weak i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic, align 16
// PURECAP-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// PURECAP-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// PURECAP-NEXT:    br i1 [[TMP3]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// PURECAP:       cmpxchg.store_expected:
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    br label [[CMPXCHG_CONTINUE]]
// PURECAP:       cmpxchg.continue:
// PURECAP-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TMP3]] to i8
// PURECAP-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[FROMBOOL]] to i1
// PURECAP-NEXT:    ret i1 [[TOBOOL]]
//
_Bool test_cmpxchg_weak(__uintcap_t *f, __uintcap_t *exp, __uintcap_t new) {
  return __atomic_compare_exchange(f, exp, &new, 1, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
}

// HYBRID-LABEL: define {{[^@]+}}@test_cmpxchg_strong
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]], i8 addrspace(200)** [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    [[TMP1:%.*]] = cmpxchg i8 addrspace(200)** [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic, align 16
// HYBRID-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// HYBRID-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// HYBRID-NEXT:    br i1 [[TMP3]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// HYBRID:       cmpxchg.store_expected:
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP2]], i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    br label [[CMPXCHG_CONTINUE]]
// HYBRID:       cmpxchg.continue:
// HYBRID-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TMP3]] to i8
// HYBRID-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[FROMBOOL]] to i1
// HYBRID-NEXT:    ret i1 [[TOBOOL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_strong
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]], i8 addrspace(200)* addrspace(200)* [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    [[TMP1:%.*]] = cmpxchg i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic, align 16
// PURECAP-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// PURECAP-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// PURECAP-NEXT:    br i1 [[TMP3]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// PURECAP:       cmpxchg.store_expected:
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    br label [[CMPXCHG_CONTINUE]]
// PURECAP:       cmpxchg.continue:
// PURECAP-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TMP3]] to i8
// PURECAP-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[FROMBOOL]] to i1
// PURECAP-NEXT:    ret i1 [[TOBOOL]]
//
_Bool test_cmpxchg_strong(__uintcap_t *f, __uintcap_t *exp, __uintcap_t new) {
  return __atomic_compare_exchange(f, exp, &new, 0, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_add_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_longptr
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)** [[TMP1]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// HYBRID-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)** [[TMP3]], align 16
// HYBRID-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)** [[TMP3]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP6:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP5]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP6]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP1]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// PURECAP-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)* addrspace(200)* [[TMP3]], align 16
// PURECAP-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP3]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP6:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP5]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP6]]
//
long *__capability test_fetch_add_longptr(long *__capability *ptr, __uintcap_t value) {
  return __atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_longptr_and_short
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)** [[TMP0]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP3]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)** [[TMP2]], align 16
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)** [[TMP2]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP5:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP4]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP5]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP0]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)* addrspace(200)* [[TMP2]], align 16
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP2]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP5:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP4]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP5]]
//
long *__capability test_fetch_add_longptr_and_short(long *__capability *ptr, short value) {
  return __atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_charptr
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_add_charptr(char *__capability *ptr, __uintcap_t value) {
  return __atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_charptr_and_short
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_add_charptr_and_short(char *__capability *ptr, short value) {
  return __atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_sub_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_longptr
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)** [[TMP1]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// HYBRID-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)** [[TMP3]], align 16
// HYBRID-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)** [[TMP3]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP6:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP5]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP6]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP1]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// PURECAP-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)* addrspace(200)* [[TMP3]], align 16
// PURECAP-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP3]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP6:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP5]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP6]]
//
long *__capability test_fetch_sub_longptr(long *__capability *ptr, __uintcap_t value) {
  return __atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_longptr_and_short
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)** [[TMP0]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP3]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)** [[TMP2]], align 16
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)** [[TMP2]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP5:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP4]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP5]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP0]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)* addrspace(200)* [[TMP2]], align 16
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP2]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP5:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP4]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP5]]
//
long *__capability test_fetch_sub_longptr_and_short(long *__capability *ptr, short value) {
  return __atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_charptr
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_sub_charptr(char *__capability *ptr, __uintcap_t value) {
  return __atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_charptr_and_short
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_sub_charptr_and_short(char *__capability *ptr, short value) {
  return __atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_and_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_and_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_and_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_and_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_and_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_fetch_and(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_or_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_or_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_or_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_or_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_or_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_fetch_or(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_xor_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_xor_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_xor_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_xor_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_xor_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_fetch_xor(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_nand_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_nand_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_nand_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_nand_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_nand_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_fetch_nand(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_max_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umax_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_max_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umax_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_max_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_fetch_max(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_min_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umin_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_min_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umin_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_min_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_fetch_min(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_add_fetch_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP3:%.*]] = add i64 [[TMP1]], [[TMP2]]
// HYBRID-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_add_fetch_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP3:%.*]] = add i64 [[TMP1]], [[TMP2]]
// PURECAP-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
__uintcap_t test_add_fetch_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_add_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_add_fetch_longptr
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)** [[TMP1]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// HYBRID-NEXT:    [[TMP5:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP6:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP2]])
// HYBRID-NEXT:    [[TMP7:%.*]] = add i64 [[TMP5]], [[TMP6]]
// HYBRID-NEXT:    [[TMP8:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP7]])
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP8]], i8 addrspace(200)** [[TMP3]], align 16
// HYBRID-NEXT:    [[TMP9:%.*]] = bitcast i8 addrspace(200)** [[TMP3]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP10:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP9]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP10]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_add_fetch_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP1]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// PURECAP-NEXT:    [[TMP5:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP6:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP2]])
// PURECAP-NEXT:    [[TMP7:%.*]] = add i64 [[TMP5]], [[TMP6]]
// PURECAP-NEXT:    [[TMP8:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP7]])
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP8]], i8 addrspace(200)* addrspace(200)* [[TMP3]], align 16
// PURECAP-NEXT:    [[TMP9:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP3]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP10:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP9]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP10]]
//
long *__capability test_add_fetch_longptr(long *__capability *ptr, __uintcap_t value) {
  return __atomic_add_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_add_fetch_longptr_and_short
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)** [[TMP0]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP3]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP5:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP1]])
// HYBRID-NEXT:    [[TMP6:%.*]] = add i64 [[TMP4]], [[TMP5]]
// HYBRID-NEXT:    [[TMP7:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP6]])
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP7]], i8 addrspace(200)** [[TMP2]], align 16
// HYBRID-NEXT:    [[TMP8:%.*]] = bitcast i8 addrspace(200)** [[TMP2]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP9:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP8]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP9]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_add_fetch_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP0]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP5:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP1]])
// PURECAP-NEXT:    [[TMP6:%.*]] = add i64 [[TMP4]], [[TMP5]]
// PURECAP-NEXT:    [[TMP7:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP6]])
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP7]], i8 addrspace(200)* addrspace(200)* [[TMP2]], align 16
// PURECAP-NEXT:    [[TMP8:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP2]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP9:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP8]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP9]]
//
long *__capability test_add_fetch_longptr_and_short(long *__capability *ptr, short value) {
  return __atomic_add_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_add_fetch_charptr
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP1]])
// HYBRID-NEXT:    [[TMP5:%.*]] = add i64 [[TMP3]], [[TMP4]]
// HYBRID-NEXT:    [[TMP6:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP5]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP6]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_add_fetch_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP1]])
// PURECAP-NEXT:    [[TMP5:%.*]] = add i64 [[TMP3]], [[TMP4]]
// PURECAP-NEXT:    [[TMP6:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP5]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP6]]
//
char *__capability test_add_fetch_charptr(char *__capability *ptr, __uintcap_t value) {
  return __atomic_add_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_add_fetch_charptr_and_short
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5)
// HYBRID-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP0]])
// HYBRID-NEXT:    [[TMP4:%.*]] = add i64 [[TMP2]], [[TMP3]]
// HYBRID-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP4]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP5]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_add_fetch_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5)
// PURECAP-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP0]])
// PURECAP-NEXT:    [[TMP4:%.*]] = add i64 [[TMP2]], [[TMP3]]
// PURECAP-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP4]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP5]]
//
char *__capability test_add_fetch_charptr_and_short(char *__capability *ptr, short value) {
  return __atomic_add_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_sub_fetch_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP3:%.*]] = sub i64 [[TMP1]], [[TMP2]]
// HYBRID-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_sub_fetch_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP3:%.*]] = sub i64 [[TMP1]], [[TMP2]]
// PURECAP-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
__uintcap_t test_sub_fetch_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_sub_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_sub_fetch_longptr
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)** [[TMP1]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// HYBRID-NEXT:    [[TMP5:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP6:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP2]])
// HYBRID-NEXT:    [[TMP7:%.*]] = sub i64 [[TMP5]], [[TMP6]]
// HYBRID-NEXT:    [[TMP8:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP7]])
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP8]], i8 addrspace(200)** [[TMP3]], align 16
// HYBRID-NEXT:    [[TMP9:%.*]] = bitcast i8 addrspace(200)** [[TMP3]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP10:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP9]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP10]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_sub_fetch_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP1]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// PURECAP-NEXT:    [[TMP5:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP6:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP2]])
// PURECAP-NEXT:    [[TMP7:%.*]] = sub i64 [[TMP5]], [[TMP6]]
// PURECAP-NEXT:    [[TMP8:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP7]])
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP8]], i8 addrspace(200)* addrspace(200)* [[TMP3]], align 16
// PURECAP-NEXT:    [[TMP9:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP3]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP10:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP9]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP10]]
//
long *__capability test_sub_fetch_longptr(long *__capability *ptr, __uintcap_t value) {
  return __atomic_sub_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_sub_fetch_longptr_and_short
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)** [[TMP0]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP3]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP5:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP1]])
// HYBRID-NEXT:    [[TMP6:%.*]] = sub i64 [[TMP4]], [[TMP5]]
// HYBRID-NEXT:    [[TMP7:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP6]])
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP7]], i8 addrspace(200)** [[TMP2]], align 16
// HYBRID-NEXT:    [[TMP8:%.*]] = bitcast i8 addrspace(200)** [[TMP2]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP9:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP8]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP9]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_sub_fetch_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP0]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP5:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP1]])
// PURECAP-NEXT:    [[TMP6:%.*]] = sub i64 [[TMP4]], [[TMP5]]
// PURECAP-NEXT:    [[TMP7:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP6]])
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP7]], i8 addrspace(200)* addrspace(200)* [[TMP2]], align 16
// PURECAP-NEXT:    [[TMP8:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP2]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP9:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP8]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP9]]
//
long *__capability test_sub_fetch_longptr_and_short(long *__capability *ptr, short value) {
  return __atomic_sub_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_sub_fetch_charptr
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP1]])
// HYBRID-NEXT:    [[TMP5:%.*]] = sub i64 [[TMP3]], [[TMP4]]
// HYBRID-NEXT:    [[TMP6:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP5]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP6]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_sub_fetch_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP4:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP1]])
// PURECAP-NEXT:    [[TMP5:%.*]] = sub i64 [[TMP3]], [[TMP4]]
// PURECAP-NEXT:    [[TMP6:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP5]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP6]]
//
char *__capability test_sub_fetch_charptr(char *__capability *ptr, __uintcap_t value) {
  return __atomic_sub_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_sub_fetch_charptr_and_short
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5)
// HYBRID-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP0]])
// HYBRID-NEXT:    [[TMP4:%.*]] = sub i64 [[TMP2]], [[TMP3]]
// HYBRID-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP4]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP5]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_sub_fetch_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5)
// PURECAP-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[TMP0]])
// PURECAP-NEXT:    [[TMP4:%.*]] = sub i64 [[TMP2]], [[TMP3]]
// PURECAP-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP4]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP5]]
//
char *__capability test_sub_fetch_charptr_and_short(char *__capability *ptr, short value) {
  return __atomic_sub_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_and_fetch_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_and_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP3:%.*]] = and i64 [[TMP1]], [[TMP2]]
// HYBRID-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_and_fetch_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_and_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP3:%.*]] = and i64 [[TMP1]], [[TMP2]]
// PURECAP-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
__uintcap_t test_and_fetch_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_and_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_or_fetch_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_or_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP3:%.*]] = or i64 [[TMP1]], [[TMP2]]
// HYBRID-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_or_fetch_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_or_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP3:%.*]] = or i64 [[TMP1]], [[TMP2]]
// PURECAP-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
__uintcap_t test_or_fetch_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_or_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_xor_fetch_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_xor_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP3:%.*]] = xor i64 [[TMP1]], [[TMP2]]
// HYBRID-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_xor_fetch_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_xor_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP3:%.*]] = xor i64 [[TMP1]], [[TMP2]]
// PURECAP-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP3]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP4]]
//
__uintcap_t test_xor_fetch_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_xor_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_nand_fetch_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_nand_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// HYBRID-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP3:%.*]] = and i64 [[TMP1]], [[TMP2]]
// HYBRID-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], -1
// HYBRID-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP4]])
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP5]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_nand_fetch_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_nand_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    [[TMP1:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[CALL]])
// PURECAP-NEXT:    [[TMP2:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP3:%.*]] = and i64 [[TMP1]], [[TMP2]]
// PURECAP-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], -1
// PURECAP-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)* [[CALL]], i64 [[TMP4]])
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP5]]
//
__uintcap_t test_nand_fetch_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_nand_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_max_fetch_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umax_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    [[TST:%.*]] = icmp ugt i8 addrspace(200)* [[CALL]], [[VALUE]]
// HYBRID-NEXT:    [[NEWVAL:%.*]] = select i1 [[TST]], i8 addrspace(200)* [[CALL]], i8 addrspace(200)* [[VALUE]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[NEWVAL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_max_fetch_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umax_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    [[TST:%.*]] = icmp ugt i8 addrspace(200)* [[CALL]], [[VALUE]]
// PURECAP-NEXT:    [[NEWVAL:%.*]] = select i1 [[TST]], i8 addrspace(200)* [[CALL]], i8 addrspace(200)* [[VALUE]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[NEWVAL]]
//
__uintcap_t test_max_fetch_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_max_fetch(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_min_fetch_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umin_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    [[TST:%.*]] = icmp ult i8 addrspace(200)* [[CALL]], [[VALUE]]
// HYBRID-NEXT:    [[NEWVAL:%.*]] = select i1 [[TST]], i8 addrspace(200)* [[CALL]], i8 addrspace(200)* [[VALUE]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[NEWVAL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_min_fetch_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umin_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    [[TST:%.*]] = icmp ult i8 addrspace(200)* [[CALL]], [[VALUE]]
// PURECAP-NEXT:    [[NEWVAL:%.*]] = select i1 [[TST]], i8 addrspace(200)* [[CALL]], i8 addrspace(200)* [[VALUE]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[NEWVAL]]
//
__uintcap_t test_min_fetch_uintcap(__uintcap_t *ptr, __uintcap_t value) {
  return __atomic_min_fetch(ptr, value, __ATOMIC_SEQ_CST);
}
