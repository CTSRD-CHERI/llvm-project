// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature
// RUN: %cheri_cc1 -o - -emit-llvm -Wno-mips-cheri-bugs %s | FileCheck %s --check-prefixes=MIPS
// RUN: %riscv32_cheri_cc1 -o - -emit-llvm %s | FileCheck %s --check-prefixes=RV32IXCHERI
// RUN: %riscv64_cheri_cc1 -o - -emit-llvm %s | FileCheck %s --check-prefixes=RV64IXCHERI

// All of these types contain capabilities and thus should be passed indirectly
// for hybrid varargs.

struct single_cap {
  void * __capability cap;
};

struct double_cap {
  void * __capability cap1;
  void * __capability cap2;
};

struct single_cap_array {
  void * __capability cap[1];
};

struct double_cap_array {
  void * __capability cap[2];
};

struct mixed {
  int i;
  void * __capability cap;
};

union int_or_cap {
  int i;
  void * __capability cap;
};

extern void callee(long sel, ...);

extern void * __capability cap;
extern void * __capability uintcap;
extern struct single_cap single_cap;
extern struct double_cap double_cap;
extern struct single_cap_array single_cap_array;
extern struct double_cap_array double_cap_array;
extern struct mixed mixed;
extern union int_or_cap int_or_cap;

// MIPS-LABEL: define {{[^@]+}}@caller() #0
// MIPS-NEXT:  entry:
// MIPS-NEXT:    [[NULL_CAP:%.*]] = alloca i8 addrspace(200)*, align 16
// MIPS-NEXT:    [[NULL_UINTCAP:%.*]] = alloca i8 addrspace(200)*, align 16
// MIPS-NEXT:    store i8 addrspace(200)* null, i8 addrspace(200)** [[NULL_CAP]], align 16
// MIPS-NEXT:    store i8 addrspace(200)* null, i8 addrspace(200)** [[NULL_UINTCAP]], align 16
// MIPS-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[NULL_CAP]], align 16
// MIPS-NEXT:    [[TMP1:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[NULL_UINTCAP]], align 16
// MIPS-NEXT:    [[TMP2:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** @cap, align 16
// MIPS-NEXT:    [[TMP3:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** @uintcap, align 16
// MIPS-NEXT:    call void (i64, ...) @callee(i64 signext 0, i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP3]])
// MIPS-NEXT:    [[TMP4:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds ({ i8 addrspace(200)* }, { i8 addrspace(200)* }* bitcast (%struct.single_cap* @single_cap to { i8 addrspace(200)* }*), i32 0, i32 0), align 16
// MIPS-NEXT:    call void (i64, ...) @callee(i64 signext 1, i64 undef, i8 addrspace(200)* inreg [[TMP4]])
// MIPS-NEXT:    [[TMP5:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds ({ i8 addrspace(200)*, i8 addrspace(200)* }, { i8 addrspace(200)*, i8 addrspace(200)* }* bitcast (%struct.double_cap* @double_cap to { i8 addrspace(200)*, i8 addrspace(200)* }*), i32 0, i32 0), align 16
// MIPS-NEXT:    [[TMP6:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds ({ i8 addrspace(200)*, i8 addrspace(200)* }, { i8 addrspace(200)*, i8 addrspace(200)* }* bitcast (%struct.double_cap* @double_cap to { i8 addrspace(200)*, i8 addrspace(200)* }*), i32 0, i32 1), align 16
// MIPS-NEXT:    call void (i64, ...) @callee(i64 signext 2, i64 undef, i8 addrspace(200)* inreg [[TMP5]], i8 addrspace(200)* inreg [[TMP6]])
// MIPS-NEXT:    [[TMP7:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds ({ i8 addrspace(200)* }, { i8 addrspace(200)* }* bitcast (%struct.single_cap_array* @single_cap_array to { i8 addrspace(200)* }*), i32 0, i32 0), align 16
// MIPS-NEXT:    call void (i64, ...) @callee(i64 signext 3, i64 undef, i8 addrspace(200)* inreg [[TMP7]])
// MIPS-NEXT:    [[TMP8:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds ({ i8 addrspace(200)*, i8 addrspace(200)* }, { i8 addrspace(200)*, i8 addrspace(200)* }* bitcast (%struct.double_cap_array* @double_cap_array to { i8 addrspace(200)*, i8 addrspace(200)* }*), i32 0, i32 0), align 16
// MIPS-NEXT:    [[TMP9:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds ({ i8 addrspace(200)*, i8 addrspace(200)* }, { i8 addrspace(200)*, i8 addrspace(200)* }* bitcast (%struct.double_cap_array* @double_cap_array to { i8 addrspace(200)*, i8 addrspace(200)* }*), i32 0, i32 1), align 16
// MIPS-NEXT:    call void (i64, ...) @callee(i64 signext 4, i64 undef, i8 addrspace(200)* inreg [[TMP8]], i8 addrspace(200)* inreg [[TMP9]])
// MIPS-NEXT:    [[TMP10:%.*]] = load i64, i64* getelementptr inbounds ({ i64, i64, i8 addrspace(200)* }, { i64, i64, i8 addrspace(200)* }* bitcast (%struct.mixed* @mixed to { i64, i64, i8 addrspace(200)* }*), i32 0, i32 0), align 16
// MIPS-NEXT:    [[TMP11:%.*]] = load i64, i64* getelementptr inbounds ({ i64, i64, i8 addrspace(200)* }, { i64, i64, i8 addrspace(200)* }* bitcast (%struct.mixed* @mixed to { i64, i64, i8 addrspace(200)* }*), i32 0, i32 1), align 8
// MIPS-NEXT:    [[TMP12:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds ({ i64, i64, i8 addrspace(200)* }, { i64, i64, i8 addrspace(200)* }* bitcast (%struct.mixed* @mixed to { i64, i64, i8 addrspace(200)* }*), i32 0, i32 2), align 16
// MIPS-NEXT:    call void (i64, ...) @callee(i64 signext 5, i64 undef, i64 inreg [[TMP10]], i64 inreg [[TMP11]], i8 addrspace(200)* inreg [[TMP12]])
// MIPS-NEXT:    [[TMP13:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds (%union.int_or_cap, %union.int_or_cap* @int_or_cap, i32 0, i32 0), align 16
// MIPS-NEXT:    call void (i64, ...) @callee(i64 signext 6, i64 undef, i8 addrspace(200)* inreg [[TMP13]])
// MIPS-NEXT:    ret void
//
// RV32IXCHERI-LABEL: define {{[^@]+}}@caller() #0
// RV32IXCHERI-NEXT:  entry:
// RV32IXCHERI-NEXT:    [[NULL_CAP:%.*]] = alloca i8 addrspace(200)*, align 8
// RV32IXCHERI-NEXT:    [[NULL_UINTCAP:%.*]] = alloca i8 addrspace(200)*, align 8
// RV32IXCHERI-NEXT:    [[BYVAL_TEMP:%.*]] = alloca [[STRUCT_DOUBLE_CAP:%.*]], align 8
// RV32IXCHERI-NEXT:    [[BYVAL_TEMP1:%.*]] = alloca [[STRUCT_DOUBLE_CAP_ARRAY:%.*]], align 8
// RV32IXCHERI-NEXT:    [[BYVAL_TEMP2:%.*]] = alloca [[STRUCT_MIXED:%.*]], align 8
// RV32IXCHERI-NEXT:    store i8 addrspace(200)* null, i8 addrspace(200)** [[NULL_CAP]], align 8
// RV32IXCHERI-NEXT:    store i8 addrspace(200)* null, i8 addrspace(200)** [[NULL_UINTCAP]], align 8
// RV32IXCHERI-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[NULL_CAP]], align 8
// RV32IXCHERI-NEXT:    [[TMP1:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[NULL_UINTCAP]], align 8
// RV32IXCHERI-NEXT:    [[TMP2:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** @cap, align 8
// RV32IXCHERI-NEXT:    [[TMP3:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** @uintcap, align 8
// RV32IXCHERI-NEXT:    call void (i32, ...) @callee(i32 0, i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP3]])
// RV32IXCHERI-NEXT:    [[TMP4:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds (%struct.single_cap, %struct.single_cap* @single_cap, i32 0, i32 0), align 8
// RV32IXCHERI-NEXT:    call void (i32, ...) @callee(i32 1, i8 addrspace(200)* [[TMP4]])
// RV32IXCHERI-NEXT:    [[TMP5:%.*]] = bitcast %struct.double_cap* [[BYVAL_TEMP]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 [[TMP5]], i8* align 8 bitcast (%struct.double_cap* @double_cap to i8*), i32 16, i1 false)
// RV32IXCHERI-NEXT:    call void (i32, ...) @callee(i32 2, %struct.double_cap* [[BYVAL_TEMP]])
// RV32IXCHERI-NEXT:    [[TMP6:%.*]] = load [1 x i8 addrspace(200)*], [1 x i8 addrspace(200)*]* getelementptr inbounds (%struct.single_cap_array, %struct.single_cap_array* @single_cap_array, i32 0, i32 0), align 8
// RV32IXCHERI-NEXT:    call void (i32, ...) @callee(i32 3, [1 x i8 addrspace(200)*] [[TMP6]])
// RV32IXCHERI-NEXT:    [[TMP7:%.*]] = bitcast %struct.double_cap_array* [[BYVAL_TEMP1]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 [[TMP7]], i8* align 8 bitcast (%struct.double_cap_array* @double_cap_array to i8*), i32 16, i1 false)
// RV32IXCHERI-NEXT:    call void (i32, ...) @callee(i32 4, %struct.double_cap_array* [[BYVAL_TEMP1]])
// RV32IXCHERI-NEXT:    [[TMP8:%.*]] = bitcast %struct.mixed* [[BYVAL_TEMP2]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 [[TMP8]], i8* align 8 bitcast (%struct.mixed* @mixed to i8*), i32 16, i1 false)
// RV32IXCHERI-NEXT:    call void (i32, ...) @callee(i32 5, %struct.mixed* [[BYVAL_TEMP2]])
// RV32IXCHERI-NEXT:    [[TMP9:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds (%union.int_or_cap, %union.int_or_cap* @int_or_cap, i32 0, i32 0), align 8
// RV32IXCHERI-NEXT:    call void (i32, ...) @callee(i32 6, i8 addrspace(200)* [[TMP9]])
// RV32IXCHERI-NEXT:    ret void
//
// RV64IXCHERI-LABEL: define {{[^@]+}}@caller() #0
// RV64IXCHERI-NEXT:  entry:
// RV64IXCHERI-NEXT:    [[NULL_CAP:%.*]] = alloca i8 addrspace(200)*, align 16
// RV64IXCHERI-NEXT:    [[NULL_UINTCAP:%.*]] = alloca i8 addrspace(200)*, align 16
// RV64IXCHERI-NEXT:    [[BYVAL_TEMP:%.*]] = alloca [[STRUCT_DOUBLE_CAP:%.*]], align 16
// RV64IXCHERI-NEXT:    [[BYVAL_TEMP1:%.*]] = alloca [[STRUCT_DOUBLE_CAP_ARRAY:%.*]], align 16
// RV64IXCHERI-NEXT:    [[BYVAL_TEMP2:%.*]] = alloca [[STRUCT_MIXED:%.*]], align 16
// RV64IXCHERI-NEXT:    store i8 addrspace(200)* null, i8 addrspace(200)** [[NULL_CAP]], align 16
// RV64IXCHERI-NEXT:    store i8 addrspace(200)* null, i8 addrspace(200)** [[NULL_UINTCAP]], align 16
// RV64IXCHERI-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[NULL_CAP]], align 16
// RV64IXCHERI-NEXT:    [[TMP1:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[NULL_UINTCAP]], align 16
// RV64IXCHERI-NEXT:    [[TMP2:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** @cap, align 16
// RV64IXCHERI-NEXT:    [[TMP3:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** @uintcap, align 16
// RV64IXCHERI-NEXT:    call void (i64, ...) @callee(i64 0, i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP3]])
// RV64IXCHERI-NEXT:    [[TMP4:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds (%struct.single_cap, %struct.single_cap* @single_cap, i32 0, i32 0), align 16
// RV64IXCHERI-NEXT:    call void (i64, ...) @callee(i64 1, i8 addrspace(200)* [[TMP4]])
// RV64IXCHERI-NEXT:    [[TMP5:%.*]] = bitcast %struct.double_cap* [[BYVAL_TEMP]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP5]], i8* align 16 bitcast (%struct.double_cap* @double_cap to i8*), i64 32, i1 false)
// RV64IXCHERI-NEXT:    call void (i64, ...) @callee(i64 2, %struct.double_cap* [[BYVAL_TEMP]])
// RV64IXCHERI-NEXT:    [[TMP6:%.*]] = load [1 x i8 addrspace(200)*], [1 x i8 addrspace(200)*]* getelementptr inbounds (%struct.single_cap_array, %struct.single_cap_array* @single_cap_array, i32 0, i32 0), align 16
// RV64IXCHERI-NEXT:    call void (i64, ...) @callee(i64 3, [1 x i8 addrspace(200)*] [[TMP6]])
// RV64IXCHERI-NEXT:    [[TMP7:%.*]] = bitcast %struct.double_cap_array* [[BYVAL_TEMP1]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP7]], i8* align 16 bitcast (%struct.double_cap_array* @double_cap_array to i8*), i64 32, i1 false)
// RV64IXCHERI-NEXT:    call void (i64, ...) @callee(i64 4, %struct.double_cap_array* [[BYVAL_TEMP1]])
// RV64IXCHERI-NEXT:    [[TMP8:%.*]] = bitcast %struct.mixed* [[BYVAL_TEMP2]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP8]], i8* align 16 bitcast (%struct.mixed* @mixed to i8*), i64 32, i1 false)
// RV64IXCHERI-NEXT:    call void (i64, ...) @callee(i64 5, %struct.mixed* [[BYVAL_TEMP2]])
// RV64IXCHERI-NEXT:    [[TMP9:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** getelementptr inbounds (%union.int_or_cap, %union.int_or_cap* @int_or_cap, i32 0, i32 0), align 16
// RV64IXCHERI-NEXT:    call void (i64, ...) @callee(i64 6, i8 addrspace(200)* [[TMP9]])
// RV64IXCHERI-NEXT:    ret void
//
void caller(void) {
  void * __capability null_cap = (void * __capability)0;
  __uintcap_t null_uintcap = 0;

  callee(0, null_cap, null_uintcap, cap, uintcap);
  callee(1, single_cap);
  callee(2, double_cap);
  callee(3, single_cap_array);
  callee(4, double_cap_array);
  callee(5, mixed);
  callee(6, int_or_cap);
}

// MIPS-LABEL: define {{[^@]+}}@callee
// MIPS-SAME: (i64 signext [[SEL:%.*]], ...) #0
// MIPS-NEXT:  entry:
// MIPS-NEXT:    [[SEL_ADDR:%.*]] = alloca i64, align 8
// MIPS-NEXT:    [[NULL_CAP:%.*]] = alloca i8 addrspace(200)*, align 16
// MIPS-NEXT:    [[NULL_UINTCAP:%.*]] = alloca i8 addrspace(200)*, align 16
// MIPS-NEXT:    [[CAP:%.*]] = alloca i8 addrspace(200)*, align 16
// MIPS-NEXT:    [[UINTCAP:%.*]] = alloca i8 addrspace(200)*, align 16
// MIPS-NEXT:    [[SINGLE_CAP:%.*]] = alloca [[STRUCT_SINGLE_CAP:%.*]], align 16
// MIPS-NEXT:    [[DOUBLE_CAP:%.*]] = alloca [[STRUCT_DOUBLE_CAP:%.*]], align 16
// MIPS-NEXT:    [[SINGLE_CAP_ARRAY:%.*]] = alloca [[STRUCT_SINGLE_CAP_ARRAY:%.*]], align 16
// MIPS-NEXT:    [[DOUBLE_CAP_ARRAY:%.*]] = alloca [[STRUCT_DOUBLE_CAP_ARRAY:%.*]], align 16
// MIPS-NEXT:    [[MIXED:%.*]] = alloca [[STRUCT_MIXED:%.*]], align 16
// MIPS-NEXT:    [[INT_OR_CAP:%.*]] = alloca [[UNION_INT_OR_CAP:%.*]], align 16
// MIPS-NEXT:    [[AP:%.*]] = alloca i8*, align 8
// MIPS-NEXT:    store i64 [[SEL]], i64* [[SEL_ADDR]], align 8
// MIPS-NEXT:    [[AP1:%.*]] = bitcast i8** [[AP]] to i8*
// MIPS-NEXT:    call void @llvm.va_start.p0i8(i8* [[AP1]])
// MIPS-NEXT:    [[TMP0:%.*]] = load i64, i64* [[SEL_ADDR]], align 8
// MIPS-NEXT:    switch i64 [[TMP0]], label [[SW_EPILOG:%.*]] [
// MIPS-NEXT:    i64 0, label [[SW_BB:%.*]]
// MIPS-NEXT:    i64 1, label [[SW_BB8:%.*]]
// MIPS-NEXT:    i64 2, label [[SW_BB11:%.*]]
// MIPS-NEXT:    i64 3, label [[SW_BB14:%.*]]
// MIPS-NEXT:    i64 4, label [[SW_BB17:%.*]]
// MIPS-NEXT:    i64 5, label [[SW_BB20:%.*]]
// MIPS-NEXT:    i64 6, label [[SW_BB23:%.*]]
// MIPS-NEXT:    ]
// MIPS:       sw.bb:
// MIPS-NEXT:    [[ARGP_CUR:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP1:%.*]] = ptrtoint i8* [[ARGP_CUR]] to i64
// MIPS-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 15
// MIPS-NEXT:    [[TMP3:%.*]] = and i64 [[TMP2]], -16
// MIPS-NEXT:    [[ARGP_CUR_ALIGNED:%.*]] = inttoptr i64 [[TMP3]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR_ALIGNED]], i64 16
// MIPS-NEXT:    store i8* [[ARGP_NEXT]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP4:%.*]] = bitcast i8* [[ARGP_CUR_ALIGNED]] to i8 addrspace(200)**
// MIPS-NEXT:    [[TMP5:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP4]], align 16
// MIPS-NEXT:    store volatile i8 addrspace(200)* [[TMP5]], i8 addrspace(200)** [[NULL_CAP]], align 16
// MIPS-NEXT:    [[ARGP_CUR2:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP6:%.*]] = ptrtoint i8* [[ARGP_CUR2]] to i64
// MIPS-NEXT:    [[TMP7:%.*]] = add i64 [[TMP6]], 15
// MIPS-NEXT:    [[TMP8:%.*]] = and i64 [[TMP7]], -16
// MIPS-NEXT:    [[ARGP_CUR2_ALIGNED:%.*]] = inttoptr i64 [[TMP8]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT3:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR2_ALIGNED]], i64 16
// MIPS-NEXT:    store i8* [[ARGP_NEXT3]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP9:%.*]] = bitcast i8* [[ARGP_CUR2_ALIGNED]] to i8 addrspace(200)**
// MIPS-NEXT:    [[TMP10:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP9]], align 16
// MIPS-NEXT:    store volatile i8 addrspace(200)* [[TMP10]], i8 addrspace(200)** [[NULL_UINTCAP]], align 16
// MIPS-NEXT:    [[ARGP_CUR4:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP11:%.*]] = ptrtoint i8* [[ARGP_CUR4]] to i64
// MIPS-NEXT:    [[TMP12:%.*]] = add i64 [[TMP11]], 15
// MIPS-NEXT:    [[TMP13:%.*]] = and i64 [[TMP12]], -16
// MIPS-NEXT:    [[ARGP_CUR4_ALIGNED:%.*]] = inttoptr i64 [[TMP13]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT5:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR4_ALIGNED]], i64 16
// MIPS-NEXT:    store i8* [[ARGP_NEXT5]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP14:%.*]] = bitcast i8* [[ARGP_CUR4_ALIGNED]] to i8 addrspace(200)**
// MIPS-NEXT:    [[TMP15:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP14]], align 16
// MIPS-NEXT:    store volatile i8 addrspace(200)* [[TMP15]], i8 addrspace(200)** [[CAP]], align 16
// MIPS-NEXT:    [[ARGP_CUR6:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP16:%.*]] = ptrtoint i8* [[ARGP_CUR6]] to i64
// MIPS-NEXT:    [[TMP17:%.*]] = add i64 [[TMP16]], 15
// MIPS-NEXT:    [[TMP18:%.*]] = and i64 [[TMP17]], -16
// MIPS-NEXT:    [[ARGP_CUR6_ALIGNED:%.*]] = inttoptr i64 [[TMP18]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT7:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR6_ALIGNED]], i64 16
// MIPS-NEXT:    store i8* [[ARGP_NEXT7]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP19:%.*]] = bitcast i8* [[ARGP_CUR6_ALIGNED]] to i8 addrspace(200)**
// MIPS-NEXT:    [[TMP20:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP19]], align 16
// MIPS-NEXT:    store volatile i8 addrspace(200)* [[TMP20]], i8 addrspace(200)** [[UINTCAP]], align 16
// MIPS-NEXT:    br label [[SW_EPILOG]]
// MIPS:       sw.bb8:
// MIPS-NEXT:    [[ARGP_CUR9:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP21:%.*]] = ptrtoint i8* [[ARGP_CUR9]] to i64
// MIPS-NEXT:    [[TMP22:%.*]] = add i64 [[TMP21]], 15
// MIPS-NEXT:    [[TMP23:%.*]] = and i64 [[TMP22]], -16
// MIPS-NEXT:    [[ARGP_CUR9_ALIGNED:%.*]] = inttoptr i64 [[TMP23]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT10:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR9_ALIGNED]], i64 16
// MIPS-NEXT:    store i8* [[ARGP_NEXT10]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP24:%.*]] = bitcast i8* [[ARGP_CUR9_ALIGNED]] to %struct.single_cap*
// MIPS-NEXT:    [[TMP25:%.*]] = bitcast %struct.single_cap* [[SINGLE_CAP]] to i8*
// MIPS-NEXT:    [[TMP26:%.*]] = bitcast %struct.single_cap* [[TMP24]] to i8*
// MIPS-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP25]], i8* align 16 [[TMP26]], i64 16, i1 true)
// MIPS-NEXT:    br label [[SW_EPILOG]]
// MIPS:       sw.bb11:
// MIPS-NEXT:    [[ARGP_CUR12:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP27:%.*]] = ptrtoint i8* [[ARGP_CUR12]] to i64
// MIPS-NEXT:    [[TMP28:%.*]] = add i64 [[TMP27]], 15
// MIPS-NEXT:    [[TMP29:%.*]] = and i64 [[TMP28]], -16
// MIPS-NEXT:    [[ARGP_CUR12_ALIGNED:%.*]] = inttoptr i64 [[TMP29]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT13:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR12_ALIGNED]], i64 32
// MIPS-NEXT:    store i8* [[ARGP_NEXT13]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP30:%.*]] = bitcast i8* [[ARGP_CUR12_ALIGNED]] to %struct.double_cap*
// MIPS-NEXT:    [[TMP31:%.*]] = bitcast %struct.double_cap* [[DOUBLE_CAP]] to i8*
// MIPS-NEXT:    [[TMP32:%.*]] = bitcast %struct.double_cap* [[TMP30]] to i8*
// MIPS-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP31]], i8* align 16 [[TMP32]], i64 32, i1 true)
// MIPS-NEXT:    br label [[SW_EPILOG]]
// MIPS:       sw.bb14:
// MIPS-NEXT:    [[ARGP_CUR15:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP33:%.*]] = ptrtoint i8* [[ARGP_CUR15]] to i64
// MIPS-NEXT:    [[TMP34:%.*]] = add i64 [[TMP33]], 15
// MIPS-NEXT:    [[TMP35:%.*]] = and i64 [[TMP34]], -16
// MIPS-NEXT:    [[ARGP_CUR15_ALIGNED:%.*]] = inttoptr i64 [[TMP35]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT16:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR15_ALIGNED]], i64 16
// MIPS-NEXT:    store i8* [[ARGP_NEXT16]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP36:%.*]] = bitcast i8* [[ARGP_CUR15_ALIGNED]] to %struct.single_cap_array*
// MIPS-NEXT:    [[TMP37:%.*]] = bitcast %struct.single_cap_array* [[SINGLE_CAP_ARRAY]] to i8*
// MIPS-NEXT:    [[TMP38:%.*]] = bitcast %struct.single_cap_array* [[TMP36]] to i8*
// MIPS-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP37]], i8* align 16 [[TMP38]], i64 16, i1 true)
// MIPS-NEXT:    br label [[SW_EPILOG]]
// MIPS:       sw.bb17:
// MIPS-NEXT:    [[ARGP_CUR18:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP39:%.*]] = ptrtoint i8* [[ARGP_CUR18]] to i64
// MIPS-NEXT:    [[TMP40:%.*]] = add i64 [[TMP39]], 15
// MIPS-NEXT:    [[TMP41:%.*]] = and i64 [[TMP40]], -16
// MIPS-NEXT:    [[ARGP_CUR18_ALIGNED:%.*]] = inttoptr i64 [[TMP41]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT19:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR18_ALIGNED]], i64 32
// MIPS-NEXT:    store i8* [[ARGP_NEXT19]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP42:%.*]] = bitcast i8* [[ARGP_CUR18_ALIGNED]] to %struct.double_cap_array*
// MIPS-NEXT:    [[TMP43:%.*]] = bitcast %struct.double_cap_array* [[DOUBLE_CAP_ARRAY]] to i8*
// MIPS-NEXT:    [[TMP44:%.*]] = bitcast %struct.double_cap_array* [[TMP42]] to i8*
// MIPS-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP43]], i8* align 16 [[TMP44]], i64 32, i1 true)
// MIPS-NEXT:    br label [[SW_EPILOG]]
// MIPS:       sw.bb20:
// MIPS-NEXT:    [[ARGP_CUR21:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP45:%.*]] = ptrtoint i8* [[ARGP_CUR21]] to i64
// MIPS-NEXT:    [[TMP46:%.*]] = add i64 [[TMP45]], 15
// MIPS-NEXT:    [[TMP47:%.*]] = and i64 [[TMP46]], -16
// MIPS-NEXT:    [[ARGP_CUR21_ALIGNED:%.*]] = inttoptr i64 [[TMP47]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT22:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR21_ALIGNED]], i64 32
// MIPS-NEXT:    store i8* [[ARGP_NEXT22]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP48:%.*]] = bitcast i8* [[ARGP_CUR21_ALIGNED]] to %struct.mixed*
// MIPS-NEXT:    [[TMP49:%.*]] = bitcast %struct.mixed* [[MIXED]] to i8*
// MIPS-NEXT:    [[TMP50:%.*]] = bitcast %struct.mixed* [[TMP48]] to i8*
// MIPS-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP49]], i8* align 16 [[TMP50]], i64 32, i1 true)
// MIPS-NEXT:    br label [[SW_EPILOG]]
// MIPS:       sw.bb23:
// MIPS-NEXT:    [[ARGP_CUR24:%.*]] = load i8*, i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP51:%.*]] = ptrtoint i8* [[ARGP_CUR24]] to i64
// MIPS-NEXT:    [[TMP52:%.*]] = add i64 [[TMP51]], 15
// MIPS-NEXT:    [[TMP53:%.*]] = and i64 [[TMP52]], -16
// MIPS-NEXT:    [[ARGP_CUR24_ALIGNED:%.*]] = inttoptr i64 [[TMP53]] to i8*
// MIPS-NEXT:    [[ARGP_NEXT25:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR24_ALIGNED]], i64 16
// MIPS-NEXT:    store i8* [[ARGP_NEXT25]], i8** [[AP]], align 8
// MIPS-NEXT:    [[TMP54:%.*]] = bitcast i8* [[ARGP_CUR24_ALIGNED]] to %union.int_or_cap*
// MIPS-NEXT:    [[TMP55:%.*]] = bitcast %union.int_or_cap* [[INT_OR_CAP]] to i8*
// MIPS-NEXT:    [[TMP56:%.*]] = bitcast %union.int_or_cap* [[TMP54]] to i8*
// MIPS-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP55]], i8* align 16 [[TMP56]], i64 16, i1 true)
// MIPS-NEXT:    br label [[SW_EPILOG]]
// MIPS:       sw.epilog:
// MIPS-NEXT:    [[AP26:%.*]] = bitcast i8** [[AP]] to i8*
// MIPS-NEXT:    call void @llvm.va_end.p0i8(i8* [[AP26]])
// MIPS-NEXT:    ret void
//
// RV32IXCHERI-LABEL: define {{[^@]+}}@callee
// RV32IXCHERI-SAME: (i32 [[SEL:%.*]], ...) #0
// RV32IXCHERI-NEXT:  entry:
// RV32IXCHERI-NEXT:    [[SEL_ADDR:%.*]] = alloca i32, align 4
// RV32IXCHERI-NEXT:    [[NULL_CAP:%.*]] = alloca i8 addrspace(200)*, align 8
// RV32IXCHERI-NEXT:    [[NULL_UINTCAP:%.*]] = alloca i8 addrspace(200)*, align 8
// RV32IXCHERI-NEXT:    [[CAP:%.*]] = alloca i8 addrspace(200)*, align 8
// RV32IXCHERI-NEXT:    [[UINTCAP:%.*]] = alloca i8 addrspace(200)*, align 8
// RV32IXCHERI-NEXT:    [[SINGLE_CAP:%.*]] = alloca [[STRUCT_SINGLE_CAP:%.*]], align 8
// RV32IXCHERI-NEXT:    [[DOUBLE_CAP:%.*]] = alloca [[STRUCT_DOUBLE_CAP:%.*]], align 8
// RV32IXCHERI-NEXT:    [[SINGLE_CAP_ARRAY:%.*]] = alloca [[STRUCT_SINGLE_CAP_ARRAY:%.*]], align 8
// RV32IXCHERI-NEXT:    [[DOUBLE_CAP_ARRAY:%.*]] = alloca [[STRUCT_DOUBLE_CAP_ARRAY:%.*]], align 8
// RV32IXCHERI-NEXT:    [[MIXED:%.*]] = alloca [[STRUCT_MIXED:%.*]], align 8
// RV32IXCHERI-NEXT:    [[INT_OR_CAP:%.*]] = alloca [[UNION_INT_OR_CAP:%.*]], align 8
// RV32IXCHERI-NEXT:    [[AP:%.*]] = alloca i8*, align 4
// RV32IXCHERI-NEXT:    store i32 [[SEL]], i32* [[SEL_ADDR]], align 4
// RV32IXCHERI-NEXT:    [[AP1:%.*]] = bitcast i8** [[AP]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.va_start.p0i8(i8* [[AP1]])
// RV32IXCHERI-NEXT:    [[TMP0:%.*]] = load i32, i32* [[SEL_ADDR]], align 4
// RV32IXCHERI-NEXT:    switch i32 [[TMP0]], label [[SW_EPILOG:%.*]] [
// RV32IXCHERI-NEXT:    i32 0, label [[SW_BB:%.*]]
// RV32IXCHERI-NEXT:    i32 1, label [[SW_BB8:%.*]]
// RV32IXCHERI-NEXT:    i32 2, label [[SW_BB11:%.*]]
// RV32IXCHERI-NEXT:    i32 3, label [[SW_BB14:%.*]]
// RV32IXCHERI-NEXT:    i32 4, label [[SW_BB17:%.*]]
// RV32IXCHERI-NEXT:    i32 5, label [[SW_BB20:%.*]]
// RV32IXCHERI-NEXT:    i32 6, label [[SW_BB23:%.*]]
// RV32IXCHERI-NEXT:    ]
// RV32IXCHERI:       sw.bb:
// RV32IXCHERI-NEXT:    [[ARGP_CUR:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP1:%.*]] = ptrtoint i8* [[ARGP_CUR]] to i32
// RV32IXCHERI-NEXT:    [[TMP2:%.*]] = add i32 [[TMP1]], 7
// RV32IXCHERI-NEXT:    [[TMP3:%.*]] = and i32 [[TMP2]], -8
// RV32IXCHERI-NEXT:    [[ARGP_CUR_ALIGNED:%.*]] = inttoptr i32 [[TMP3]] to i8*
// RV32IXCHERI-NEXT:    [[ARGP_NEXT:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR_ALIGNED]], i32 8
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP4:%.*]] = bitcast i8* [[ARGP_CUR_ALIGNED]] to i8 addrspace(200)**
// RV32IXCHERI-NEXT:    [[TMP5:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP4]], align 8
// RV32IXCHERI-NEXT:    store volatile i8 addrspace(200)* [[TMP5]], i8 addrspace(200)** [[NULL_CAP]], align 8
// RV32IXCHERI-NEXT:    [[ARGP_CUR2:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP6:%.*]] = ptrtoint i8* [[ARGP_CUR2]] to i32
// RV32IXCHERI-NEXT:    [[TMP7:%.*]] = add i32 [[TMP6]], 7
// RV32IXCHERI-NEXT:    [[TMP8:%.*]] = and i32 [[TMP7]], -8
// RV32IXCHERI-NEXT:    [[ARGP_CUR2_ALIGNED:%.*]] = inttoptr i32 [[TMP8]] to i8*
// RV32IXCHERI-NEXT:    [[ARGP_NEXT3:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR2_ALIGNED]], i32 8
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT3]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP9:%.*]] = bitcast i8* [[ARGP_CUR2_ALIGNED]] to i8 addrspace(200)**
// RV32IXCHERI-NEXT:    [[TMP10:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP9]], align 8
// RV32IXCHERI-NEXT:    store volatile i8 addrspace(200)* [[TMP10]], i8 addrspace(200)** [[NULL_UINTCAP]], align 8
// RV32IXCHERI-NEXT:    [[ARGP_CUR4:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP11:%.*]] = ptrtoint i8* [[ARGP_CUR4]] to i32
// RV32IXCHERI-NEXT:    [[TMP12:%.*]] = add i32 [[TMP11]], 7
// RV32IXCHERI-NEXT:    [[TMP13:%.*]] = and i32 [[TMP12]], -8
// RV32IXCHERI-NEXT:    [[ARGP_CUR4_ALIGNED:%.*]] = inttoptr i32 [[TMP13]] to i8*
// RV32IXCHERI-NEXT:    [[ARGP_NEXT5:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR4_ALIGNED]], i32 8
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT5]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP14:%.*]] = bitcast i8* [[ARGP_CUR4_ALIGNED]] to i8 addrspace(200)**
// RV32IXCHERI-NEXT:    [[TMP15:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP14]], align 8
// RV32IXCHERI-NEXT:    store volatile i8 addrspace(200)* [[TMP15]], i8 addrspace(200)** [[CAP]], align 8
// RV32IXCHERI-NEXT:    [[ARGP_CUR6:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP16:%.*]] = ptrtoint i8* [[ARGP_CUR6]] to i32
// RV32IXCHERI-NEXT:    [[TMP17:%.*]] = add i32 [[TMP16]], 7
// RV32IXCHERI-NEXT:    [[TMP18:%.*]] = and i32 [[TMP17]], -8
// RV32IXCHERI-NEXT:    [[ARGP_CUR6_ALIGNED:%.*]] = inttoptr i32 [[TMP18]] to i8*
// RV32IXCHERI-NEXT:    [[ARGP_NEXT7:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR6_ALIGNED]], i32 8
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT7]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP19:%.*]] = bitcast i8* [[ARGP_CUR6_ALIGNED]] to i8 addrspace(200)**
// RV32IXCHERI-NEXT:    [[TMP20:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP19]], align 8
// RV32IXCHERI-NEXT:    store volatile i8 addrspace(200)* [[TMP20]], i8 addrspace(200)** [[UINTCAP]], align 8
// RV32IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV32IXCHERI:       sw.bb8:
// RV32IXCHERI-NEXT:    [[ARGP_CUR9:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP21:%.*]] = ptrtoint i8* [[ARGP_CUR9]] to i32
// RV32IXCHERI-NEXT:    [[TMP22:%.*]] = add i32 [[TMP21]], 7
// RV32IXCHERI-NEXT:    [[TMP23:%.*]] = and i32 [[TMP22]], -8
// RV32IXCHERI-NEXT:    [[ARGP_CUR9_ALIGNED:%.*]] = inttoptr i32 [[TMP23]] to i8*
// RV32IXCHERI-NEXT:    [[ARGP_NEXT10:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR9_ALIGNED]], i32 8
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT10]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP24:%.*]] = bitcast i8* [[ARGP_CUR9_ALIGNED]] to %struct.single_cap*
// RV32IXCHERI-NEXT:    [[TMP25:%.*]] = bitcast %struct.single_cap* [[SINGLE_CAP]] to i8*
// RV32IXCHERI-NEXT:    [[TMP26:%.*]] = bitcast %struct.single_cap* [[TMP24]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 [[TMP25]], i8* align 8 [[TMP26]], i32 8, i1 true)
// RV32IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV32IXCHERI:       sw.bb11:
// RV32IXCHERI-NEXT:    [[ARGP_CUR12:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[ARGP_NEXT13:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR12]], i32 4
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT13]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP27:%.*]] = bitcast i8* [[ARGP_CUR12]] to %struct.double_cap**
// RV32IXCHERI-NEXT:    [[TMP28:%.*]] = load %struct.double_cap*, %struct.double_cap** [[TMP27]], align 4
// RV32IXCHERI-NEXT:    [[TMP29:%.*]] = bitcast %struct.double_cap* [[DOUBLE_CAP]] to i8*
// RV32IXCHERI-NEXT:    [[TMP30:%.*]] = bitcast %struct.double_cap* [[TMP28]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 [[TMP29]], i8* align 8 [[TMP30]], i32 16, i1 true)
// RV32IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV32IXCHERI:       sw.bb14:
// RV32IXCHERI-NEXT:    [[ARGP_CUR15:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP31:%.*]] = ptrtoint i8* [[ARGP_CUR15]] to i32
// RV32IXCHERI-NEXT:    [[TMP32:%.*]] = add i32 [[TMP31]], 7
// RV32IXCHERI-NEXT:    [[TMP33:%.*]] = and i32 [[TMP32]], -8
// RV32IXCHERI-NEXT:    [[ARGP_CUR15_ALIGNED:%.*]] = inttoptr i32 [[TMP33]] to i8*
// RV32IXCHERI-NEXT:    [[ARGP_NEXT16:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR15_ALIGNED]], i32 8
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT16]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP34:%.*]] = bitcast i8* [[ARGP_CUR15_ALIGNED]] to %struct.single_cap_array*
// RV32IXCHERI-NEXT:    [[TMP35:%.*]] = bitcast %struct.single_cap_array* [[SINGLE_CAP_ARRAY]] to i8*
// RV32IXCHERI-NEXT:    [[TMP36:%.*]] = bitcast %struct.single_cap_array* [[TMP34]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 [[TMP35]], i8* align 8 [[TMP36]], i32 8, i1 true)
// RV32IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV32IXCHERI:       sw.bb17:
// RV32IXCHERI-NEXT:    [[ARGP_CUR18:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[ARGP_NEXT19:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR18]], i32 4
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT19]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP37:%.*]] = bitcast i8* [[ARGP_CUR18]] to %struct.double_cap_array**
// RV32IXCHERI-NEXT:    [[TMP38:%.*]] = load %struct.double_cap_array*, %struct.double_cap_array** [[TMP37]], align 4
// RV32IXCHERI-NEXT:    [[TMP39:%.*]] = bitcast %struct.double_cap_array* [[DOUBLE_CAP_ARRAY]] to i8*
// RV32IXCHERI-NEXT:    [[TMP40:%.*]] = bitcast %struct.double_cap_array* [[TMP38]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 [[TMP39]], i8* align 8 [[TMP40]], i32 16, i1 true)
// RV32IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV32IXCHERI:       sw.bb20:
// RV32IXCHERI-NEXT:    [[ARGP_CUR21:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[ARGP_NEXT22:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR21]], i32 4
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT22]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP41:%.*]] = bitcast i8* [[ARGP_CUR21]] to %struct.mixed**
// RV32IXCHERI-NEXT:    [[TMP42:%.*]] = load %struct.mixed*, %struct.mixed** [[TMP41]], align 4
// RV32IXCHERI-NEXT:    [[TMP43:%.*]] = bitcast %struct.mixed* [[MIXED]] to i8*
// RV32IXCHERI-NEXT:    [[TMP44:%.*]] = bitcast %struct.mixed* [[TMP42]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 [[TMP43]], i8* align 8 [[TMP44]], i32 16, i1 true)
// RV32IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV32IXCHERI:       sw.bb23:
// RV32IXCHERI-NEXT:    [[ARGP_CUR24:%.*]] = load i8*, i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP45:%.*]] = ptrtoint i8* [[ARGP_CUR24]] to i32
// RV32IXCHERI-NEXT:    [[TMP46:%.*]] = add i32 [[TMP45]], 7
// RV32IXCHERI-NEXT:    [[TMP47:%.*]] = and i32 [[TMP46]], -8
// RV32IXCHERI-NEXT:    [[ARGP_CUR24_ALIGNED:%.*]] = inttoptr i32 [[TMP47]] to i8*
// RV32IXCHERI-NEXT:    [[ARGP_NEXT25:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR24_ALIGNED]], i32 8
// RV32IXCHERI-NEXT:    store i8* [[ARGP_NEXT25]], i8** [[AP]], align 4
// RV32IXCHERI-NEXT:    [[TMP48:%.*]] = bitcast i8* [[ARGP_CUR24_ALIGNED]] to %union.int_or_cap*
// RV32IXCHERI-NEXT:    [[TMP49:%.*]] = bitcast %union.int_or_cap* [[INT_OR_CAP]] to i8*
// RV32IXCHERI-NEXT:    [[TMP50:%.*]] = bitcast %union.int_or_cap* [[TMP48]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 [[TMP49]], i8* align 8 [[TMP50]], i32 8, i1 true)
// RV32IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV32IXCHERI:       sw.epilog:
// RV32IXCHERI-NEXT:    [[AP26:%.*]] = bitcast i8** [[AP]] to i8*
// RV32IXCHERI-NEXT:    call void @llvm.va_end.p0i8(i8* [[AP26]])
// RV32IXCHERI-NEXT:    ret void
//
// RV64IXCHERI-LABEL: define {{[^@]+}}@callee
// RV64IXCHERI-SAME: (i64 [[SEL:%.*]], ...) #0
// RV64IXCHERI-NEXT:  entry:
// RV64IXCHERI-NEXT:    [[SEL_ADDR:%.*]] = alloca i64, align 8
// RV64IXCHERI-NEXT:    [[NULL_CAP:%.*]] = alloca i8 addrspace(200)*, align 16
// RV64IXCHERI-NEXT:    [[NULL_UINTCAP:%.*]] = alloca i8 addrspace(200)*, align 16
// RV64IXCHERI-NEXT:    [[CAP:%.*]] = alloca i8 addrspace(200)*, align 16
// RV64IXCHERI-NEXT:    [[UINTCAP:%.*]] = alloca i8 addrspace(200)*, align 16
// RV64IXCHERI-NEXT:    [[SINGLE_CAP:%.*]] = alloca [[STRUCT_SINGLE_CAP:%.*]], align 16
// RV64IXCHERI-NEXT:    [[DOUBLE_CAP:%.*]] = alloca [[STRUCT_DOUBLE_CAP:%.*]], align 16
// RV64IXCHERI-NEXT:    [[SINGLE_CAP_ARRAY:%.*]] = alloca [[STRUCT_SINGLE_CAP_ARRAY:%.*]], align 16
// RV64IXCHERI-NEXT:    [[DOUBLE_CAP_ARRAY:%.*]] = alloca [[STRUCT_DOUBLE_CAP_ARRAY:%.*]], align 16
// RV64IXCHERI-NEXT:    [[MIXED:%.*]] = alloca [[STRUCT_MIXED:%.*]], align 16
// RV64IXCHERI-NEXT:    [[INT_OR_CAP:%.*]] = alloca [[UNION_INT_OR_CAP:%.*]], align 16
// RV64IXCHERI-NEXT:    [[AP:%.*]] = alloca i8*, align 8
// RV64IXCHERI-NEXT:    store i64 [[SEL]], i64* [[SEL_ADDR]], align 8
// RV64IXCHERI-NEXT:    [[AP1:%.*]] = bitcast i8** [[AP]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.va_start.p0i8(i8* [[AP1]])
// RV64IXCHERI-NEXT:    [[TMP0:%.*]] = load i64, i64* [[SEL_ADDR]], align 8
// RV64IXCHERI-NEXT:    switch i64 [[TMP0]], label [[SW_EPILOG:%.*]] [
// RV64IXCHERI-NEXT:    i64 0, label [[SW_BB:%.*]]
// RV64IXCHERI-NEXT:    i64 1, label [[SW_BB8:%.*]]
// RV64IXCHERI-NEXT:    i64 2, label [[SW_BB11:%.*]]
// RV64IXCHERI-NEXT:    i64 3, label [[SW_BB14:%.*]]
// RV64IXCHERI-NEXT:    i64 4, label [[SW_BB17:%.*]]
// RV64IXCHERI-NEXT:    i64 5, label [[SW_BB20:%.*]]
// RV64IXCHERI-NEXT:    i64 6, label [[SW_BB23:%.*]]
// RV64IXCHERI-NEXT:    ]
// RV64IXCHERI:       sw.bb:
// RV64IXCHERI-NEXT:    [[ARGP_CUR:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP1:%.*]] = ptrtoint i8* [[ARGP_CUR]] to i64
// RV64IXCHERI-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 15
// RV64IXCHERI-NEXT:    [[TMP3:%.*]] = and i64 [[TMP2]], -16
// RV64IXCHERI-NEXT:    [[ARGP_CUR_ALIGNED:%.*]] = inttoptr i64 [[TMP3]] to i8*
// RV64IXCHERI-NEXT:    [[ARGP_NEXT:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR_ALIGNED]], i64 16
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP4:%.*]] = bitcast i8* [[ARGP_CUR_ALIGNED]] to i8 addrspace(200)**
// RV64IXCHERI-NEXT:    [[TMP5:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP4]], align 16
// RV64IXCHERI-NEXT:    store volatile i8 addrspace(200)* [[TMP5]], i8 addrspace(200)** [[NULL_CAP]], align 16
// RV64IXCHERI-NEXT:    [[ARGP_CUR2:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP6:%.*]] = ptrtoint i8* [[ARGP_CUR2]] to i64
// RV64IXCHERI-NEXT:    [[TMP7:%.*]] = add i64 [[TMP6]], 15
// RV64IXCHERI-NEXT:    [[TMP8:%.*]] = and i64 [[TMP7]], -16
// RV64IXCHERI-NEXT:    [[ARGP_CUR2_ALIGNED:%.*]] = inttoptr i64 [[TMP8]] to i8*
// RV64IXCHERI-NEXT:    [[ARGP_NEXT3:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR2_ALIGNED]], i64 16
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT3]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP9:%.*]] = bitcast i8* [[ARGP_CUR2_ALIGNED]] to i8 addrspace(200)**
// RV64IXCHERI-NEXT:    [[TMP10:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP9]], align 16
// RV64IXCHERI-NEXT:    store volatile i8 addrspace(200)* [[TMP10]], i8 addrspace(200)** [[NULL_UINTCAP]], align 16
// RV64IXCHERI-NEXT:    [[ARGP_CUR4:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP11:%.*]] = ptrtoint i8* [[ARGP_CUR4]] to i64
// RV64IXCHERI-NEXT:    [[TMP12:%.*]] = add i64 [[TMP11]], 15
// RV64IXCHERI-NEXT:    [[TMP13:%.*]] = and i64 [[TMP12]], -16
// RV64IXCHERI-NEXT:    [[ARGP_CUR4_ALIGNED:%.*]] = inttoptr i64 [[TMP13]] to i8*
// RV64IXCHERI-NEXT:    [[ARGP_NEXT5:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR4_ALIGNED]], i64 16
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT5]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP14:%.*]] = bitcast i8* [[ARGP_CUR4_ALIGNED]] to i8 addrspace(200)**
// RV64IXCHERI-NEXT:    [[TMP15:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP14]], align 16
// RV64IXCHERI-NEXT:    store volatile i8 addrspace(200)* [[TMP15]], i8 addrspace(200)** [[CAP]], align 16
// RV64IXCHERI-NEXT:    [[ARGP_CUR6:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP16:%.*]] = ptrtoint i8* [[ARGP_CUR6]] to i64
// RV64IXCHERI-NEXT:    [[TMP17:%.*]] = add i64 [[TMP16]], 15
// RV64IXCHERI-NEXT:    [[TMP18:%.*]] = and i64 [[TMP17]], -16
// RV64IXCHERI-NEXT:    [[ARGP_CUR6_ALIGNED:%.*]] = inttoptr i64 [[TMP18]] to i8*
// RV64IXCHERI-NEXT:    [[ARGP_NEXT7:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR6_ALIGNED]], i64 16
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT7]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP19:%.*]] = bitcast i8* [[ARGP_CUR6_ALIGNED]] to i8 addrspace(200)**
// RV64IXCHERI-NEXT:    [[TMP20:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[TMP19]], align 16
// RV64IXCHERI-NEXT:    store volatile i8 addrspace(200)* [[TMP20]], i8 addrspace(200)** [[UINTCAP]], align 16
// RV64IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV64IXCHERI:       sw.bb8:
// RV64IXCHERI-NEXT:    [[ARGP_CUR9:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP21:%.*]] = ptrtoint i8* [[ARGP_CUR9]] to i64
// RV64IXCHERI-NEXT:    [[TMP22:%.*]] = add i64 [[TMP21]], 15
// RV64IXCHERI-NEXT:    [[TMP23:%.*]] = and i64 [[TMP22]], -16
// RV64IXCHERI-NEXT:    [[ARGP_CUR9_ALIGNED:%.*]] = inttoptr i64 [[TMP23]] to i8*
// RV64IXCHERI-NEXT:    [[ARGP_NEXT10:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR9_ALIGNED]], i64 16
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT10]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP24:%.*]] = bitcast i8* [[ARGP_CUR9_ALIGNED]] to %struct.single_cap*
// RV64IXCHERI-NEXT:    [[TMP25:%.*]] = bitcast %struct.single_cap* [[SINGLE_CAP]] to i8*
// RV64IXCHERI-NEXT:    [[TMP26:%.*]] = bitcast %struct.single_cap* [[TMP24]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP25]], i8* align 16 [[TMP26]], i64 16, i1 true)
// RV64IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV64IXCHERI:       sw.bb11:
// RV64IXCHERI-NEXT:    [[ARGP_CUR12:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[ARGP_NEXT13:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR12]], i64 8
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT13]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP27:%.*]] = bitcast i8* [[ARGP_CUR12]] to %struct.double_cap**
// RV64IXCHERI-NEXT:    [[TMP28:%.*]] = load %struct.double_cap*, %struct.double_cap** [[TMP27]], align 8
// RV64IXCHERI-NEXT:    [[TMP29:%.*]] = bitcast %struct.double_cap* [[DOUBLE_CAP]] to i8*
// RV64IXCHERI-NEXT:    [[TMP30:%.*]] = bitcast %struct.double_cap* [[TMP28]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP29]], i8* align 16 [[TMP30]], i64 32, i1 true)
// RV64IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV64IXCHERI:       sw.bb14:
// RV64IXCHERI-NEXT:    [[ARGP_CUR15:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP31:%.*]] = ptrtoint i8* [[ARGP_CUR15]] to i64
// RV64IXCHERI-NEXT:    [[TMP32:%.*]] = add i64 [[TMP31]], 15
// RV64IXCHERI-NEXT:    [[TMP33:%.*]] = and i64 [[TMP32]], -16
// RV64IXCHERI-NEXT:    [[ARGP_CUR15_ALIGNED:%.*]] = inttoptr i64 [[TMP33]] to i8*
// RV64IXCHERI-NEXT:    [[ARGP_NEXT16:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR15_ALIGNED]], i64 16
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT16]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP34:%.*]] = bitcast i8* [[ARGP_CUR15_ALIGNED]] to %struct.single_cap_array*
// RV64IXCHERI-NEXT:    [[TMP35:%.*]] = bitcast %struct.single_cap_array* [[SINGLE_CAP_ARRAY]] to i8*
// RV64IXCHERI-NEXT:    [[TMP36:%.*]] = bitcast %struct.single_cap_array* [[TMP34]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP35]], i8* align 16 [[TMP36]], i64 16, i1 true)
// RV64IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV64IXCHERI:       sw.bb17:
// RV64IXCHERI-NEXT:    [[ARGP_CUR18:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[ARGP_NEXT19:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR18]], i64 8
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT19]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP37:%.*]] = bitcast i8* [[ARGP_CUR18]] to %struct.double_cap_array**
// RV64IXCHERI-NEXT:    [[TMP38:%.*]] = load %struct.double_cap_array*, %struct.double_cap_array** [[TMP37]], align 8
// RV64IXCHERI-NEXT:    [[TMP39:%.*]] = bitcast %struct.double_cap_array* [[DOUBLE_CAP_ARRAY]] to i8*
// RV64IXCHERI-NEXT:    [[TMP40:%.*]] = bitcast %struct.double_cap_array* [[TMP38]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP39]], i8* align 16 [[TMP40]], i64 32, i1 true)
// RV64IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV64IXCHERI:       sw.bb20:
// RV64IXCHERI-NEXT:    [[ARGP_CUR21:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[ARGP_NEXT22:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR21]], i64 8
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT22]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP41:%.*]] = bitcast i8* [[ARGP_CUR21]] to %struct.mixed**
// RV64IXCHERI-NEXT:    [[TMP42:%.*]] = load %struct.mixed*, %struct.mixed** [[TMP41]], align 8
// RV64IXCHERI-NEXT:    [[TMP43:%.*]] = bitcast %struct.mixed* [[MIXED]] to i8*
// RV64IXCHERI-NEXT:    [[TMP44:%.*]] = bitcast %struct.mixed* [[TMP42]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP43]], i8* align 16 [[TMP44]], i64 32, i1 true)
// RV64IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV64IXCHERI:       sw.bb23:
// RV64IXCHERI-NEXT:    [[ARGP_CUR24:%.*]] = load i8*, i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP45:%.*]] = ptrtoint i8* [[ARGP_CUR24]] to i64
// RV64IXCHERI-NEXT:    [[TMP46:%.*]] = add i64 [[TMP45]], 15
// RV64IXCHERI-NEXT:    [[TMP47:%.*]] = and i64 [[TMP46]], -16
// RV64IXCHERI-NEXT:    [[ARGP_CUR24_ALIGNED:%.*]] = inttoptr i64 [[TMP47]] to i8*
// RV64IXCHERI-NEXT:    [[ARGP_NEXT25:%.*]] = getelementptr inbounds i8, i8* [[ARGP_CUR24_ALIGNED]], i64 16
// RV64IXCHERI-NEXT:    store i8* [[ARGP_NEXT25]], i8** [[AP]], align 8
// RV64IXCHERI-NEXT:    [[TMP48:%.*]] = bitcast i8* [[ARGP_CUR24_ALIGNED]] to %union.int_or_cap*
// RV64IXCHERI-NEXT:    [[TMP49:%.*]] = bitcast %union.int_or_cap* [[INT_OR_CAP]] to i8*
// RV64IXCHERI-NEXT:    [[TMP50:%.*]] = bitcast %union.int_or_cap* [[TMP48]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 [[TMP49]], i8* align 16 [[TMP50]], i64 16, i1 true)
// RV64IXCHERI-NEXT:    br label [[SW_EPILOG]]
// RV64IXCHERI:       sw.epilog:
// RV64IXCHERI-NEXT:    [[AP26:%.*]] = bitcast i8** [[AP]] to i8*
// RV64IXCHERI-NEXT:    call void @llvm.va_end.p0i8(i8* [[AP26]])
// RV64IXCHERI-NEXT:    ret void
//
void callee(long sel, ...) {
  void * __capability volatile null_cap;
  volatile __uintcap_t null_uintcap;
  void * __capability volatile cap;
  volatile __uintcap_t uintcap;
  volatile struct single_cap single_cap;
  volatile struct double_cap double_cap;
  volatile struct single_cap_array single_cap_array;
  volatile struct double_cap_array double_cap_array;
  volatile struct mixed mixed;
  volatile union int_or_cap int_or_cap;
  __builtin_va_list ap;

  __builtin_va_start(ap, sel);

  switch (sel) {
  case 0:
    null_cap = __builtin_va_arg(ap, void * __capability);
    null_uintcap = __builtin_va_arg(ap, __uintcap_t);
    cap = __builtin_va_arg(ap, void * __capability);
    uintcap = __builtin_va_arg(ap, __uintcap_t);
    break;
  case 1:
    single_cap = __builtin_va_arg(ap, struct single_cap);
    break;
  case 2:
    double_cap = __builtin_va_arg(ap, struct double_cap);
    break;
  case 3:
    single_cap_array = __builtin_va_arg(ap, struct single_cap_array);
    break;
  case 4:
    double_cap_array = __builtin_va_arg(ap, struct double_cap_array);
    break;
  case 5:
    mixed = __builtin_va_arg(ap, struct mixed);
    break;
  case 6:
    int_or_cap = __builtin_va_arg(ap, union int_or_cap);
    break;
  }

  __builtin_va_end(ap);
}
