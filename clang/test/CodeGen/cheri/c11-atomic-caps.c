// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature
// RUN: %riscv64_cheri_cc1 -std=c11 -o - -emit-llvm -disable-O0-optnone -Wno-atomic-alignment %s \
// RUN:   | opt -S -mem2reg | FileCheck --check-prefix=HYBRID %s
// RUN: %riscv64_cheri_purecap_cc1 -std=c11 -o - -emit-llvm -disable-O0-optnone -Wno-atomic-alignment %s \
// RUN:   | opt -S -mem2reg | FileCheck --check-prefix=PURECAP %s

// HYBRID-LABEL: define {{[^@]+}}@test_init
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0:[0-9]+]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    store i8 addrspace(200)* [[VALUE]], i8 addrspace(200)** [[F]], align 16
// HYBRID-NEXT:    ret void
//
// PURECAP-LABEL: define {{[^@]+}}@test_init
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0:[0-9]+]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    store i8 addrspace(200)* [[VALUE]], i8 addrspace(200)* addrspace(200)* [[F]], align 16
// PURECAP-NEXT:    ret void
//
void test_init(_Atomic(__uintcap_t) *f, __uintcap_t value) {
  __c11_atomic_init(f, value);
}

// HYBRID-LABEL: define {{[^@]+}}@test_load
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load atomic i8 addrspace(200)*, i8 addrspace(200)** [[F]] seq_cst, align 16
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_load
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load atomic i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[F]] seq_cst, align 16
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
__uintcap_t test_load(_Atomic(__uintcap_t) *f) {
  return __c11_atomic_load(f, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_store
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    store atomic i8 addrspace(200)* [[VALUE]], i8 addrspace(200)** [[F]] seq_cst, align 16
// HYBRID-NEXT:    ret void
//
// PURECAP-LABEL: define {{[^@]+}}@test_store
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    store atomic i8 addrspace(200)* [[VALUE]], i8 addrspace(200)* addrspace(200)* [[F]] seq_cst, align 16
// PURECAP-NEXT:    ret void
//
void test_store(_Atomic(__uintcap_t) *f, __uintcap_t value) {
  __c11_atomic_store(f, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_xchg
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i8 addrspace(200)** [[F]], i8 addrspace(200)* [[VALUE]] seq_cst, align 16
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_xchg
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[VALUE]] seq_cst, align 16
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
__uintcap_t test_xchg(_Atomic(__uintcap_t) *f, __uintcap_t value) {
  return __c11_atomic_exchange(f, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_xchg_long_ptr
// HYBRID-SAME: (i64 addrspace(200)** [[F:%.*]], i64 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i64 addrspace(200)** [[F]], i64 addrspace(200)* [[VALUE]] seq_cst, align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP0]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_xchg_long_ptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[F:%.*]], i64 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i64 addrspace(200)* addrspace(200)* [[F]], i64 addrspace(200)* [[VALUE]] seq_cst, align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP0]]
//
long *__capability test_xchg_long_ptr(_Atomic(long *__capability) *f, long *__capability value) {
  return __c11_atomic_exchange(f, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_cmpxchg_weak
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]], i8 addrspace(200)** [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    [[TMP1:%.*]] = cmpxchg weak i8 addrspace(200)** [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic, align 16
// HYBRID-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// HYBRID-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// HYBRID-NEXT:    br i1 [[TMP3]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// HYBRID:       cmpxchg.store_expected:
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP2]], i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    br label [[CMPXCHG_CONTINUE]]
// HYBRID:       cmpxchg.continue:
// HYBRID-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TMP3]] to i8
// HYBRID-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[FROMBOOL]] to i1
// HYBRID-NEXT:    ret i1 [[TOBOOL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_weak
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]], i8 addrspace(200)* addrspace(200)* [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    [[TMP1:%.*]] = cmpxchg weak i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic, align 16
// PURECAP-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// PURECAP-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// PURECAP-NEXT:    br i1 [[TMP3]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// PURECAP:       cmpxchg.store_expected:
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    br label [[CMPXCHG_CONTINUE]]
// PURECAP:       cmpxchg.continue:
// PURECAP-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TMP3]] to i8
// PURECAP-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[FROMBOOL]] to i1
// PURECAP-NEXT:    ret i1 [[TOBOOL]]
//
_Bool test_cmpxchg_weak(_Atomic(__uintcap_t) *f, __uintcap_t *exp, __uintcap_t new) {
  return __c11_atomic_compare_exchange_weak(f, exp, new, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
}

// HYBRID-LABEL: define {{[^@]+}}@test_cmpxchg_strong
// HYBRID-SAME: (i8 addrspace(200)** [[F:%.*]], i8 addrspace(200)** [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    [[TMP1:%.*]] = cmpxchg i8 addrspace(200)** [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic, align 16
// HYBRID-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// HYBRID-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// HYBRID-NEXT:    br i1 [[TMP3]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// HYBRID:       cmpxchg.store_expected:
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP2]], i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    br label [[CMPXCHG_CONTINUE]]
// HYBRID:       cmpxchg.continue:
// HYBRID-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TMP3]] to i8
// HYBRID-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[FROMBOOL]] to i1
// HYBRID-NEXT:    ret i1 [[TOBOOL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_strong
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[F:%.*]], i8 addrspace(200)* addrspace(200)* [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    [[TMP1:%.*]] = cmpxchg i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic, align 16
// PURECAP-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// PURECAP-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// PURECAP-NEXT:    br i1 [[TMP3]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// PURECAP:       cmpxchg.store_expected:
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    br label [[CMPXCHG_CONTINUE]]
// PURECAP:       cmpxchg.continue:
// PURECAP-NEXT:    [[FROMBOOL:%.*]] = zext i1 [[TMP3]] to i8
// PURECAP-NEXT:    [[TOBOOL:%.*]] = trunc i8 [[FROMBOOL]] to i1
// PURECAP-NEXT:    ret i1 [[TOBOOL]]
//
_Bool test_cmpxchg_strong(_Atomic(__uintcap_t) *f, __uintcap_t *exp, __uintcap_t new) {
  return __c11_atomic_compare_exchange_strong(f, exp, new, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
}

// Most are only supported for __uintcap_t, but fetch_add/fetch_sub also work
// with pointer types (and multiply the increment by the size of the pointee)

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_add_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_longptr
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 8
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP3:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)** [[TMP2]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP5]], i8 addrspace(200)* [[TMP3]], i32 signext 5)
// HYBRID-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)** [[TMP4]], align 16
// HYBRID-NEXT:    [[TMP6:%.*]] = bitcast i8 addrspace(200)** [[TMP4]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP7:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP6]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP7]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 8
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP3:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP2]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP5]], i8 addrspace(200)* [[TMP3]], i32 signext 5)
// PURECAP-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)* addrspace(200)* [[TMP4]], align 16
// PURECAP-NEXT:    [[TMP6:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP4]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP7:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP6]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP7]]
//
long *__capability test_fetch_add_longptr(_Atomic(long *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_longptr_and_short
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = mul i64 [[CONV]], 8
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)** [[TMP1]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// HYBRID-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)** [[TMP3]], align 16
// HYBRID-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)** [[TMP3]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP6:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP5]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP6]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = mul i64 [[CONV]], 8
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP1]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// PURECAP-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)* addrspace(200)* [[TMP3]], align 16
// PURECAP-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP3]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP6:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP5]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP6]]
//
long *__capability test_fetch_add_longptr_and_short(_Atomic(long *__capability) *ptr, short value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_charptr
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 1
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 1
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_add_charptr(_Atomic(char *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_charptr_and_short
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = mul i64 [[CONV]], 1
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = mul i64 [[CONV]], 1
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_add_charptr_and_short(_Atomic(char *__capability) *ptr, short value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_sub_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_longptr
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 8
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP3:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)** [[TMP2]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP5]], i8 addrspace(200)* [[TMP3]], i32 signext 5)
// HYBRID-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)** [[TMP4]], align 16
// HYBRID-NEXT:    [[TMP6:%.*]] = bitcast i8 addrspace(200)** [[TMP4]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP7:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP6]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP7]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 8
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP3:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP2]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP5]], i8 addrspace(200)* [[TMP3]], i32 signext 5)
// PURECAP-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)* addrspace(200)* [[TMP4]], align 16
// PURECAP-NEXT:    [[TMP6:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP4]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP7:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP6]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP7]]
//
long *__capability test_fetch_sub_longptr(_Atomic(long *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_longptr_and_short
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = mul i64 [[CONV]], 8
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)** [[ATOMIC_TEMP]] to i8 addrspace(200)**
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)** [[TMP1]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// HYBRID-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)** [[TMP3]], align 16
// HYBRID-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)** [[TMP3]] to i64 addrspace(200)**
// HYBRID-NEXT:    [[TMP6:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)** [[TMP5]], align 16
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP6]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[ATOMIC_TEMP:%.*]] = alloca i64 addrspace(200)*, align 16, addrspace(200)
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = mul i64 [[CONV]], 8
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[ATOMIC_TEMP]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP1]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP4]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// PURECAP-NEXT:    store i8 addrspace(200)* [[CALL]], i8 addrspace(200)* addrspace(200)* [[TMP3]], align 16
// PURECAP-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[TMP3]] to i64 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP6:%.*]] = load i64 addrspace(200)*, i64 addrspace(200)* addrspace(200)* [[TMP5]], align 16
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP6]]
//
long *__capability test_fetch_sub_longptr_and_short(_Atomic(long *__capability) *ptr, short value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_charptr
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 1
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 1
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_sub_charptr(_Atomic(char *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_charptr_and_short
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = mul i64 [[CONV]], 1
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = mul i64 [[CONV]], 1
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_sub_charptr_and_short(_Atomic(char *__capability) *ptr, short value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_and_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_and_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_and_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_and_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_and_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_and(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_or_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_or_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_or_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_or_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_or_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_or(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_xor_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_xor_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_xor_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_xor_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_xor_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_xor(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_max_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umax_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_max_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umax_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_max_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_max(ptr, value, __ATOMIC_SEQ_CST);
}

// HYBRID-LABEL: define {{[^@]+}}@test_fetch_min_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) #[[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umin_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// PURECAP-LABEL: define {{[^@]+}}@test_fetch_min_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) addrspace(200) #[[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umin_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5)
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_min_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_min(ptr, value, __ATOMIC_SEQ_CST);
}
