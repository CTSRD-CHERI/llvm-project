// Note: generated by update_cc_test_checks, but manually removed tbaa metadata to share checks between MIPS+RISC-V
// RUN: %cheri_purecap_cc1 -std=c11 -o - -emit-llvm -O1 %s -verify=purecap | FileCheck %s --check-prefix=PURECAP
// RUN: %riscv64_cheri_purecap_cc1 -std=c11 -o - -emit-llvm -O1 %s -verify=purecap | FileCheck %s --check-prefix=PURECAP

// PURECAP-LABEL: define {{[^@]+}}@test_load
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture readonly [[F:%.*]]) local_unnamed_addr addrspace(200) #0
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load atomic i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[F]] seq_cst, align 16
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
__uintcap_t test_load(_Atomic(__uintcap_t) *f) {
  return __c11_atomic_load(f, __ATOMIC_SEQ_CST);
}

// PURECAP-LABEL: define {{[^@]+}}@test_store
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #0
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    store atomic i8 addrspace(200)* [[VALUE]], i8 addrspace(200)* addrspace(200)* [[F]] seq_cst, align 16
// PURECAP-NEXT:    ret void
//
void test_store(_Atomic(__uintcap_t) *f, __uintcap_t value) {
  __c11_atomic_store(f, value, __ATOMIC_SEQ_CST);
}

// PURECAP-LABEL: define {{[^@]+}}@test_init
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #1
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    store i8 addrspace(200)* [[VALUE]], i8 addrspace(200)* addrspace(200)* [[F]], align 16
// PURECAP-NEXT:    ret void
//
void test_init(_Atomic(__uintcap_t) *f, __uintcap_t value) {
  __c11_atomic_init(f, value);
}

// PURECAP-LABEL: define {{[^@]+}}@test_xchg
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #0
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[VALUE]] seq_cst
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
__uintcap_t test_xchg(_Atomic(__uintcap_t) *f, __uintcap_t value) {
  return __c11_atomic_exchange(f, value, __ATOMIC_SEQ_CST);
}

// PURECAP-LABEL: define {{[^@]+}}@test_xchg_long_ptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i64 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #0
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[F]] to i8 addrspace(200)* addrspace(200)*
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i64 addrspace(200)* [[VALUE]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[TMP2:%.*]] = atomicrmw xchg i8 addrspace(200)* addrspace(200)* [[TMP0]], i8 addrspace(200)* [[TMP1]] seq_cst
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[TMP2]] to i64 addrspace(200)*
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP3]]
//
long *__capability test_xchg_long_ptr(_Atomic(long *__capability) *f, long *__capability value) {
  return __c11_atomic_exchange(f, value, __ATOMIC_SEQ_CST);
}

// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_weak
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* addrspace(200)* nocapture [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) local_unnamed_addr addrspace(200) #0
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    [[TMP1:%.*]] = cmpxchg weak i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic
// PURECAP-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// PURECAP-NEXT:    br i1 [[TMP2]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// PURECAP:       cmpxchg.store_expected:
// PURECAP-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    br label [[CMPXCHG_CONTINUE]]
// PURECAP:       cmpxchg.continue:
// PURECAP-NEXT:    ret i1 [[TMP2]]
//
_Bool test_cmpxchg_weak(_Atomic(__uintcap_t) *f, __uintcap_t *exp, __uintcap_t new) {
  return __c11_atomic_compare_exchange_weak(f, exp, new, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
}
// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_strong
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* addrspace(200)* nocapture [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) local_unnamed_addr addrspace(200) #0
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    [[TMP1:%.*]] = cmpxchg i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic
// PURECAP-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// PURECAP-NEXT:    br i1 [[TMP2]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// PURECAP:       cmpxchg.store_expected:
// PURECAP-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    br label [[CMPXCHG_CONTINUE]]
// PURECAP:       cmpxchg.continue:
// PURECAP-NEXT:    ret i1 [[TMP2]]
//
_Bool test_cmpxchg_strong(_Atomic(__uintcap_t) *f, __uintcap_t *exp, __uintcap_t new) {
  return __c11_atomic_compare_exchange_strong(f, exp, new, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
}

// Most are only supported for __uintcap_t, but fetch_add/fetch_sub also work
// with pointer types (and multiply the increment by the size of the pointee)

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_add_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = shl i64 [[TMP0]], 3
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5) #4
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP4]]
//
long *test_fetch_add_longptr(_Atomic(long *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = shl nsw i64 [[CONV]], 3
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) #4
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP3]]
//
long *test_fetch_add_longptr_and_short(_Atomic(long *__capability) *ptr, short value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *test_fetch_add_charptr(_Atomic(char *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *test_fetch_add_charptr_and_short(_Atomic(char *__capability) *ptr, short value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_sub_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = shl i64 [[TMP0]], 3
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5) #4
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP4]]
//
long *test_fetch_sub_longptr(_Atomic(long *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = shl nsw i64 [[CONV]], 3
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) #4
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP3]]
//
long *test_fetch_sub_longptr_and_short(_Atomic(long *) *ptr, short value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *test_fetch_sub_charptr(_Atomic(char *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *test_fetch_sub_charptr_and_short(_Atomic(char *__capability) *ptr, short value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_and_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_and_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_and_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_and(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_or_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_or_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_or_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_or(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_xor_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_xor_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_xor_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_xor(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_max_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umax_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_max_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_max(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_min_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) #2
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umin_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) #4
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_min_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_min(ptr, value, __ATOMIC_SEQ_CST);
  // purecap-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}
