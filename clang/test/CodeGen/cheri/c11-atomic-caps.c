// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature
// RUN: %cheri_purecap_cc1 -std=c11 -o - -emit-llvm -O1 %s -verify=expected,purecap | FileCheck %s --check-prefixes=PURECAP,PURECAP-MIPS64
// RUN: %riscv64_cheri_purecap_cc1 -std=c11 -o - -emit-llvm -O1 %s -verify=expected,purecap | FileCheck %s --check-prefixes=PURECAP,PURECAP-RISCV64
// RUN: %cheri_cc1 -std=c11 -o - -emit-llvm -O1 %s -verify=expected,hybrid | FileCheck %s --check-prefixes=HYBRID,HYBRID-MIPS64
// RUN: %riscv64_cheri_cc1 -std=c11 -o - -emit-llvm -O1 -verify=expected,hybrid %s | FileCheck %s --check-prefixes=HYBRID,HYBRID-RISCV64

// PURECAP-LABEL: define {{[^@]+}}@test_load
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture readonly [[F:%.*]]) local_unnamed_addr addrspace(200) [[ATTR0:#.*]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load atomic i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[F]] seq_cst, align 16
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_load
// HYBRID-SAME: (i8 addrspace(200)** nocapture readonly [[F:%.*]]) local_unnamed_addr [[ATTR0:#.*]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load atomic i8 addrspace(200)*, i8 addrspace(200)** [[F]] seq_cst, align 16
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
__uintcap_t test_load(_Atomic(__uintcap_t) *f) {
  return __c11_atomic_load(f, __ATOMIC_SEQ_CST);
}

// PURECAP-LABEL: define {{[^@]+}}@test_store
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    store atomic i8 addrspace(200)* [[VALUE]], i8 addrspace(200)* addrspace(200)* [[F]] seq_cst, align 16
// PURECAP-NEXT:    ret void
//
// HYBRID-LABEL: define {{[^@]+}}@test_store
// HYBRID-SAME: (i8 addrspace(200)** nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    store atomic i8 addrspace(200)* [[VALUE]], i8 addrspace(200)** [[F]] seq_cst, align 16
// HYBRID-NEXT:    ret void
//
void test_store(_Atomic(__uintcap_t) *f, __uintcap_t value) {
  __c11_atomic_store(f, value, __ATOMIC_SEQ_CST);
}

// PURECAP-MIPS64-LABEL: define {{[^@]+}}@test_init
// PURECAP-MIPS64-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR1:#.*]] {
// PURECAP-MIPS64-NEXT:  entry:
// PURECAP-MIPS64-NEXT:    store i8 addrspace(200)* [[VALUE]], i8 addrspace(200)* addrspace(200)* [[F]], align 16, [[TBAA2:!tbaa !.*]]
// PURECAP-MIPS64-NEXT:    ret void
//
// PURECAP-RISCV64-LABEL: define {{[^@]+}}@test_init
// PURECAP-RISCV64-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR1:#.*]] {
// PURECAP-RISCV64-NEXT:  entry:
// PURECAP-RISCV64-NEXT:    store i8 addrspace(200)* [[VALUE]], i8 addrspace(200)* addrspace(200)* [[F]], align 16, [[TBAA4:!tbaa !.*]]
// PURECAP-RISCV64-NEXT:    ret void
//
// HYBRID-MIPS64-LABEL: define {{[^@]+}}@test_init
// HYBRID-MIPS64-SAME: (i8 addrspace(200)** nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR1:#.*]] {
// HYBRID-MIPS64-NEXT:  entry:
// HYBRID-MIPS64-NEXT:    store i8 addrspace(200)* [[VALUE]], i8 addrspace(200)** [[F]], align 16, [[TBAA2:!tbaa !.*]]
// HYBRID-MIPS64-NEXT:    ret void
//
// HYBRID-RISCV64-LABEL: define {{[^@]+}}@test_init
// HYBRID-RISCV64-SAME: (i8 addrspace(200)** nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR1:#.*]] {
// HYBRID-RISCV64-NEXT:  entry:
// HYBRID-RISCV64-NEXT:    store i8 addrspace(200)* [[VALUE]], i8 addrspace(200)** [[F]], align 16, [[TBAA4:!tbaa !.*]]
// HYBRID-RISCV64-NEXT:    ret void
//
void test_init(_Atomic(__uintcap_t) *f, __uintcap_t value) {
  __c11_atomic_init(f, value);
}

// PURECAP-LABEL: define {{[^@]+}}@test_xchg
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[VALUE]] seq_cst
// PURECAP-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_xchg
// HYBRID-SAME: (i8 addrspace(200)** nocapture [[F:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i8 addrspace(200)** [[F]], i8 addrspace(200)* [[VALUE]] seq_cst
// HYBRID-NEXT:    ret i8 addrspace(200)* [[TMP0]]
//
__uintcap_t test_xchg(_Atomic(__uintcap_t) *f, __uintcap_t value) {
  return __c11_atomic_exchange(f, value, __ATOMIC_SEQ_CST);
}

// PURECAP-LABEL: define {{[^@]+}}@test_xchg_long_ptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i64 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i64 addrspace(200)* addrspace(200)* [[F]], i64 addrspace(200)* [[VALUE]] seq_cst
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP0]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_xchg_long_ptr
// HYBRID-SAME: (i64 addrspace(200)** nocapture [[F:%.*]], i64 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = atomicrmw xchg i64 addrspace(200)** [[F]], i64 addrspace(200)* [[VALUE]] seq_cst
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP0]]
//
long *__capability test_xchg_long_ptr(_Atomic(long *__capability) *f, long *__capability value) {
  return __c11_atomic_exchange(f, value, __ATOMIC_SEQ_CST);
}

// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_weak
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* addrspace(200)* nocapture [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) local_unnamed_addr addrspace(200) [[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    [[TMP1:%.*]] = cmpxchg weak i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic
// PURECAP-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// PURECAP-NEXT:    br i1 [[TMP2]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// PURECAP:       cmpxchg.store_expected:
// PURECAP-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    br label [[CMPXCHG_CONTINUE]]
// PURECAP:       cmpxchg.continue:
// PURECAP-NEXT:    ret i1 [[TMP2]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_cmpxchg_weak
// HYBRID-SAME: (i8 addrspace(200)** nocapture [[F:%.*]], i8 addrspace(200)** nocapture [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) local_unnamed_addr [[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    [[TMP1:%.*]] = cmpxchg weak i8 addrspace(200)** [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic
// HYBRID-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// HYBRID-NEXT:    br i1 [[TMP2]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// HYBRID:       cmpxchg.store_expected:
// HYBRID-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP3]], i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    br label [[CMPXCHG_CONTINUE]]
// HYBRID:       cmpxchg.continue:
// HYBRID-NEXT:    ret i1 [[TMP2]]
//
_Bool test_cmpxchg_weak(_Atomic(__uintcap_t) *f, __uintcap_t *exp, __uintcap_t new) {
  return __c11_atomic_compare_exchange_weak(f, exp, new, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
}
// PURECAP-LABEL: define {{[^@]+}}@test_cmpxchg_strong
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* nocapture [[F:%.*]], i8 addrspace(200)* addrspace(200)* nocapture [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) local_unnamed_addr addrspace(200) [[ATTR0]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    [[TMP1:%.*]] = cmpxchg i8 addrspace(200)* addrspace(200)* [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic
// PURECAP-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// PURECAP-NEXT:    br i1 [[TMP2]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// PURECAP:       cmpxchg.store_expected:
// PURECAP-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// PURECAP-NEXT:    store i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* addrspace(200)* [[EXP]], align 16
// PURECAP-NEXT:    br label [[CMPXCHG_CONTINUE]]
// PURECAP:       cmpxchg.continue:
// PURECAP-NEXT:    ret i1 [[TMP2]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_cmpxchg_strong
// HYBRID-SAME: (i8 addrspace(200)** nocapture [[F:%.*]], i8 addrspace(200)** nocapture [[EXP:%.*]], i8 addrspace(200)* [[NEW:%.*]]) local_unnamed_addr [[ATTR0]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = load i8 addrspace(200)*, i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    [[TMP1:%.*]] = cmpxchg i8 addrspace(200)** [[F]], i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[NEW]] monotonic monotonic
// HYBRID-NEXT:    [[TMP2:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 1
// HYBRID-NEXT:    br i1 [[TMP2]], label [[CMPXCHG_CONTINUE:%.*]], label [[CMPXCHG_STORE_EXPECTED:%.*]]
// HYBRID:       cmpxchg.store_expected:
// HYBRID-NEXT:    [[TMP3:%.*]] = extractvalue { i8 addrspace(200)*, i1 } [[TMP1]], 0
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP3]], i8 addrspace(200)** [[EXP]], align 16
// HYBRID-NEXT:    br label [[CMPXCHG_CONTINUE]]
// HYBRID:       cmpxchg.continue:
// HYBRID-NEXT:    ret i1 [[TMP2]]
//
_Bool test_cmpxchg_strong(_Atomic(__uintcap_t) *f, __uintcap_t *exp, __uintcap_t new) {
  return __c11_atomic_compare_exchange_strong(f, exp, new, __ATOMIC_RELAXED, __ATOMIC_RELAXED);
}

// Most are only supported for __uintcap_t, but fetch_add/fetch_sub also work
// with pointer types (and multiply the increment by the size of the pointee)

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2:#.*]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4:#.*]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2:#.*]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4:#.*]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_add_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = shl i64 [[TMP0]], 3
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP4]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_longptr
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = shl i64 [[TMP0]], 3
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP4]]
//
long *__capability test_fetch_add_longptr(_Atomic(long *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = shl nsw i64 [[CONV]], 3
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP3]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_longptr_and_short
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = shl nsw i64 [[CONV]], 3
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP3]]
//
long *__capability test_fetch_add_longptr_and_short(_Atomic(long *__capability) *ptr, short value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_charptr
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_add_charptr(_Atomic(char *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_add_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_add_charptr_and_short
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_add_cap(i8* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_add_charptr_and_short(_Atomic(char *__capability) *ptr, short value) {
  return __c11_atomic_fetch_add(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_sub_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_longptr
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = shl i64 [[TMP0]], 3
// PURECAP-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP4]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_longptr
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = shl i64 [[TMP0]], 3
// HYBRID-NEXT:    [[TMP2:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP1]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP3]], i8 addrspace(200)* [[TMP2]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP4]]
//
long *__capability test_fetch_sub_longptr(_Atomic(long *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_longptr_and_short
// PURECAP-SAME: (i64 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = shl nsw i64 [[CONV]], 3
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// PURECAP-NEXT:    ret i64 addrspace(200)* [[TMP3]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_longptr_and_short
// HYBRID-SAME: (i64 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = shl nsw i64 [[CONV]], 3
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i64 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[CALL]] to i64 addrspace(200)*
// HYBRID-NEXT:    ret i64 addrspace(200)* [[TMP3]]
//
long *__capability test_fetch_sub_longptr_and_short(_Atomic(long *__capability) *ptr, short value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_charptr
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// PURECAP-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// PURECAP-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_charptr
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = call i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)* [[VALUE]])
// HYBRID-NEXT:    [[TMP1:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[TMP0]]
// HYBRID-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP2]], i8 addrspace(200)* [[TMP1]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_sub_charptr(_Atomic(char *__capability) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_sub_charptr_and_short
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// PURECAP-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// PURECAP-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8 addrspace(200)* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_sub_charptr_and_short
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i16 signext [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[CONV:%.*]] = sext i16 [[VALUE]] to i64
// HYBRID-NEXT:    [[TMP0:%.*]] = getelementptr i8, i8 addrspace(200)* null, i64 [[CONV]]
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_sub_cap(i8* [[TMP1]], i8 addrspace(200)* [[TMP0]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
char *__capability test_fetch_sub_charptr_and_short(_Atomic(char *__capability) *ptr, short value) {
  return __c11_atomic_fetch_sub(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_and_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_and_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_and_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_and_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_and_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_and(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_or_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_or_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_or_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_or_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_or_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_or(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_xor_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_xor_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_xor_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_xor_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_xor_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_xor(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_max_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umax_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_max_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umax_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_max_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_max(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}

// PURECAP-LABEL: define {{[^@]+}}@test_fetch_min_uintcap
// PURECAP-SAME: (i8 addrspace(200)* addrspace(200)* [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr addrspace(200) [[ATTR2]] {
// PURECAP-NEXT:  entry:
// PURECAP-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* addrspace(200)* [[PTR]] to i8 addrspace(200)*
// PURECAP-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umin_cap(i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// PURECAP-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
// HYBRID-LABEL: define {{[^@]+}}@test_fetch_min_uintcap
// HYBRID-SAME: (i8 addrspace(200)** [[PTR:%.*]], i8 addrspace(200)* [[VALUE:%.*]]) local_unnamed_addr [[ATTR2]] {
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)** [[PTR]] to i8*
// HYBRID-NEXT:    [[CALL:%.*]] = call i8 addrspace(200)* @__atomic_fetch_umin_cap(i8* [[TMP0]], i8 addrspace(200)* [[VALUE]], i32 signext 5) [[ATTR4]]
// HYBRID-NEXT:    ret i8 addrspace(200)* [[CALL]]
//
__uintcap_t test_fetch_min_uintcap(_Atomic(__uintcap_t) *ptr, __uintcap_t value) {
  return __c11_atomic_fetch_min(ptr, value, __ATOMIC_SEQ_CST);
  // expected-warning@-1{{unsupported atomic operation may incur significant performance penalty}}
}
