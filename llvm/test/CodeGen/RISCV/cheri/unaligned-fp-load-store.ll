; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --extra_scrub
; TODO: Once https://github.com/CTSRD-CHERI/llvm-project/issues/459 has been fixed use +d and il32pc64d
; RUN: llc -mtriple=riscv32 -mattr=+f,+xcheri,+cap-mode -target-abi il32pc64f -verify-machineinstrs < %s \
; RUN:   | %cheri64_FileCheck --check-prefixes=CHECK,RV32IFXCHERI %s --allow-unused-prefixes
; RUN: llc -mtriple=riscv64 -mattr=+f,+xcheri,+cap-mode -target-abi l64pc128f -verify-machineinstrs < %s \
; RUN:   | %cheri128_FileCheck --check-prefixes=CHECK,RV64IFXCHERI %s --allow-unused-prefixes
; Previously expandUnalignedLoad() would trigger assertions for unaligned floating-point values since we were creating
; incorrect iFATPTR constants.
declare i32 @printf(i8 addrspace(200)*, ...) addrspace(200)

define i32 @unaligned_float(float addrspace(200)* nocapture readonly %arg, float %newval) unnamed_addr addrspace(200) nounwind {
; RV32IFXCHERI-LABEL: unaligned_float:
; RV32IFXCHERI:       # %bb.0: # %bb
; RV32IFXCHERI-NEXT:    cincoffset csp, csp, -32
; RV32IFXCHERI-NEXT:    csc cra, 24(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs0, 16(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    cfsw fs0, 12(csp) # 4-byte Folded Spill
; RV32IFXCHERI-NEXT:    fmv.s fs0, fa0
; RV32IFXCHERI-NEXT:    cmove cs0, ca0
; RV32IFXCHERI-NEXT:    clbu a0, 1(ca0)
; RV32IFXCHERI-NEXT:    clbu a1, 0(cs0)
; RV32IFXCHERI-NEXT:    clbu a2, 3(cs0)
; RV32IFXCHERI-NEXT:    clbu a3, 2(cs0)
; RV32IFXCHERI-NEXT:    slli a0, a0, 8
; RV32IFXCHERI-NEXT:    or a0, a0, a1
; RV32IFXCHERI-NEXT:    slli a1, a2, 8
; RV32IFXCHERI-NEXT:    or a1, a1, a3
; RV32IFXCHERI-NEXT:    slli a1, a1, 16
; RV32IFXCHERI-NEXT:    or a0, a1, a0
; RV32IFXCHERI-NEXT:    fmv.w.x fa0, a0
; RV32IFXCHERI-NEXT:    ccall __extendsfdf2
; RV32IFXCHERI-NEXT:    csw a1, 4(csp)
; RV32IFXCHERI-NEXT:    csw a0, 0(csp)
; RV32IFXCHERI-NEXT:    ccall printf
; RV32IFXCHERI-NEXT:    fmv.x.w a1, fs0
; RV32IFXCHERI-NEXT:    srli a2, a1, 24
; RV32IFXCHERI-NEXT:    csb a2, 3(cs0)
; RV32IFXCHERI-NEXT:    srli a2, a1, 16
; RV32IFXCHERI-NEXT:    csb a2, 2(cs0)
; RV32IFXCHERI-NEXT:    srli a2, a1, 8
; RV32IFXCHERI-NEXT:    csb a2, 1(cs0)
; RV32IFXCHERI-NEXT:    csb a1, 0(cs0)
; RV32IFXCHERI-NEXT:    cflw fs0, 12(csp) # 4-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cs0, 16(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cra, 24(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    cincoffset csp, csp, 32
; RV32IFXCHERI-NEXT:    cret
;
; RV64IFXCHERI-LABEL: unaligned_float:
; RV64IFXCHERI:       # %bb.0: # %bb
; RV64IFXCHERI-NEXT:    cincoffset csp, csp, -64
; RV64IFXCHERI-NEXT:    csc cra, 48(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    csc cs0, 32(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    cfsw fs0, 28(csp) # 4-byte Folded Spill
; RV64IFXCHERI-NEXT:    fmv.s fs0, fa0
; RV64IFXCHERI-NEXT:    cmove cs0, ca0
; RV64IFXCHERI-NEXT:    clbu a0, 1(ca0)
; RV64IFXCHERI-NEXT:    clbu a1, 0(cs0)
; RV64IFXCHERI-NEXT:    clb a2, 3(cs0)
; RV64IFXCHERI-NEXT:    clbu a3, 2(cs0)
; RV64IFXCHERI-NEXT:    slli a0, a0, 8
; RV64IFXCHERI-NEXT:    or a0, a0, a1
; RV64IFXCHERI-NEXT:    slli a1, a2, 8
; RV64IFXCHERI-NEXT:    or a1, a1, a3
; RV64IFXCHERI-NEXT:    slli a1, a1, 16
; RV64IFXCHERI-NEXT:    or a0, a1, a0
; RV64IFXCHERI-NEXT:    csw a0, 8(csp)
; RV64IFXCHERI-NEXT:    cflw fa0, 8(csp)
; RV64IFXCHERI-NEXT:    ccall __extendsfdf2
; RV64IFXCHERI-NEXT:    csd a0, 0(csp)
; RV64IFXCHERI-NEXT:    ccall printf
; RV64IFXCHERI-NEXT:    cfsw fs0, 16(csp)
; RV64IFXCHERI-NEXT:    clw a1, 16(csp)
; RV64IFXCHERI-NEXT:    csb a1, 0(cs0)
; RV64IFXCHERI-NEXT:    srli a2, a1, 24
; RV64IFXCHERI-NEXT:    csb a2, 3(cs0)
; RV64IFXCHERI-NEXT:    srli a2, a1, 16
; RV64IFXCHERI-NEXT:    csb a2, 2(cs0)
; RV64IFXCHERI-NEXT:    srli a1, a1, 8
; RV64IFXCHERI-NEXT:    csb a1, 1(cs0)
; RV64IFXCHERI-NEXT:    cflw fs0, 28(csp) # 4-byte Folded Reload
; RV64IFXCHERI-NEXT:    clc cs0, 32(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    clc cra, 48(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    cincoffset csp, csp, 64
; RV64IFXCHERI-NEXT:    cret
bb:
  %unaligned_load = load float, float addrspace(200)* %arg, align 1
  %ext = fpext float %unaligned_load to double
  %ret = tail call i32 (i8 addrspace(200)*, ...) @printf(i8 addrspace(200)* nonnull dereferenceable(1) undef, double %ext)
  store float %newval, float addrspace(200)* %arg, align 1
  ret i32 %ret
}

define i32 @unaligned_double(double addrspace(200)* nocapture readonly %arg, double %newval) unnamed_addr addrspace(200) nounwind {
; RV32IFXCHERI-LABEL: unaligned_double:
; RV32IFXCHERI:       # %bb.0: # %bb
; RV32IFXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IFXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    mv s0, a2
; RV32IFXCHERI-NEXT:    mv s2, a1
; RV32IFXCHERI-NEXT:    cmove cs1, ca0
; RV32IFXCHERI-NEXT:    clhu a0, 2(ca0)
; RV32IFXCHERI-NEXT:    clhu a1, 0(cs1)
; RV32IFXCHERI-NEXT:    clhu a2, 6(cs1)
; RV32IFXCHERI-NEXT:    clhu a3, 4(cs1)
; RV32IFXCHERI-NEXT:    slli a0, a0, 16
; RV32IFXCHERI-NEXT:    or a0, a0, a1
; RV32IFXCHERI-NEXT:    slli a1, a2, 16
; RV32IFXCHERI-NEXT:    or a1, a1, a3
; RV32IFXCHERI-NEXT:    csw a1, 4(csp)
; RV32IFXCHERI-NEXT:    csw a0, 0(csp)
; RV32IFXCHERI-NEXT:    ccall printf
; RV32IFXCHERI-NEXT:    srli a1, s0, 16
; RV32IFXCHERI-NEXT:    csh a1, 6(cs1)
; RV32IFXCHERI-NEXT:    csh s0, 4(cs1)
; RV32IFXCHERI-NEXT:    srli a1, s2, 16
; RV32IFXCHERI-NEXT:    csh a1, 2(cs1)
; RV32IFXCHERI-NEXT:    csh s2, 0(cs1)
; RV32IFXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IFXCHERI-NEXT:    cret
;
; RV64IFXCHERI-LABEL: unaligned_double:
; RV64IFXCHERI:       # %bb.0: # %bb
; RV64IFXCHERI-NEXT:    cincoffset csp, csp, -64
; RV64IFXCHERI-NEXT:    csc cra, 48(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    csc cs0, 32(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    csc cs1, 16(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    mv s0, a1
; RV64IFXCHERI-NEXT:    cmove cs1, ca0
; RV64IFXCHERI-NEXT:    clhu a0, 2(ca0)
; RV64IFXCHERI-NEXT:    clhu a1, 0(cs1)
; RV64IFXCHERI-NEXT:    clhu a2, 6(cs1)
; RV64IFXCHERI-NEXT:    clhu a3, 4(cs1)
; RV64IFXCHERI-NEXT:    slli a0, a0, 16
; RV64IFXCHERI-NEXT:    or a0, a0, a1
; RV64IFXCHERI-NEXT:    slli a1, a2, 16
; RV64IFXCHERI-NEXT:    or a1, a1, a3
; RV64IFXCHERI-NEXT:    slli a1, a1, 32
; RV64IFXCHERI-NEXT:    or a0, a1, a0
; RV64IFXCHERI-NEXT:    csd a0, 0(csp)
; RV64IFXCHERI-NEXT:    ccall printf
; RV64IFXCHERI-NEXT:    srli a1, s0, 48
; RV64IFXCHERI-NEXT:    csh a1, 6(cs1)
; RV64IFXCHERI-NEXT:    srli a1, s0, 32
; RV64IFXCHERI-NEXT:    csh a1, 4(cs1)
; RV64IFXCHERI-NEXT:    srli a1, s0, 16
; RV64IFXCHERI-NEXT:    csh a1, 2(cs1)
; RV64IFXCHERI-NEXT:    csh s0, 0(cs1)
; RV64IFXCHERI-NEXT:    clc cs1, 16(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    clc cs0, 32(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    clc cra, 48(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    cincoffset csp, csp, 64
; RV64IFXCHERI-NEXT:    cret
bb:
  %unaligned_load = load double, double addrspace(200)* %arg, align 2
  %ret = tail call i32 (i8 addrspace(200)*, ...) @printf(i8 addrspace(200)* nonnull dereferenceable(1) undef, double %unaligned_load)
  store double %newval, double addrspace(200)* %arg, align 2
  ret i32 %ret
}

define i32 @unaligned_fp128(fp128 addrspace(200)* nocapture readonly %arg, fp128 %newval) unnamed_addr addrspace(200) nounwind {
; RV32IFXCHERI-LABEL: unaligned_fp128:
; RV32IFXCHERI:       # %bb.0: # %bb
; RV32IFXCHERI-NEXT:    cincoffset csp, csp, -80
; RV32IFXCHERI-NEXT:    csc cra, 72(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs0, 64(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs1, 56(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs2, 48(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs3, 40(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs4, 32(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    cmove cs0, ca0
; RV32IFXCHERI-NEXT:    clw s2, 0(ca1)
; RV32IFXCHERI-NEXT:    clw s3, 4(ca1)
; RV32IFXCHERI-NEXT:    clw s4, 8(ca1)
; RV32IFXCHERI-NEXT:    clw s1, 12(ca1)
; RV32IFXCHERI-NEXT:    clw a1, 0(ca0)
; RV32IFXCHERI-NEXT:    clw a0, 4(ca0)
; RV32IFXCHERI-NEXT:    clw a2, 8(cs0)
; RV32IFXCHERI-NEXT:    clw a3, 12(cs0)
; RV32IFXCHERI-NEXT:    csw a3, 28(csp)
; RV32IFXCHERI-NEXT:    csw a2, 24(csp)
; RV32IFXCHERI-NEXT:    csw a0, 20(csp)
; RV32IFXCHERI-NEXT:    cincoffset ca0, csp, 16
; RV32IFXCHERI-NEXT:    csw a1, 16(csp)
; RV32IFXCHERI-NEXT:    ccall __trunctfdf2
; RV32IFXCHERI-NEXT:    csw a1, 4(csp)
; RV32IFXCHERI-NEXT:    csw a0, 0(csp)
; RV32IFXCHERI-NEXT:    ccall printf
; RV32IFXCHERI-NEXT:    csw s1, 12(cs0)
; RV32IFXCHERI-NEXT:    csw s4, 8(cs0)
; RV32IFXCHERI-NEXT:    csw s3, 4(cs0)
; RV32IFXCHERI-NEXT:    csw s2, 0(cs0)
; RV32IFXCHERI-NEXT:    clc cs4, 32(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cs3, 40(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cs2, 48(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cs1, 56(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cs0, 64(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cra, 72(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    cincoffset csp, csp, 80
; RV32IFXCHERI-NEXT:    cret
;
; RV64IFXCHERI-LABEL: unaligned_fp128:
; RV64IFXCHERI:       # %bb.0: # %bb
; RV64IFXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IFXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    mv s0, a2
; RV64IFXCHERI-NEXT:    mv s2, a1
; RV64IFXCHERI-NEXT:    cmove cs1, ca0
; RV64IFXCHERI-NEXT:    clwu a0, 4(ca0)
; RV64IFXCHERI-NEXT:    clwu a1, 0(cs1)
; RV64IFXCHERI-NEXT:    clwu a2, 12(cs1)
; RV64IFXCHERI-NEXT:    clwu a3, 8(cs1)
; RV64IFXCHERI-NEXT:    slli a0, a0, 32
; RV64IFXCHERI-NEXT:    or a0, a0, a1
; RV64IFXCHERI-NEXT:    slli a1, a2, 32
; RV64IFXCHERI-NEXT:    or a1, a1, a3
; RV64IFXCHERI-NEXT:    ccall __trunctfdf2
; RV64IFXCHERI-NEXT:    csd a0, 0(csp)
; RV64IFXCHERI-NEXT:    ccall printf
; RV64IFXCHERI-NEXT:    srli a1, s0, 32
; RV64IFXCHERI-NEXT:    csw a1, 12(cs1)
; RV64IFXCHERI-NEXT:    csw s0, 8(cs1)
; RV64IFXCHERI-NEXT:    srli a1, s2, 32
; RV64IFXCHERI-NEXT:    csw a1, 4(cs1)
; RV64IFXCHERI-NEXT:    csw s2, 0(cs1)
; RV64IFXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IFXCHERI-NEXT:    cret
bb:
  %unaligned_load = load fp128, fp128 addrspace(200)* %arg, align 4
  %trunc = fptrunc fp128 %unaligned_load to double
  %ret = tail call i32 (i8 addrspace(200)*, ...) @printf(i8 addrspace(200)* nonnull dereferenceable(1) undef, double %trunc)
  store fp128 %newval, fp128 addrspace(200)* %arg, align 4
  ret i32 %ret
}

define i32 @unaligned_int(i32 addrspace(200)* nocapture readonly %arg) unnamed_addr addrspace(200) nounwind {
; RV32IFXCHERI-LABEL: unaligned_int:
; RV32IFXCHERI:       # %bb.0: # %bb
; RV32IFXCHERI-NEXT:    cincoffset csp, csp, -32
; RV32IFXCHERI-NEXT:    csc cra, 24(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    csc cs0, 16(csp) # 8-byte Folded Spill
; RV32IFXCHERI-NEXT:    cmove cs0, ca0
; RV32IFXCHERI-NEXT:    clbu a0, 1(ca0)
; RV32IFXCHERI-NEXT:    clbu a1, 0(cs0)
; RV32IFXCHERI-NEXT:    clbu a2, 3(cs0)
; RV32IFXCHERI-NEXT:    clbu a3, 2(cs0)
; RV32IFXCHERI-NEXT:    slli a0, a0, 8
; RV32IFXCHERI-NEXT:    or a0, a0, a1
; RV32IFXCHERI-NEXT:    slli a1, a2, 8
; RV32IFXCHERI-NEXT:    or a1, a1, a3
; RV32IFXCHERI-NEXT:    slli a1, a1, 16
; RV32IFXCHERI-NEXT:    or a0, a1, a0
; RV32IFXCHERI-NEXT:    csw a0, 0(csp)
; RV32IFXCHERI-NEXT:    ccall printf
; RV32IFXCHERI-NEXT:    csb zero, 3(cs0)
; RV32IFXCHERI-NEXT:    csb zero, 2(cs0)
; RV32IFXCHERI-NEXT:    csb zero, 1(cs0)
; RV32IFXCHERI-NEXT:    addi a1, zero, 1
; RV32IFXCHERI-NEXT:    csb a1, 0(cs0)
; RV32IFXCHERI-NEXT:    clc cs0, 16(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    clc cra, 24(csp) # 8-byte Folded Reload
; RV32IFXCHERI-NEXT:    cincoffset csp, csp, 32
; RV32IFXCHERI-NEXT:    cret
;
; RV64IFXCHERI-LABEL: unaligned_int:
; RV64IFXCHERI:       # %bb.0: # %bb
; RV64IFXCHERI-NEXT:    cincoffset csp, csp, -48
; RV64IFXCHERI-NEXT:    csc cra, 32(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    csc cs0, 16(csp) # 16-byte Folded Spill
; RV64IFXCHERI-NEXT:    cmove cs0, ca0
; RV64IFXCHERI-NEXT:    clbu a0, 1(ca0)
; RV64IFXCHERI-NEXT:    clbu a1, 0(cs0)
; RV64IFXCHERI-NEXT:    clb a2, 3(cs0)
; RV64IFXCHERI-NEXT:    clbu a3, 2(cs0)
; RV64IFXCHERI-NEXT:    slli a0, a0, 8
; RV64IFXCHERI-NEXT:    or a0, a0, a1
; RV64IFXCHERI-NEXT:    slli a1, a2, 8
; RV64IFXCHERI-NEXT:    or a1, a1, a3
; RV64IFXCHERI-NEXT:    slli a1, a1, 16
; RV64IFXCHERI-NEXT:    or a0, a1, a0
; RV64IFXCHERI-NEXT:    csd a0, 0(csp)
; RV64IFXCHERI-NEXT:    ccall printf
; RV64IFXCHERI-NEXT:    csb zero, 3(cs0)
; RV64IFXCHERI-NEXT:    csb zero, 2(cs0)
; RV64IFXCHERI-NEXT:    csb zero, 1(cs0)
; RV64IFXCHERI-NEXT:    addi a1, zero, 1
; RV64IFXCHERI-NEXT:    csb a1, 0(cs0)
; RV64IFXCHERI-NEXT:    clc cs0, 16(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    clc cra, 32(csp) # 16-byte Folded Reload
; RV64IFXCHERI-NEXT:    cincoffset csp, csp, 48
; RV64IFXCHERI-NEXT:    cret
bb:
  %unaligned_load = load i32, i32 addrspace(200)* %arg, align 1
  %ret = tail call i32 (i8 addrspace(200)*, ...) @printf(i8 addrspace(200)* nonnull dereferenceable(1) undef, i32 %unaligned_load)
  store i32 1, i32 addrspace(200)* %arg, align 1
  ret i32 %ret
}
