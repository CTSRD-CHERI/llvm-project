; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: %riscv32_cheri_purecap_llc -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV32IXCHERI %s
; RUN: %riscv32_cheri_purecap_llc -mattr=+a -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV32IAXCHERI %s
; RUN: %riscv64_cheri_purecap_llc -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV64IXCHERI %s
; RUN: %riscv64_cheri_purecap_llc -mattr=+a -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV64IAXCHERI %s

define i8 @atomic_load_i8_unordered(i8 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i8_unordered:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 0
; RV32IXCHERI-NEXT:    ccall __atomic_load_1
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i8_unordered:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    lb a0, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i8_unordered:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 0
; RV64IXCHERI-NEXT:    ccall __atomic_load_1
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i8_unordered:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    lb a0, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i8, i8 addrspace(200)* %a unordered, align 1
  ret i8 %1
}

define i8 @atomic_load_i8_monotonic(i8 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 0
; RV32IXCHERI-NEXT:    ccall __atomic_load_1
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    lb a0, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 0
; RV64IXCHERI-NEXT:    ccall __atomic_load_1
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    lb a0, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i8, i8 addrspace(200)* %a monotonic, align 1
  ret i8 %1
}

define i8 @atomic_load_i8_acquire(i8 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 2
; RV32IXCHERI-NEXT:    ccall __atomic_load_1
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    lb a0, 0(ca0)
; RV32IAXCHERI-NEXT:    fence r, rw
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 2
; RV64IXCHERI-NEXT:    ccall __atomic_load_1
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    lb a0, 0(ca0)
; RV64IAXCHERI-NEXT:    fence r, rw
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i8, i8 addrspace(200)* %a acquire, align 1
  ret i8 %1
}

define i8 @atomic_load_i8_seq_cst(i8 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 5
; RV32IXCHERI-NEXT:    ccall __atomic_load_1
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    fence rw, rw
; RV32IAXCHERI-NEXT:    lb a0, 0(ca0)
; RV32IAXCHERI-NEXT:    fence r, rw
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 5
; RV64IXCHERI-NEXT:    ccall __atomic_load_1
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, rw
; RV64IAXCHERI-NEXT:    lb a0, 0(ca0)
; RV64IAXCHERI-NEXT:    fence r, rw
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i8, i8 addrspace(200)* %a seq_cst, align 1
  ret i8 %1
}

define i16 @atomic_load_i16_unordered(i16 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i16_unordered:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 0
; RV32IXCHERI-NEXT:    ccall __atomic_load_2
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i16_unordered:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    lh a0, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i16_unordered:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 0
; RV64IXCHERI-NEXT:    ccall __atomic_load_2
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i16_unordered:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    lh a0, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i16, i16 addrspace(200)* %a unordered, align 2
  ret i16 %1
}

define i16 @atomic_load_i16_monotonic(i16 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 0
; RV32IXCHERI-NEXT:    ccall __atomic_load_2
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    lh a0, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 0
; RV64IXCHERI-NEXT:    ccall __atomic_load_2
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    lh a0, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i16, i16 addrspace(200)* %a monotonic, align 2
  ret i16 %1
}

define i16 @atomic_load_i16_acquire(i16 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 2
; RV32IXCHERI-NEXT:    ccall __atomic_load_2
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    lh a0, 0(ca0)
; RV32IAXCHERI-NEXT:    fence r, rw
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 2
; RV64IXCHERI-NEXT:    ccall __atomic_load_2
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    lh a0, 0(ca0)
; RV64IAXCHERI-NEXT:    fence r, rw
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i16, i16 addrspace(200)* %a acquire, align 2
  ret i16 %1
}

define i16 @atomic_load_i16_seq_cst(i16 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 5
; RV32IXCHERI-NEXT:    ccall __atomic_load_2
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    fence rw, rw
; RV32IAXCHERI-NEXT:    lh a0, 0(ca0)
; RV32IAXCHERI-NEXT:    fence r, rw
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 5
; RV64IXCHERI-NEXT:    ccall __atomic_load_2
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, rw
; RV64IAXCHERI-NEXT:    lh a0, 0(ca0)
; RV64IAXCHERI-NEXT:    fence r, rw
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i16, i16 addrspace(200)* %a seq_cst, align 2
  ret i16 %1
}

define i32 @atomic_load_i32_unordered(i32 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i32_unordered:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 0
; RV32IXCHERI-NEXT:    ccall __atomic_load_4
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i32_unordered:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    lw a0, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i32_unordered:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 0
; RV64IXCHERI-NEXT:    ccall __atomic_load_4
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i32_unordered:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    lw a0, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i32, i32 addrspace(200)* %a unordered, align 4
  ret i32 %1
}

define i32 @atomic_load_i32_monotonic(i32 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 0
; RV32IXCHERI-NEXT:    ccall __atomic_load_4
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    lw a0, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 0
; RV64IXCHERI-NEXT:    ccall __atomic_load_4
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    lw a0, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i32, i32 addrspace(200)* %a monotonic, align 4
  ret i32 %1
}

define i32 @atomic_load_i32_acquire(i32 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 2
; RV32IXCHERI-NEXT:    ccall __atomic_load_4
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    lw a0, 0(ca0)
; RV32IAXCHERI-NEXT:    fence r, rw
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 2
; RV64IXCHERI-NEXT:    ccall __atomic_load_4
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    lw a0, 0(ca0)
; RV64IAXCHERI-NEXT:    fence r, rw
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i32, i32 addrspace(200)* %a acquire, align 4
  ret i32 %1
}

define i32 @atomic_load_i32_seq_cst(i32 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 5
; RV32IXCHERI-NEXT:    ccall __atomic_load_4
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    fence rw, rw
; RV32IAXCHERI-NEXT:    lw a0, 0(ca0)
; RV32IAXCHERI-NEXT:    fence r, rw
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 5
; RV64IXCHERI-NEXT:    ccall __atomic_load_4
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, rw
; RV64IAXCHERI-NEXT:    lw a0, 0(ca0)
; RV64IAXCHERI-NEXT:    fence r, rw
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i32, i32 addrspace(200)* %a seq_cst, align 4
  ret i32 %1
}

define i64 @atomic_load_i64_unordered(i64 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i64_unordered:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 0
; RV32IXCHERI-NEXT:    ccall __atomic_load_8
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i64_unordered:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    li a1, 0
; RV32IAXCHERI-NEXT:    ccall __atomic_load_8
; RV32IAXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i64_unordered:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 0
; RV64IXCHERI-NEXT:    ccall __atomic_load_8
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i64_unordered:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    ld a0, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i64, i64 addrspace(200)* %a unordered, align 8
  ret i64 %1
}

define i64 @atomic_load_i64_monotonic(i64 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 0
; RV32IXCHERI-NEXT:    ccall __atomic_load_8
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    li a1, 0
; RV32IAXCHERI-NEXT:    ccall __atomic_load_8
; RV32IAXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 0
; RV64IXCHERI-NEXT:    ccall __atomic_load_8
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    ld a0, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i64, i64 addrspace(200)* %a monotonic, align 8
  ret i64 %1
}

define i64 @atomic_load_i64_acquire(i64 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 2
; RV32IXCHERI-NEXT:    ccall __atomic_load_8
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    li a1, 2
; RV32IAXCHERI-NEXT:    ccall __atomic_load_8
; RV32IAXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 2
; RV64IXCHERI-NEXT:    ccall __atomic_load_8
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    ld a0, 0(ca0)
; RV64IAXCHERI-NEXT:    fence r, rw
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i64, i64 addrspace(200)* %a acquire, align 8
  ret i64 %1
}

define i64 @atomic_load_i64_seq_cst(i64 addrspace(200)* %a) nounwind {
; RV32IXCHERI-LABEL: atomic_load_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a1, 5
; RV32IXCHERI-NEXT:    ccall __atomic_load_8
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_load_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    li a1, 5
; RV32IAXCHERI-NEXT:    ccall __atomic_load_8
; RV32IAXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_load_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a1, 5
; RV64IXCHERI-NEXT:    ccall __atomic_load_8
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_load_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, rw
; RV64IAXCHERI-NEXT:    ld a0, 0(ca0)
; RV64IAXCHERI-NEXT:    fence r, rw
; RV64IAXCHERI-NEXT:    ret
  %1 = load atomic i64, i64 addrspace(200)* %a seq_cst, align 8
  ret i64 %1
}

define void @atomic_store_i8_unordered(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i8_unordered:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 0
; RV32IXCHERI-NEXT:    ccall __atomic_store_1
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i8_unordered:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    sb a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i8_unordered:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 0
; RV64IXCHERI-NEXT:    ccall __atomic_store_1
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i8_unordered:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    sb a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i8 %b, i8 addrspace(200)* %a unordered, align 1
  ret void
}

define void @atomic_store_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 0
; RV32IXCHERI-NEXT:    ccall __atomic_store_1
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    sb a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 0
; RV64IXCHERI-NEXT:    ccall __atomic_store_1
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    sb a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i8 %b, i8 addrspace(200)* %a monotonic, align 1
  ret void
}

define void @atomic_store_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 3
; RV32IXCHERI-NEXT:    ccall __atomic_store_1
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    fence rw, w
; RV32IAXCHERI-NEXT:    sb a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 3
; RV64IXCHERI-NEXT:    ccall __atomic_store_1
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, w
; RV64IAXCHERI-NEXT:    sb a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i8 %b, i8 addrspace(200)* %a release, align 1
  ret void
}

define void @atomic_store_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 5
; RV32IXCHERI-NEXT:    ccall __atomic_store_1
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    fence rw, w
; RV32IAXCHERI-NEXT:    sb a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 5
; RV64IXCHERI-NEXT:    ccall __atomic_store_1
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, w
; RV64IAXCHERI-NEXT:    sb a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i8 %b, i8 addrspace(200)* %a seq_cst, align 1
  ret void
}

define void @atomic_store_i16_unordered(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i16_unordered:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 0
; RV32IXCHERI-NEXT:    ccall __atomic_store_2
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i16_unordered:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    sh a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i16_unordered:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 0
; RV64IXCHERI-NEXT:    ccall __atomic_store_2
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i16_unordered:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    sh a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i16 %b, i16 addrspace(200)* %a unordered, align 2
  ret void
}

define void @atomic_store_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 0
; RV32IXCHERI-NEXT:    ccall __atomic_store_2
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    sh a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 0
; RV64IXCHERI-NEXT:    ccall __atomic_store_2
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    sh a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i16 %b, i16 addrspace(200)* %a monotonic, align 2
  ret void
}

define void @atomic_store_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 3
; RV32IXCHERI-NEXT:    ccall __atomic_store_2
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    fence rw, w
; RV32IAXCHERI-NEXT:    sh a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 3
; RV64IXCHERI-NEXT:    ccall __atomic_store_2
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, w
; RV64IAXCHERI-NEXT:    sh a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i16 %b, i16 addrspace(200)* %a release, align 2
  ret void
}

define void @atomic_store_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 5
; RV32IXCHERI-NEXT:    ccall __atomic_store_2
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    fence rw, w
; RV32IAXCHERI-NEXT:    sh a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 5
; RV64IXCHERI-NEXT:    ccall __atomic_store_2
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, w
; RV64IAXCHERI-NEXT:    sh a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i16 %b, i16 addrspace(200)* %a seq_cst, align 2
  ret void
}

define void @atomic_store_i32_unordered(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i32_unordered:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 0
; RV32IXCHERI-NEXT:    ccall __atomic_store_4
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i32_unordered:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    sw a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i32_unordered:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 0
; RV64IXCHERI-NEXT:    ccall __atomic_store_4
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i32_unordered:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    sw a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i32 %b, i32 addrspace(200)* %a unordered, align 4
  ret void
}

define void @atomic_store_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 0
; RV32IXCHERI-NEXT:    ccall __atomic_store_4
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    sw a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 0
; RV64IXCHERI-NEXT:    ccall __atomic_store_4
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    sw a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i32 %b, i32 addrspace(200)* %a monotonic, align 4
  ret void
}

define void @atomic_store_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 3
; RV32IXCHERI-NEXT:    ccall __atomic_store_4
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    fence rw, w
; RV32IAXCHERI-NEXT:    sw a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 3
; RV64IXCHERI-NEXT:    ccall __atomic_store_4
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, w
; RV64IAXCHERI-NEXT:    sw a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i32 %b, i32 addrspace(200)* %a release, align 4
  ret void
}

define void @atomic_store_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a2, 5
; RV32IXCHERI-NEXT:    ccall __atomic_store_4
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    fence rw, w
; RV32IAXCHERI-NEXT:    sw a1, 0(ca0)
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 5
; RV64IXCHERI-NEXT:    ccall __atomic_store_4
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, w
; RV64IAXCHERI-NEXT:    sw a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i32 %b, i32 addrspace(200)* %a seq_cst, align 4
  ret void
}

define void @atomic_store_i64_unordered(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i64_unordered:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a3, 0
; RV32IXCHERI-NEXT:    ccall __atomic_store_8
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i64_unordered:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    li a3, 0
; RV32IAXCHERI-NEXT:    ccall __atomic_store_8
; RV32IAXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i64_unordered:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 0
; RV64IXCHERI-NEXT:    ccall __atomic_store_8
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i64_unordered:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    sd a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i64 %b, i64 addrspace(200)* %a unordered, align 8
  ret void
}

define void @atomic_store_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a3, 0
; RV32IXCHERI-NEXT:    ccall __atomic_store_8
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    li a3, 0
; RV32IAXCHERI-NEXT:    ccall __atomic_store_8
; RV32IAXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 0
; RV64IXCHERI-NEXT:    ccall __atomic_store_8
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    sd a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i64 %b, i64 addrspace(200)* %a monotonic, align 8
  ret void
}

define void @atomic_store_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a3, 3
; RV32IXCHERI-NEXT:    ccall __atomic_store_8
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    li a3, 3
; RV32IAXCHERI-NEXT:    ccall __atomic_store_8
; RV32IAXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 3
; RV64IXCHERI-NEXT:    ccall __atomic_store_8
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, w
; RV64IAXCHERI-NEXT:    sd a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i64 %b, i64 addrspace(200)* %a release, align 8
  ret void
}

define void @atomic_store_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomic_store_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    li a3, 5
; RV32IXCHERI-NEXT:    ccall __atomic_store_8
; RV32IXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: atomic_store_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    sc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    li a3, 5
; RV32IAXCHERI-NEXT:    ccall __atomic_store_8
; RV32IAXCHERI-NEXT:    lc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: atomic_store_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    sc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    li a2, 5
; RV64IXCHERI-NEXT:    ccall __atomic_store_8
; RV64IXCHERI-NEXT:    lc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: atomic_store_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    fence rw, w
; RV64IAXCHERI-NEXT:    sd a1, 0(ca0)
; RV64IAXCHERI-NEXT:    ret
  store atomic i64 %b, i64 addrspace(200)* %a seq_cst, align 8
  ret void
}
