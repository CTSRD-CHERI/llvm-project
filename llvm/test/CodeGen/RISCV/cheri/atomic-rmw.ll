; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: %riscv32_cheri_purecap_llc -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV32IXCHERI %s
; RUN: %riscv32_cheri_purecap_llc -mattr=+a -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV32IAXCHERI %s
; RUN: %riscv64_cheri_purecap_llc -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV64IXCHERI %s
; RUN: %riscv64_cheri_purecap_llc -mattr=+a -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV64IAXCHERI %s

define i8 @atomicrmw_xchg_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB0_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB0_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB0_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB0_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_xchg_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB1_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB1_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB1_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB1_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_xchg_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB2_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB2_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB2_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB2_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_xchg_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB3_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB3_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB3_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB3_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_xchg_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB4_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB4_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB4_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB4_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_add_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB5_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB5_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB5_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB5_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_add_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB6_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB6_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB6_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB6_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_add_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB7_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB7_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB7_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB7_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_add_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB8_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB8_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB8_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB8_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_add_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB9_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB9_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB9_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB9_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_sub_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB10_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB10_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB10_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB10_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_sub_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB11_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB11_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB11_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB11_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_sub_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB12_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB12_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB12_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB12_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_sub_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB13_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB13_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB13_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB13_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_sub_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB14_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB14_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB14_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB14_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_and_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB15_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB15_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB15_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB15_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_and_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB16_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB16_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB16_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB16_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_and_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB17_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB17_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB17_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB17_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_and_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB18_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB18_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB18_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB18_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_and_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB19_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB19_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB19_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB19_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_nand_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB20_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB20_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB20_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB20_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_nand_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB21_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB21_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB21_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB21_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_nand_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB22_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB22_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB22_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB22_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_nand_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB23_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB23_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB23_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB23_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_nand_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB24_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB24_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB24_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB24_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_or_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB25_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB25_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB25_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB25_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_or_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB26_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB26_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB26_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB26_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_or_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB27_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB27_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB27_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB27_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_or_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB28_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB28_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB28_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB28_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_or_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB29_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB29_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB29_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB29_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_xor_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB30_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB30_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB30_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB30_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_xor_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB31_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB31_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB31_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB31_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_xor_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB32_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB32_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB32_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB32_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_xor_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB33_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB33_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB33_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB33_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_xor_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB34_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB34_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_1
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB34_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB34_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_max_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB35_2
; RV32IXCHERI-NEXT:  .LBB35_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB35_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB35_4
; RV32IXCHERI-NEXT:  .LBB35_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB35_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB35_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB35_1
; RV32IXCHERI-NEXT:  .LBB35_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB35_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB35_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB35_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB35_3: # in Loop: Header=BB35_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB35_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB35_2
; RV64IXCHERI-NEXT:  .LBB35_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB35_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB35_4
; RV64IXCHERI-NEXT:  .LBB35_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB35_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB35_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB35_1
; RV64IXCHERI-NEXT:  .LBB35_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB35_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB35_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB35_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB35_3: # in Loop: Header=BB35_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB35_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_max_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB36_2
; RV32IXCHERI-NEXT:  .LBB36_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB36_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB36_4
; RV32IXCHERI-NEXT:  .LBB36_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB36_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB36_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB36_1
; RV32IXCHERI-NEXT:  .LBB36_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB36_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB36_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB36_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB36_3: # in Loop: Header=BB36_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB36_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB36_2
; RV64IXCHERI-NEXT:  .LBB36_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB36_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB36_4
; RV64IXCHERI-NEXT:  .LBB36_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB36_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB36_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB36_1
; RV64IXCHERI-NEXT:  .LBB36_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB36_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB36_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB36_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB36_3: # in Loop: Header=BB36_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB36_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_max_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB37_2
; RV32IXCHERI-NEXT:  .LBB37_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB37_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB37_4
; RV32IXCHERI-NEXT:  .LBB37_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB37_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB37_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB37_1
; RV32IXCHERI-NEXT:  .LBB37_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB37_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB37_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB37_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB37_3: # in Loop: Header=BB37_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB37_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB37_2
; RV64IXCHERI-NEXT:  .LBB37_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB37_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB37_4
; RV64IXCHERI-NEXT:  .LBB37_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB37_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB37_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB37_1
; RV64IXCHERI-NEXT:  .LBB37_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB37_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB37_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB37_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB37_3: # in Loop: Header=BB37_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB37_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_max_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB38_2
; RV32IXCHERI-NEXT:  .LBB38_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB38_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB38_4
; RV32IXCHERI-NEXT:  .LBB38_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB38_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB38_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB38_1
; RV32IXCHERI-NEXT:  .LBB38_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB38_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB38_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB38_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB38_3: # in Loop: Header=BB38_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB38_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB38_2
; RV64IXCHERI-NEXT:  .LBB38_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB38_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB38_4
; RV64IXCHERI-NEXT:  .LBB38_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB38_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB38_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB38_1
; RV64IXCHERI-NEXT:  .LBB38_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB38_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB38_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB38_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB38_3: # in Loop: Header=BB38_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB38_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_max_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB39_2
; RV32IXCHERI-NEXT:  .LBB39_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB39_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB39_4
; RV32IXCHERI-NEXT:  .LBB39_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB39_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB39_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB39_1
; RV32IXCHERI-NEXT:  .LBB39_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB39_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB39_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB39_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB39_3: # in Loop: Header=BB39_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB39_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB39_2
; RV64IXCHERI-NEXT:  .LBB39_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB39_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB39_4
; RV64IXCHERI-NEXT:  .LBB39_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB39_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB39_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB39_1
; RV64IXCHERI-NEXT:  .LBB39_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB39_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB39_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB39_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB39_3: # in Loop: Header=BB39_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB39_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_min_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB40_2
; RV32IXCHERI-NEXT:  .LBB40_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB40_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB40_4
; RV32IXCHERI-NEXT:  .LBB40_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB40_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB40_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB40_1
; RV32IXCHERI-NEXT:  .LBB40_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB40_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB40_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB40_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB40_3: # in Loop: Header=BB40_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB40_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB40_2
; RV64IXCHERI-NEXT:  .LBB40_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB40_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB40_4
; RV64IXCHERI-NEXT:  .LBB40_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB40_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB40_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB40_1
; RV64IXCHERI-NEXT:  .LBB40_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB40_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB40_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB40_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB40_3: # in Loop: Header=BB40_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB40_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_min_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB41_2
; RV32IXCHERI-NEXT:  .LBB41_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB41_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB41_4
; RV32IXCHERI-NEXT:  .LBB41_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB41_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB41_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB41_1
; RV32IXCHERI-NEXT:  .LBB41_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB41_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB41_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB41_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB41_3: # in Loop: Header=BB41_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB41_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB41_2
; RV64IXCHERI-NEXT:  .LBB41_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB41_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB41_4
; RV64IXCHERI-NEXT:  .LBB41_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB41_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB41_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB41_1
; RV64IXCHERI-NEXT:  .LBB41_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB41_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB41_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB41_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB41_3: # in Loop: Header=BB41_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB41_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_min_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB42_2
; RV32IXCHERI-NEXT:  .LBB42_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB42_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB42_4
; RV32IXCHERI-NEXT:  .LBB42_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB42_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB42_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB42_1
; RV32IXCHERI-NEXT:  .LBB42_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB42_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB42_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB42_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB42_3: # in Loop: Header=BB42_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB42_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB42_2
; RV64IXCHERI-NEXT:  .LBB42_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB42_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB42_4
; RV64IXCHERI-NEXT:  .LBB42_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB42_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB42_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB42_1
; RV64IXCHERI-NEXT:  .LBB42_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB42_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB42_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB42_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB42_3: # in Loop: Header=BB42_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB42_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_min_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB43_2
; RV32IXCHERI-NEXT:  .LBB43_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB43_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB43_4
; RV32IXCHERI-NEXT:  .LBB43_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB43_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB43_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB43_1
; RV32IXCHERI-NEXT:  .LBB43_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB43_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB43_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB43_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB43_3: # in Loop: Header=BB43_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB43_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB43_2
; RV64IXCHERI-NEXT:  .LBB43_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB43_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB43_4
; RV64IXCHERI-NEXT:  .LBB43_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB43_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB43_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB43_1
; RV64IXCHERI-NEXT:  .LBB43_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB43_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB43_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB43_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB43_3: # in Loop: Header=BB43_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB43_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_min_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 24
; RV32IXCHERI-NEXT:    srai s1, a0, 24
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB44_2
; RV32IXCHERI-NEXT:  .LBB44_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB44_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB44_4
; RV32IXCHERI-NEXT:  .LBB44_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 24
; RV32IXCHERI-NEXT:    srai a0, a0, 24
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB44_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB44_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB44_1
; RV32IXCHERI-NEXT:  .LBB44_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB44_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB44_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB44_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB44_3: # in Loop: Header=BB44_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB44_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 56
; RV64IXCHERI-NEXT:    srai s1, a0, 56
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB44_2
; RV64IXCHERI-NEXT:  .LBB44_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB44_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB44_4
; RV64IXCHERI-NEXT:  .LBB44_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 56
; RV64IXCHERI-NEXT:    srai a0, a0, 56
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB44_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB44_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB44_1
; RV64IXCHERI-NEXT:  .LBB44_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB44_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB44_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB44_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB44_3: # in Loop: Header=BB44_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB44_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_umax_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB45_2
; RV32IXCHERI-NEXT:  .LBB45_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB45_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB45_4
; RV32IXCHERI-NEXT:  .LBB45_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s1, a0, .LBB45_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB45_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB45_1
; RV32IXCHERI-NEXT:  .LBB45_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB45_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB45_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB45_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB45_3: # in Loop: Header=BB45_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB45_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB45_2
; RV64IXCHERI-NEXT:  .LBB45_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB45_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB45_4
; RV64IXCHERI-NEXT:  .LBB45_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB45_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB45_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB45_1
; RV64IXCHERI-NEXT:  .LBB45_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB45_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB45_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB45_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB45_3: # in Loop: Header=BB45_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB45_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_umax_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB46_2
; RV32IXCHERI-NEXT:  .LBB46_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB46_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB46_4
; RV32IXCHERI-NEXT:  .LBB46_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s1, a0, .LBB46_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB46_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB46_1
; RV32IXCHERI-NEXT:  .LBB46_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB46_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB46_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB46_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB46_3: # in Loop: Header=BB46_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB46_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB46_2
; RV64IXCHERI-NEXT:  .LBB46_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB46_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB46_4
; RV64IXCHERI-NEXT:  .LBB46_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB46_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB46_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB46_1
; RV64IXCHERI-NEXT:  .LBB46_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB46_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB46_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB46_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB46_3: # in Loop: Header=BB46_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB46_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_umax_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB47_2
; RV32IXCHERI-NEXT:  .LBB47_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB47_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB47_4
; RV32IXCHERI-NEXT:  .LBB47_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s1, a0, .LBB47_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB47_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB47_1
; RV32IXCHERI-NEXT:  .LBB47_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB47_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB47_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB47_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB47_3: # in Loop: Header=BB47_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB47_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB47_2
; RV64IXCHERI-NEXT:  .LBB47_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB47_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB47_4
; RV64IXCHERI-NEXT:  .LBB47_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB47_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB47_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB47_1
; RV64IXCHERI-NEXT:  .LBB47_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB47_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB47_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB47_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB47_3: # in Loop: Header=BB47_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB47_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_umax_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB48_2
; RV32IXCHERI-NEXT:  .LBB48_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB48_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB48_4
; RV32IXCHERI-NEXT:  .LBB48_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s1, a0, .LBB48_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB48_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB48_1
; RV32IXCHERI-NEXT:  .LBB48_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB48_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB48_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB48_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB48_3: # in Loop: Header=BB48_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB48_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB48_2
; RV64IXCHERI-NEXT:  .LBB48_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB48_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB48_4
; RV64IXCHERI-NEXT:  .LBB48_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB48_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB48_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB48_1
; RV64IXCHERI-NEXT:  .LBB48_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB48_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB48_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB48_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB48_3: # in Loop: Header=BB48_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB48_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_umax_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB49_2
; RV32IXCHERI-NEXT:  .LBB49_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB49_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB49_4
; RV32IXCHERI-NEXT:  .LBB49_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s1, a0, .LBB49_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB49_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB49_1
; RV32IXCHERI-NEXT:  .LBB49_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB49_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB49_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB49_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB49_3: # in Loop: Header=BB49_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB49_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB49_2
; RV64IXCHERI-NEXT:  .LBB49_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB49_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB49_4
; RV64IXCHERI-NEXT:  .LBB49_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB49_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB49_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB49_1
; RV64IXCHERI-NEXT:  .LBB49_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB49_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB49_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB49_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB49_3: # in Loop: Header=BB49_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB49_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i8 @atomicrmw_umin_i8_monotonic(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i8_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB50_2
; RV32IXCHERI-NEXT:  .LBB50_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB50_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB50_4
; RV32IXCHERI-NEXT:  .LBB50_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s1, a0, .LBB50_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB50_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB50_1
; RV32IXCHERI-NEXT:  .LBB50_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i8_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB50_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB50_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB50_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB50_3: # in Loop: Header=BB50_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB50_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i8_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB50_2
; RV64IXCHERI-NEXT:  .LBB50_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB50_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB50_4
; RV64IXCHERI-NEXT:  .LBB50_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB50_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB50_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB50_1
; RV64IXCHERI-NEXT:  .LBB50_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i8_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB50_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB50_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB50_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB50_3: # in Loop: Header=BB50_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB50_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i8 addrspace(200)* %a, i8 %b monotonic
  ret i8 %1
}

define i8 @atomicrmw_umin_i8_acquire(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i8_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB51_2
; RV32IXCHERI-NEXT:  .LBB51_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB51_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB51_4
; RV32IXCHERI-NEXT:  .LBB51_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s1, a0, .LBB51_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB51_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB51_1
; RV32IXCHERI-NEXT:  .LBB51_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i8_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB51_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB51_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB51_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB51_3: # in Loop: Header=BB51_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB51_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i8_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB51_2
; RV64IXCHERI-NEXT:  .LBB51_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB51_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB51_4
; RV64IXCHERI-NEXT:  .LBB51_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB51_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB51_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB51_1
; RV64IXCHERI-NEXT:  .LBB51_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i8_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB51_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB51_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB51_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB51_3: # in Loop: Header=BB51_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB51_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i8 addrspace(200)* %a, i8 %b acquire
  ret i8 %1
}

define i8 @atomicrmw_umin_i8_release(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i8_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB52_2
; RV32IXCHERI-NEXT:  .LBB52_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB52_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB52_4
; RV32IXCHERI-NEXT:  .LBB52_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s1, a0, .LBB52_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB52_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB52_1
; RV32IXCHERI-NEXT:  .LBB52_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i8_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB52_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB52_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB52_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB52_3: # in Loop: Header=BB52_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB52_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i8_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB52_2
; RV64IXCHERI-NEXT:  .LBB52_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB52_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB52_4
; RV64IXCHERI-NEXT:  .LBB52_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB52_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB52_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB52_1
; RV64IXCHERI-NEXT:  .LBB52_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i8_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB52_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB52_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB52_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB52_3: # in Loop: Header=BB52_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB52_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i8 addrspace(200)* %a, i8 %b release
  ret i8 %1
}

define i8 @atomicrmw_umin_i8_acq_rel(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i8_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB53_2
; RV32IXCHERI-NEXT:  .LBB53_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB53_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB53_4
; RV32IXCHERI-NEXT:  .LBB53_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s1, a0, .LBB53_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB53_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB53_1
; RV32IXCHERI-NEXT:  .LBB53_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i8_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB53_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB53_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB53_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB53_3: # in Loop: Header=BB53_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB53_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i8_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB53_2
; RV64IXCHERI-NEXT:  .LBB53_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB53_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB53_4
; RV64IXCHERI-NEXT:  .LBB53_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB53_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB53_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB53_1
; RV64IXCHERI-NEXT:  .LBB53_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i8_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB53_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB53_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB53_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB53_3: # in Loop: Header=BB53_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB53_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i8 addrspace(200)* %a, i8 %b acq_rel
  ret i8 %1
}

define i8 @atomicrmw_umin_i8_seq_cst(i8 addrspace(200)* %a, i8 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i8_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV32IXCHERI-NEXT:    andi s1, s2, 255
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 7
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV32IXCHERI-NEXT:    j .LBB54_2
; RV32IXCHERI-NEXT:  .LBB54_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB54_2 Depth=1
; RV32IXCHERI-NEXT:    csb a1, 7(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV32IXCHERI-NEXT:    clb a1, 7(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB54_4
; RV32IXCHERI-NEXT:  .LBB54_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    andi a0, a1, 255
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s1, a0, .LBB54_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB54_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB54_1
; RV32IXCHERI-NEXT:  .LBB54_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i8_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB54_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB54_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB54_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB54_3: # in Loop: Header=BB54_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB54_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i8_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clbu a1, 0(ca0)
; RV64IXCHERI-NEXT:    andi s1, s2, 255
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 15
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 1
; RV64IXCHERI-NEXT:    j .LBB54_2
; RV64IXCHERI-NEXT:  .LBB54_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB54_2 Depth=1
; RV64IXCHERI-NEXT:    csb a1, 15(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_1
; RV64IXCHERI-NEXT:    clb a1, 15(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB54_4
; RV64IXCHERI-NEXT:  .LBB54_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    andi a0, a1, 255
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB54_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB54_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB54_1
; RV64IXCHERI-NEXT:  .LBB54_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i8_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB54_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.b.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB54_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB54_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB54_3: # in Loop: Header=BB54_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.b.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB54_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i8 addrspace(200)* %a, i8 %b seq_cst
  ret i8 %1
}

define i16 @atomicrmw_xchg_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB55_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB55_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB55_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB55_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_xchg_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB56_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB56_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB56_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB56_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_xchg_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB57_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB57_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB57_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB57_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_xchg_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB58_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB58_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB58_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB58_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_xchg_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB59_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, zero, a1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB59_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB59_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, zero, a1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB59_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_add_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB60_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB60_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB60_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB60_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_add_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB61_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB61_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB61_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB61_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_add_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB62_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB62_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB62_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB62_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_add_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB63_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB63_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB63_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB63_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_add_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB64_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    add a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB64_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB64_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    add a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB64_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_sub_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB65_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB65_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB65_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB65_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_sub_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB66_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB66_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB66_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB66_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_sub_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB67_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB67_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB67_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB67_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_sub_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB68_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB68_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB68_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB68_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_sub_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB69_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    sub a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB69_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB69_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    sub a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB69_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_and_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB70_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB70_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB70_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB70_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_and_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB71_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB71_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB71_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB71_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_and_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB72_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB72_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB72_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB72_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_and_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB73_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB73_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB73_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB73_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_and_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB74_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB74_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB74_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB74_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_nand_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB75_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB75_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB75_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB75_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_nand_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB76_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB76_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB76_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB76_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_nand_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB77_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB77_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB77_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB77_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_nand_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB78_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB78_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB78_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB78_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_nand_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB79_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB79_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB79_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB79_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_or_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB80_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB80_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB80_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB80_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_or_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB81_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB81_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB81_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB81_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_or_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB82_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB82_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB82_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB82_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_or_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB83_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB83_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB83_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB83_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_or_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB84_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    or a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB84_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB84_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    or a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB84_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_xor_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB85_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB85_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB85_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB85_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_xor_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB86_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB86_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB86_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB86_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_xor_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB87_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB87_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB87_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB87_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_xor_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB88_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB88_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB88_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB88_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_xor_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB89_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    xor a3, a2, a1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB89_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_2
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB89_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    xor a3, a2, a1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB89_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_max_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB90_2
; RV32IXCHERI-NEXT:  .LBB90_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB90_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB90_4
; RV32IXCHERI-NEXT:  .LBB90_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB90_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB90_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB90_1
; RV32IXCHERI-NEXT:  .LBB90_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB90_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB90_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB90_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB90_3: # in Loop: Header=BB90_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB90_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB90_2
; RV64IXCHERI-NEXT:  .LBB90_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB90_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB90_4
; RV64IXCHERI-NEXT:  .LBB90_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB90_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB90_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB90_1
; RV64IXCHERI-NEXT:  .LBB90_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB90_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB90_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB90_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB90_3: # in Loop: Header=BB90_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB90_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_max_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB91_2
; RV32IXCHERI-NEXT:  .LBB91_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB91_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB91_4
; RV32IXCHERI-NEXT:  .LBB91_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB91_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB91_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB91_1
; RV32IXCHERI-NEXT:  .LBB91_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB91_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB91_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB91_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB91_3: # in Loop: Header=BB91_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB91_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB91_2
; RV64IXCHERI-NEXT:  .LBB91_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB91_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB91_4
; RV64IXCHERI-NEXT:  .LBB91_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB91_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB91_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB91_1
; RV64IXCHERI-NEXT:  .LBB91_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB91_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB91_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB91_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB91_3: # in Loop: Header=BB91_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB91_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_max_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB92_2
; RV32IXCHERI-NEXT:  .LBB92_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB92_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB92_4
; RV32IXCHERI-NEXT:  .LBB92_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB92_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB92_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB92_1
; RV32IXCHERI-NEXT:  .LBB92_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB92_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB92_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB92_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB92_3: # in Loop: Header=BB92_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB92_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB92_2
; RV64IXCHERI-NEXT:  .LBB92_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB92_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB92_4
; RV64IXCHERI-NEXT:  .LBB92_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB92_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB92_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB92_1
; RV64IXCHERI-NEXT:  .LBB92_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB92_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB92_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB92_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB92_3: # in Loop: Header=BB92_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB92_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_max_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB93_2
; RV32IXCHERI-NEXT:  .LBB93_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB93_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB93_4
; RV32IXCHERI-NEXT:  .LBB93_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB93_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB93_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB93_1
; RV32IXCHERI-NEXT:  .LBB93_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB93_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB93_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB93_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB93_3: # in Loop: Header=BB93_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB93_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB93_2
; RV64IXCHERI-NEXT:  .LBB93_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB93_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB93_4
; RV64IXCHERI-NEXT:  .LBB93_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB93_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB93_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB93_1
; RV64IXCHERI-NEXT:  .LBB93_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB93_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB93_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB93_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB93_3: # in Loop: Header=BB93_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB93_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_max_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB94_2
; RV32IXCHERI-NEXT:  .LBB94_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB94_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB94_4
; RV32IXCHERI-NEXT:  .LBB94_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    blt s1, a0, .LBB94_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB94_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB94_1
; RV32IXCHERI-NEXT:  .LBB94_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB94_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a3, a1, .LBB94_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB94_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB94_3: # in Loop: Header=BB94_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB94_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB94_2
; RV64IXCHERI-NEXT:  .LBB94_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB94_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB94_4
; RV64IXCHERI-NEXT:  .LBB94_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB94_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB94_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB94_1
; RV64IXCHERI-NEXT:  .LBB94_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB94_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a3, a1, .LBB94_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB94_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB94_3: # in Loop: Header=BB94_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB94_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_min_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB95_2
; RV32IXCHERI-NEXT:  .LBB95_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB95_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB95_4
; RV32IXCHERI-NEXT:  .LBB95_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB95_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB95_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB95_1
; RV32IXCHERI-NEXT:  .LBB95_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB95_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB95_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB95_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB95_3: # in Loop: Header=BB95_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB95_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB95_2
; RV64IXCHERI-NEXT:  .LBB95_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB95_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB95_4
; RV64IXCHERI-NEXT:  .LBB95_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB95_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB95_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB95_1
; RV64IXCHERI-NEXT:  .LBB95_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB95_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB95_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB95_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB95_3: # in Loop: Header=BB95_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB95_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_min_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB96_2
; RV32IXCHERI-NEXT:  .LBB96_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB96_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB96_4
; RV32IXCHERI-NEXT:  .LBB96_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB96_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB96_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB96_1
; RV32IXCHERI-NEXT:  .LBB96_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB96_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB96_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB96_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB96_3: # in Loop: Header=BB96_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB96_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB96_2
; RV64IXCHERI-NEXT:  .LBB96_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB96_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB96_4
; RV64IXCHERI-NEXT:  .LBB96_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB96_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB96_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB96_1
; RV64IXCHERI-NEXT:  .LBB96_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB96_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB96_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB96_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB96_3: # in Loop: Header=BB96_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB96_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_min_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB97_2
; RV32IXCHERI-NEXT:  .LBB97_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB97_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB97_4
; RV32IXCHERI-NEXT:  .LBB97_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB97_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB97_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB97_1
; RV32IXCHERI-NEXT:  .LBB97_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB97_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB97_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB97_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB97_3: # in Loop: Header=BB97_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB97_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB97_2
; RV64IXCHERI-NEXT:  .LBB97_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB97_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB97_4
; RV64IXCHERI-NEXT:  .LBB97_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB97_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB97_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB97_1
; RV64IXCHERI-NEXT:  .LBB97_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB97_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB97_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB97_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB97_3: # in Loop: Header=BB97_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB97_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_min_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB98_2
; RV32IXCHERI-NEXT:  .LBB98_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB98_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB98_4
; RV32IXCHERI-NEXT:  .LBB98_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB98_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB98_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB98_1
; RV32IXCHERI-NEXT:  .LBB98_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB98_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB98_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB98_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB98_3: # in Loop: Header=BB98_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB98_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB98_2
; RV64IXCHERI-NEXT:  .LBB98_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB98_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB98_4
; RV64IXCHERI-NEXT:  .LBB98_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB98_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB98_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB98_1
; RV64IXCHERI-NEXT:  .LBB98_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB98_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB98_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB98_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB98_3: # in Loop: Header=BB98_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB98_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_min_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    slli a0, s2, 16
; RV32IXCHERI-NEXT:    srai s1, a0, 16
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 6
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB99_2
; RV32IXCHERI-NEXT:  .LBB99_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB99_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 6(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 6(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB99_4
; RV32IXCHERI-NEXT:  .LBB99_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    slli a0, a1, 16
; RV32IXCHERI-NEXT:    srai a0, a0, 16
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bge s1, a0, .LBB99_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB99_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB99_1
; RV32IXCHERI-NEXT:  .LBB99_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB99_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bge a1, a3, .LBB99_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB99_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB99_3: # in Loop: Header=BB99_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB99_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    slli a0, s2, 48
; RV64IXCHERI-NEXT:    srai s1, a0, 48
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB99_2
; RV64IXCHERI-NEXT:  .LBB99_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB99_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB99_4
; RV64IXCHERI-NEXT:  .LBB99_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    slli a0, a1, 48
; RV64IXCHERI-NEXT:    srai a0, a0, 48
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB99_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB99_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB99_1
; RV64IXCHERI-NEXT:  .LBB99_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB99_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bge a1, a3, .LBB99_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB99_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB99_3: # in Loop: Header=BB99_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB99_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_umax_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB100_2
; RV32IXCHERI-NEXT:  .LBB100_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB100_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB100_4
; RV32IXCHERI-NEXT:  .LBB100_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s4, a0, .LBB100_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB100_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB100_1
; RV32IXCHERI-NEXT:  .LBB100_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB100_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB100_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB100_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB100_3: # in Loop: Header=BB100_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB100_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB100_2
; RV64IXCHERI-NEXT:  .LBB100_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB100_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB100_4
; RV64IXCHERI-NEXT:  .LBB100_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s4, a0, .LBB100_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB100_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB100_1
; RV64IXCHERI-NEXT:  .LBB100_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB100_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB100_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB100_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB100_3: # in Loop: Header=BB100_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB100_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_umax_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB101_2
; RV32IXCHERI-NEXT:  .LBB101_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB101_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB101_4
; RV32IXCHERI-NEXT:  .LBB101_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s4, a0, .LBB101_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB101_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB101_1
; RV32IXCHERI-NEXT:  .LBB101_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB101_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB101_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB101_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB101_3: # in Loop: Header=BB101_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB101_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB101_2
; RV64IXCHERI-NEXT:  .LBB101_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB101_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB101_4
; RV64IXCHERI-NEXT:  .LBB101_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s4, a0, .LBB101_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB101_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB101_1
; RV64IXCHERI-NEXT:  .LBB101_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB101_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB101_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB101_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB101_3: # in Loop: Header=BB101_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB101_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_umax_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB102_2
; RV32IXCHERI-NEXT:  .LBB102_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB102_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB102_4
; RV32IXCHERI-NEXT:  .LBB102_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s4, a0, .LBB102_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB102_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB102_1
; RV32IXCHERI-NEXT:  .LBB102_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB102_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB102_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB102_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB102_3: # in Loop: Header=BB102_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB102_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB102_2
; RV64IXCHERI-NEXT:  .LBB102_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB102_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB102_4
; RV64IXCHERI-NEXT:  .LBB102_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s4, a0, .LBB102_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB102_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB102_1
; RV64IXCHERI-NEXT:  .LBB102_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB102_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB102_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB102_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB102_3: # in Loop: Header=BB102_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB102_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_umax_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB103_2
; RV32IXCHERI-NEXT:  .LBB103_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB103_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB103_4
; RV32IXCHERI-NEXT:  .LBB103_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s4, a0, .LBB103_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB103_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB103_1
; RV32IXCHERI-NEXT:  .LBB103_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB103_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB103_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB103_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB103_3: # in Loop: Header=BB103_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB103_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB103_2
; RV64IXCHERI-NEXT:  .LBB103_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB103_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB103_4
; RV64IXCHERI-NEXT:  .LBB103_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s4, a0, .LBB103_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB103_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB103_1
; RV64IXCHERI-NEXT:  .LBB103_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB103_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB103_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB103_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB103_3: # in Loop: Header=BB103_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB103_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_umax_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB104_2
; RV32IXCHERI-NEXT:  .LBB104_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB104_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB104_4
; RV32IXCHERI-NEXT:  .LBB104_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bltu s4, a0, .LBB104_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB104_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB104_1
; RV32IXCHERI-NEXT:  .LBB104_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB104_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a3, a1, .LBB104_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB104_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB104_3: # in Loop: Header=BB104_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB104_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB104_2
; RV64IXCHERI-NEXT:  .LBB104_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB104_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB104_4
; RV64IXCHERI-NEXT:  .LBB104_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s4, a0, .LBB104_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB104_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB104_1
; RV64IXCHERI-NEXT:  .LBB104_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB104_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a3, a1, .LBB104_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB104_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB104_3: # in Loop: Header=BB104_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB104_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i16 @atomicrmw_umin_i16_monotonic(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i16_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB105_2
; RV32IXCHERI-NEXT:  .LBB105_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB105_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB105_4
; RV32IXCHERI-NEXT:  .LBB105_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s4, a0, .LBB105_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB105_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB105_1
; RV32IXCHERI-NEXT:  .LBB105_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i16_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB105_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB105_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB105_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB105_3: # in Loop: Header=BB105_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB105_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i16_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB105_2
; RV64IXCHERI-NEXT:  .LBB105_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB105_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB105_4
; RV64IXCHERI-NEXT:  .LBB105_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s4, a0, .LBB105_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB105_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB105_1
; RV64IXCHERI-NEXT:  .LBB105_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i16_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB105_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB105_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB105_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB105_3: # in Loop: Header=BB105_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB105_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i16 addrspace(200)* %a, i16 %b monotonic
  ret i16 %1
}

define i16 @atomicrmw_umin_i16_acquire(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i16_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB106_2
; RV32IXCHERI-NEXT:  .LBB106_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB106_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB106_4
; RV32IXCHERI-NEXT:  .LBB106_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s4, a0, .LBB106_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB106_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB106_1
; RV32IXCHERI-NEXT:  .LBB106_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i16_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB106_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB106_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB106_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB106_3: # in Loop: Header=BB106_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB106_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i16_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB106_2
; RV64IXCHERI-NEXT:  .LBB106_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB106_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB106_4
; RV64IXCHERI-NEXT:  .LBB106_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s4, a0, .LBB106_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB106_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB106_1
; RV64IXCHERI-NEXT:  .LBB106_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i16_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB106_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB106_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB106_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB106_3: # in Loop: Header=BB106_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB106_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i16 addrspace(200)* %a, i16 %b acquire
  ret i16 %1
}

define i16 @atomicrmw_umin_i16_release(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i16_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB107_2
; RV32IXCHERI-NEXT:  .LBB107_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB107_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB107_4
; RV32IXCHERI-NEXT:  .LBB107_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s4, a0, .LBB107_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB107_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB107_1
; RV32IXCHERI-NEXT:  .LBB107_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i16_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB107_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB107_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB107_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB107_3: # in Loop: Header=BB107_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB107_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i16_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB107_2
; RV64IXCHERI-NEXT:  .LBB107_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB107_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB107_4
; RV64IXCHERI-NEXT:  .LBB107_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s4, a0, .LBB107_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB107_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB107_1
; RV64IXCHERI-NEXT:  .LBB107_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i16_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB107_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB107_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB107_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB107_3: # in Loop: Header=BB107_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB107_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i16 addrspace(200)* %a, i16 %b release
  ret i16 %1
}

define i16 @atomicrmw_umin_i16_acq_rel(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i16_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB108_2
; RV32IXCHERI-NEXT:  .LBB108_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB108_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB108_4
; RV32IXCHERI-NEXT:  .LBB108_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s4, a0, .LBB108_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB108_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB108_1
; RV32IXCHERI-NEXT:  .LBB108_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i16_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB108_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB108_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB108_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB108_3: # in Loop: Header=BB108_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB108_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i16_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB108_2
; RV64IXCHERI-NEXT:  .LBB108_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB108_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB108_4
; RV64IXCHERI-NEXT:  .LBB108_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s4, a0, .LBB108_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB108_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB108_1
; RV64IXCHERI-NEXT:  .LBB108_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i16_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB108_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB108_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB108_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB108_3: # in Loop: Header=BB108_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB108_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i16 addrspace(200)* %a, i16 %b acq_rel
  ret i16 %1
}

define i16 @atomicrmw_umin_i16_seq_cst(i16 addrspace(200)* %a, i16 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i16_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -64
; RV32IXCHERI-NEXT:    csc cra, 56(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 48(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs4, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV32IXCHERI-NEXT:    lui a0, 16
; RV32IXCHERI-NEXT:    addi s1, a0, -1
; RV32IXCHERI-NEXT:    and s4, s2, s1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV32IXCHERI-NEXT:    j .LBB109_2
; RV32IXCHERI-NEXT:  .LBB109_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB109_2 Depth=1
; RV32IXCHERI-NEXT:    csh a1, 14(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV32IXCHERI-NEXT:    clh a1, 14(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB109_4
; RV32IXCHERI-NEXT:  .LBB109_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    and a0, a1, s1
; RV32IXCHERI-NEXT:    mv a2, a1
; RV32IXCHERI-NEXT:    bgeu s4, a0, .LBB109_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB109_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    j .LBB109_1
; RV32IXCHERI-NEXT:  .LBB109_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a1
; RV32IXCHERI-NEXT:    clc cs4, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs3, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 48(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 56(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 64
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i16_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB109_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    mv a3, a2
; RV32IAXCHERI-NEXT:    bgeu a1, a3, .LBB109_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB109_1 Depth=1
; RV32IAXCHERI-NEXT:    mv a3, a1
; RV32IAXCHERI-NEXT:  .LBB109_3: # in Loop: Header=BB109_1 Depth=1
; RV32IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB109_1
; RV32IAXCHERI-NEXT:  # %bb.4:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i16_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -112
; RV64IXCHERI-NEXT:    csc cra, 96(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs4, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clhu a1, 0(ca0)
; RV64IXCHERI-NEXT:    lui a0, 16
; RV64IXCHERI-NEXT:    addiw s1, a0, -1
; RV64IXCHERI-NEXT:    and s4, s2, s1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 14
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 2
; RV64IXCHERI-NEXT:    j .LBB109_2
; RV64IXCHERI-NEXT:  .LBB109_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB109_2 Depth=1
; RV64IXCHERI-NEXT:    csh a1, 14(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_2
; RV64IXCHERI-NEXT:    clh a1, 14(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB109_4
; RV64IXCHERI-NEXT:  .LBB109_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    and a0, a1, s1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s4, a0, .LBB109_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB109_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB109_1
; RV64IXCHERI-NEXT:  .LBB109_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs4, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs3, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 96(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 112
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i16_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB109_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.h.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    mv a3, a2
; RV64IAXCHERI-NEXT:    bgeu a1, a3, .LBB109_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB109_1 Depth=1
; RV64IAXCHERI-NEXT:    mv a3, a1
; RV64IAXCHERI-NEXT:  .LBB109_3: # in Loop: Header=BB109_1 Depth=1
; RV64IAXCHERI-NEXT:    csc.h.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB109_1
; RV64IAXCHERI-NEXT:  # %bb.4:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i16 addrspace(200)* %a, i16 %b seq_cst
  ret i16 %1
}

define i32 @atomicrmw_xchg_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_xchg_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_xchg_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_xchg_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_xchg_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_add_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoadd.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_add_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoadd.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_add_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoadd.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_add_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoadd.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_add_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoadd.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_sub_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    neg a1, a1
; RV32IAXCHERI-NEXT:    camoadd.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_sub_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    neg a1, a1
; RV32IAXCHERI-NEXT:    camoadd.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_sub_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    neg a1, a1
; RV32IAXCHERI-NEXT:    camoadd.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_sub_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    neg a1, a1
; RV32IAXCHERI-NEXT:    camoadd.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_sub_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    neg a1, a1
; RV32IAXCHERI-NEXT:    camoadd.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_and_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoand.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_and_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoand.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_and_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoand.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_and_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoand.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_and_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoand.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_nand_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB130_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.w a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.w a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB130_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB130_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.w a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.w a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB130_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_nand_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB131_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.w.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.w a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB131_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB131_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.w.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.w a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB131_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_nand_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB132_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.w a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.w.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB132_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB132_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.w a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.w.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB132_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_nand_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB133_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.w.aq a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.w.rl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB133_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB133_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.w.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.w.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB133_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_nand_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB134_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    clr.w.aqrl a2, (ca0)
; RV32IAXCHERI-NEXT:    and a3, a2, a1
; RV32IAXCHERI-NEXT:    not a3, a3
; RV32IAXCHERI-NEXT:    csc.w.aqrl a3, a3, (ca0)
; RV32IAXCHERI-NEXT:    bnez a3, .LBB134_1
; RV32IAXCHERI-NEXT:  # %bb.2:
; RV32IAXCHERI-NEXT:    mv a0, a2
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB134_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.w.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.w.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB134_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_or_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoor.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_or_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoor.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_or_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoor.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_or_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoor.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_or_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoor.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_xor_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoxor.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_xor_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoxor.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_xor_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoxor.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_xor_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoxor.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_xor_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoxor.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_4
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_max_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB145_2
; RV32IXCHERI-NEXT:  .LBB145_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB145_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB145_4
; RV32IXCHERI-NEXT:  .LBB145_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    blt s1, a3, .LBB145_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB145_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB145_1
; RV32IXCHERI-NEXT:  .LBB145_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomax.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB145_2
; RV64IXCHERI-NEXT:  .LBB145_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB145_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB145_4
; RV64IXCHERI-NEXT:  .LBB145_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB145_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB145_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB145_1
; RV64IXCHERI-NEXT:  .LBB145_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_max_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB146_2
; RV32IXCHERI-NEXT:  .LBB146_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB146_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB146_4
; RV32IXCHERI-NEXT:  .LBB146_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    blt s1, a3, .LBB146_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB146_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB146_1
; RV32IXCHERI-NEXT:  .LBB146_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomax.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB146_2
; RV64IXCHERI-NEXT:  .LBB146_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB146_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB146_4
; RV64IXCHERI-NEXT:  .LBB146_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB146_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB146_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB146_1
; RV64IXCHERI-NEXT:  .LBB146_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_max_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB147_2
; RV32IXCHERI-NEXT:  .LBB147_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB147_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB147_4
; RV32IXCHERI-NEXT:  .LBB147_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    blt s1, a3, .LBB147_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB147_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB147_1
; RV32IXCHERI-NEXT:  .LBB147_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomax.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB147_2
; RV64IXCHERI-NEXT:  .LBB147_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB147_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB147_4
; RV64IXCHERI-NEXT:  .LBB147_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB147_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB147_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB147_1
; RV64IXCHERI-NEXT:  .LBB147_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_max_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB148_2
; RV32IXCHERI-NEXT:  .LBB148_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB148_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB148_4
; RV32IXCHERI-NEXT:  .LBB148_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    blt s1, a3, .LBB148_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB148_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB148_1
; RV32IXCHERI-NEXT:  .LBB148_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomax.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB148_2
; RV64IXCHERI-NEXT:  .LBB148_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB148_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB148_4
; RV64IXCHERI-NEXT:  .LBB148_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB148_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB148_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB148_1
; RV64IXCHERI-NEXT:  .LBB148_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_max_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB149_2
; RV32IXCHERI-NEXT:  .LBB149_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB149_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB149_4
; RV32IXCHERI-NEXT:  .LBB149_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    blt s1, a3, .LBB149_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB149_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB149_1
; RV32IXCHERI-NEXT:  .LBB149_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomax.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB149_2
; RV64IXCHERI-NEXT:  .LBB149_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB149_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB149_4
; RV64IXCHERI-NEXT:  .LBB149_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    blt s1, a0, .LBB149_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB149_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB149_1
; RV64IXCHERI-NEXT:  .LBB149_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_min_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB150_2
; RV32IXCHERI-NEXT:  .LBB150_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB150_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB150_4
; RV32IXCHERI-NEXT:  .LBB150_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bge s1, a3, .LBB150_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB150_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB150_1
; RV32IXCHERI-NEXT:  .LBB150_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomin.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB150_2
; RV64IXCHERI-NEXT:  .LBB150_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB150_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB150_4
; RV64IXCHERI-NEXT:  .LBB150_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB150_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB150_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB150_1
; RV64IXCHERI-NEXT:  .LBB150_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_min_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB151_2
; RV32IXCHERI-NEXT:  .LBB151_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB151_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB151_4
; RV32IXCHERI-NEXT:  .LBB151_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bge s1, a3, .LBB151_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB151_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB151_1
; RV32IXCHERI-NEXT:  .LBB151_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomin.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB151_2
; RV64IXCHERI-NEXT:  .LBB151_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB151_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB151_4
; RV64IXCHERI-NEXT:  .LBB151_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB151_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB151_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB151_1
; RV64IXCHERI-NEXT:  .LBB151_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_min_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB152_2
; RV32IXCHERI-NEXT:  .LBB152_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB152_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB152_4
; RV32IXCHERI-NEXT:  .LBB152_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bge s1, a3, .LBB152_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB152_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB152_1
; RV32IXCHERI-NEXT:  .LBB152_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomin.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB152_2
; RV64IXCHERI-NEXT:  .LBB152_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB152_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB152_4
; RV64IXCHERI-NEXT:  .LBB152_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB152_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB152_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB152_1
; RV64IXCHERI-NEXT:  .LBB152_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_min_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB153_2
; RV32IXCHERI-NEXT:  .LBB153_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB153_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB153_4
; RV32IXCHERI-NEXT:  .LBB153_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bge s1, a3, .LBB153_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB153_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB153_1
; RV32IXCHERI-NEXT:  .LBB153_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomin.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB153_2
; RV64IXCHERI-NEXT:  .LBB153_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB153_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB153_4
; RV64IXCHERI-NEXT:  .LBB153_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB153_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB153_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB153_1
; RV64IXCHERI-NEXT:  .LBB153_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_min_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB154_2
; RV32IXCHERI-NEXT:  .LBB154_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB154_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB154_4
; RV32IXCHERI-NEXT:  .LBB154_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bge s1, a3, .LBB154_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB154_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB154_1
; RV32IXCHERI-NEXT:  .LBB154_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomin.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB154_2
; RV64IXCHERI-NEXT:  .LBB154_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB154_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB154_4
; RV64IXCHERI-NEXT:  .LBB154_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bge s1, a0, .LBB154_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB154_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB154_1
; RV64IXCHERI-NEXT:  .LBB154_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_umax_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB155_2
; RV32IXCHERI-NEXT:  .LBB155_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB155_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB155_4
; RV32IXCHERI-NEXT:  .LBB155_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bltu s1, a3, .LBB155_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB155_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB155_1
; RV32IXCHERI-NEXT:  .LBB155_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomaxu.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB155_2
; RV64IXCHERI-NEXT:  .LBB155_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB155_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB155_4
; RV64IXCHERI-NEXT:  .LBB155_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB155_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB155_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB155_1
; RV64IXCHERI-NEXT:  .LBB155_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_umax_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB156_2
; RV32IXCHERI-NEXT:  .LBB156_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB156_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB156_4
; RV32IXCHERI-NEXT:  .LBB156_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bltu s1, a3, .LBB156_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB156_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB156_1
; RV32IXCHERI-NEXT:  .LBB156_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomaxu.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB156_2
; RV64IXCHERI-NEXT:  .LBB156_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB156_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB156_4
; RV64IXCHERI-NEXT:  .LBB156_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB156_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB156_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB156_1
; RV64IXCHERI-NEXT:  .LBB156_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_umax_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB157_2
; RV32IXCHERI-NEXT:  .LBB157_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB157_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB157_4
; RV32IXCHERI-NEXT:  .LBB157_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bltu s1, a3, .LBB157_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB157_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB157_1
; RV32IXCHERI-NEXT:  .LBB157_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomaxu.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB157_2
; RV64IXCHERI-NEXT:  .LBB157_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB157_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB157_4
; RV64IXCHERI-NEXT:  .LBB157_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB157_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB157_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB157_1
; RV64IXCHERI-NEXT:  .LBB157_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_umax_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB158_2
; RV32IXCHERI-NEXT:  .LBB158_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB158_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB158_4
; RV32IXCHERI-NEXT:  .LBB158_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bltu s1, a3, .LBB158_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB158_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB158_1
; RV32IXCHERI-NEXT:  .LBB158_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomaxu.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB158_2
; RV64IXCHERI-NEXT:  .LBB158_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB158_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB158_4
; RV64IXCHERI-NEXT:  .LBB158_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB158_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB158_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB158_1
; RV64IXCHERI-NEXT:  .LBB158_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_umax_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB159_2
; RV32IXCHERI-NEXT:  .LBB159_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB159_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB159_4
; RV32IXCHERI-NEXT:  .LBB159_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bltu s1, a3, .LBB159_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB159_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB159_1
; RV32IXCHERI-NEXT:  .LBB159_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camomaxu.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB159_2
; RV64IXCHERI-NEXT:  .LBB159_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB159_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB159_4
; RV64IXCHERI-NEXT:  .LBB159_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bltu s1, a0, .LBB159_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB159_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB159_1
; RV64IXCHERI-NEXT:  .LBB159_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i32 @atomicrmw_umin_i32_monotonic(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i32_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB160_2
; RV32IXCHERI-NEXT:  .LBB160_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB160_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB160_4
; RV32IXCHERI-NEXT:  .LBB160_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bgeu s1, a3, .LBB160_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB160_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB160_1
; RV32IXCHERI-NEXT:  .LBB160_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i32_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camominu.w a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i32_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB160_2
; RV64IXCHERI-NEXT:  .LBB160_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB160_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB160_4
; RV64IXCHERI-NEXT:  .LBB160_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB160_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB160_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB160_1
; RV64IXCHERI-NEXT:  .LBB160_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i32_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.w a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i32 addrspace(200)* %a, i32 %b monotonic
  ret i32 %1
}

define i32 @atomicrmw_umin_i32_acquire(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i32_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB161_2
; RV32IXCHERI-NEXT:  .LBB161_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB161_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB161_4
; RV32IXCHERI-NEXT:  .LBB161_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bgeu s1, a3, .LBB161_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB161_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB161_1
; RV32IXCHERI-NEXT:  .LBB161_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i32_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camominu.w.aq a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i32_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB161_2
; RV64IXCHERI-NEXT:  .LBB161_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB161_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB161_4
; RV64IXCHERI-NEXT:  .LBB161_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB161_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB161_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB161_1
; RV64IXCHERI-NEXT:  .LBB161_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i32_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.w.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i32 addrspace(200)* %a, i32 %b acquire
  ret i32 %1
}

define i32 @atomicrmw_umin_i32_release(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i32_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB162_2
; RV32IXCHERI-NEXT:  .LBB162_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB162_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB162_4
; RV32IXCHERI-NEXT:  .LBB162_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bgeu s1, a3, .LBB162_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB162_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB162_1
; RV32IXCHERI-NEXT:  .LBB162_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i32_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camominu.w.rl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i32_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB162_2
; RV64IXCHERI-NEXT:  .LBB162_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB162_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB162_4
; RV64IXCHERI-NEXT:  .LBB162_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB162_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB162_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB162_1
; RV64IXCHERI-NEXT:  .LBB162_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i32_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.w.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i32 addrspace(200)* %a, i32 %b release
  ret i32 %1
}

define i32 @atomicrmw_umin_i32_acq_rel(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i32_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB163_2
; RV32IXCHERI-NEXT:  .LBB163_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB163_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB163_4
; RV32IXCHERI-NEXT:  .LBB163_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bgeu s1, a3, .LBB163_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB163_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB163_1
; RV32IXCHERI-NEXT:  .LBB163_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i32_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camominu.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i32_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB163_2
; RV64IXCHERI-NEXT:  .LBB163_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB163_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB163_4
; RV64IXCHERI-NEXT:  .LBB163_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB163_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB163_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB163_1
; RV64IXCHERI-NEXT:  .LBB163_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i32_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i32 addrspace(200)* %a, i32 %b acq_rel
  ret i32 %1
}

define i32 @atomicrmw_umin_i32_seq_cst(i32 addrspace(200)* %a, i32 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i32_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs2, ca0
; RV32IXCHERI-NEXT:    clw a3, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV32IXCHERI-NEXT:    j .LBB164_2
; RV32IXCHERI-NEXT:  .LBB164_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB164_2 Depth=1
; RV32IXCHERI-NEXT:    csw a3, 12(csp)
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs2
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV32IXCHERI-NEXT:    clw a3, 12(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB164_4
; RV32IXCHERI-NEXT:  .LBB164_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    mv a2, a3
; RV32IXCHERI-NEXT:    bgeu s1, a3, .LBB164_1
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB164_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s1
; RV32IXCHERI-NEXT:    j .LBB164_1
; RV32IXCHERI-NEXT:  .LBB164_4: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a3
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i32_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camominu.w.aqrl a0, a1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i32_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -96
; RV64IXCHERI-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv s2, a1
; RV64IXCHERI-NEXT:    cmove cs3, ca0
; RV64IXCHERI-NEXT:    clwu a1, 0(ca0)
; RV64IXCHERI-NEXT:    sext.w s1, s2
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 12
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 4
; RV64IXCHERI-NEXT:    j .LBB164_2
; RV64IXCHERI-NEXT:  .LBB164_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB164_2 Depth=1
; RV64IXCHERI-NEXT:    csw a1, 12(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs3
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_4
; RV64IXCHERI-NEXT:    clw a1, 12(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB164_4
; RV64IXCHERI-NEXT:  .LBB164_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    sext.w a0, a1
; RV64IXCHERI-NEXT:    mv a2, a1
; RV64IXCHERI-NEXT:    bgeu s1, a0, .LBB164_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB164_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s2
; RV64IXCHERI-NEXT:    j .LBB164_1
; RV64IXCHERI-NEXT:  .LBB164_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a1
; RV64IXCHERI-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 96
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i32_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.w.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i32 addrspace(200)* %a, i32 %b seq_cst
  ret i32 %1
}

define i64 @atomicrmw_xchg_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    mv a3, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_xchg_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 2
; RV32IAXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_xchg_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 3
; RV32IAXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_xchg_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 4
; RV32IAXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_xchg_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 5
; RV32IAXCHERI-NEXT:    ccall __atomic_exchange_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_add_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    mv a3, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_add_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 2
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_add_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 3
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_add_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 4
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_add_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_add_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_add_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 5
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_add_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_add_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_add_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoadd.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw add i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_sub_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    mv a3, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_sub_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 2
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_sub_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 3
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_sub_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 4
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_sub_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_sub_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_sub_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 5
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_sub_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_sub_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_sub_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    neg a1, a1
; RV64IAXCHERI-NEXT:    camoadd.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw sub i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_and_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    mv a3, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_and_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 2
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_and_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 3
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_and_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 4
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_and_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_and_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_and_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 5
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_and_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_and_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_and_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoand.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw and i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_nand_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    mv a3, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB185_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.d a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.d a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB185_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_nand_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 2
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB186_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.d.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.d a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB186_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_nand_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 3
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB187_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.d a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.d.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB187_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_nand_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 4
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB188_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.d.aq a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.d.rl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB188_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_nand_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_nand_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_nand_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 5
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_nand_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_nand_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_nand_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB189_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    clr.d.aqrl a2, (ca0)
; RV64IAXCHERI-NEXT:    and a3, a2, a1
; RV64IAXCHERI-NEXT:    not a3, a3
; RV64IAXCHERI-NEXT:    csc.d.aqrl a3, a3, (ca0)
; RV64IAXCHERI-NEXT:    bnez a3, .LBB189_1
; RV64IAXCHERI-NEXT:  # %bb.2:
; RV64IAXCHERI-NEXT:    mv a0, a2
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw nand i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_or_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    mv a3, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_or_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 2
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_or_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 3
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_or_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 4
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_or_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_or_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_or_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 5
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_or_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_or_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_or_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoor.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw or i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_xor_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    mv a3, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_xor_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 2
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_xor_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 3
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_xor_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 4
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_xor_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xor_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xor_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IAXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    addi a3, zero, 5
; RV32IAXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV32IAXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xor_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_fetch_xor_8
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xor_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoxor.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xor i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_max_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB200_2
; RV32IXCHERI-NEXT:  .LBB200_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    mv a5, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB200_7
; RV32IXCHERI-NEXT:  .LBB200_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB200_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB200_5
; RV32IXCHERI-NEXT:  .LBB200_4: # in Loop: Header=BB200_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB200_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB200_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB200_1
; RV32IXCHERI-NEXT:  .LBB200_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB200_2
; RV32IAXCHERI-NEXT:  .LBB200_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    mv a4, zero
; RV32IAXCHERI-NEXT:    mv a5, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB200_7
; RV32IAXCHERI-NEXT:  .LBB200_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB200_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB200_5
; RV32IAXCHERI-NEXT:  .LBB200_4: # in Loop: Header=BB200_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB200_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB200_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB200_1
; RV32IAXCHERI-NEXT:  .LBB200_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB200_2
; RV64IXCHERI-NEXT:  .LBB200_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB200_4
; RV64IXCHERI-NEXT:  .LBB200_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    blt s1, a3, .LBB200_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB200_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB200_1
; RV64IXCHERI-NEXT:  .LBB200_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_max_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB201_2
; RV32IXCHERI-NEXT:  .LBB201_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    addi a5, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB201_7
; RV32IXCHERI-NEXT:  .LBB201_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB201_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB201_5
; RV32IXCHERI-NEXT:  .LBB201_4: # in Loop: Header=BB201_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB201_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB201_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB201_1
; RV32IXCHERI-NEXT:  .LBB201_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB201_2
; RV32IAXCHERI-NEXT:  .LBB201_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 2
; RV32IAXCHERI-NEXT:    addi a5, zero, 2
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB201_7
; RV32IAXCHERI-NEXT:  .LBB201_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB201_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB201_5
; RV32IAXCHERI-NEXT:  .LBB201_4: # in Loop: Header=BB201_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB201_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB201_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB201_1
; RV32IAXCHERI-NEXT:  .LBB201_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB201_2
; RV64IXCHERI-NEXT:  .LBB201_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB201_4
; RV64IXCHERI-NEXT:  .LBB201_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    blt s1, a3, .LBB201_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB201_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB201_1
; RV64IXCHERI-NEXT:  .LBB201_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_max_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB202_2
; RV32IXCHERI-NEXT:  .LBB202_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a5, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB202_7
; RV32IXCHERI-NEXT:  .LBB202_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB202_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB202_5
; RV32IXCHERI-NEXT:  .LBB202_4: # in Loop: Header=BB202_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB202_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB202_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB202_1
; RV32IXCHERI-NEXT:  .LBB202_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB202_2
; RV32IAXCHERI-NEXT:  .LBB202_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 3
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    mv a5, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB202_7
; RV32IAXCHERI-NEXT:  .LBB202_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB202_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB202_5
; RV32IAXCHERI-NEXT:  .LBB202_4: # in Loop: Header=BB202_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB202_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB202_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB202_1
; RV32IAXCHERI-NEXT:  .LBB202_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB202_2
; RV64IXCHERI-NEXT:  .LBB202_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB202_4
; RV64IXCHERI-NEXT:  .LBB202_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    blt s1, a3, .LBB202_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB202_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB202_1
; RV64IXCHERI-NEXT:  .LBB202_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_max_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB203_2
; RV32IXCHERI-NEXT:  .LBB203_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 4
; RV32IXCHERI-NEXT:    addi a5, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB203_7
; RV32IXCHERI-NEXT:  .LBB203_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB203_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB203_5
; RV32IXCHERI-NEXT:  .LBB203_4: # in Loop: Header=BB203_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB203_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB203_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB203_1
; RV32IXCHERI-NEXT:  .LBB203_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB203_2
; RV32IAXCHERI-NEXT:  .LBB203_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 4
; RV32IAXCHERI-NEXT:    addi a5, zero, 2
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB203_7
; RV32IAXCHERI-NEXT:  .LBB203_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB203_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB203_5
; RV32IAXCHERI-NEXT:  .LBB203_4: # in Loop: Header=BB203_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB203_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB203_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB203_1
; RV32IAXCHERI-NEXT:  .LBB203_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB203_2
; RV64IXCHERI-NEXT:  .LBB203_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB203_4
; RV64IXCHERI-NEXT:  .LBB203_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    blt s1, a3, .LBB203_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB203_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB203_1
; RV64IXCHERI-NEXT:  .LBB203_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_max_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_max_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB204_2
; RV32IXCHERI-NEXT:  .LBB204_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    addi a5, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB204_7
; RV32IXCHERI-NEXT:  .LBB204_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB204_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB204_5
; RV32IXCHERI-NEXT:  .LBB204_4: # in Loop: Header=BB204_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB204_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB204_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB204_1
; RV32IXCHERI-NEXT:  .LBB204_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_max_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB204_2
; RV32IAXCHERI-NEXT:  .LBB204_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 5
; RV32IAXCHERI-NEXT:    addi a5, zero, 5
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB204_7
; RV32IAXCHERI-NEXT:  .LBB204_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB204_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB204_5
; RV32IAXCHERI-NEXT:  .LBB204_4: # in Loop: Header=BB204_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB204_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB204_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB204_1
; RV32IAXCHERI-NEXT:  .LBB204_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_max_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB204_2
; RV64IXCHERI-NEXT:  .LBB204_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB204_4
; RV64IXCHERI-NEXT:  .LBB204_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    blt s1, a3, .LBB204_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB204_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB204_1
; RV64IXCHERI-NEXT:  .LBB204_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_max_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomax.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw max i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_min_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB205_2
; RV32IXCHERI-NEXT:  .LBB205_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    mv a5, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB205_7
; RV32IXCHERI-NEXT:  .LBB205_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB205_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB205_5
; RV32IXCHERI-NEXT:  .LBB205_4: # in Loop: Header=BB205_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB205_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB205_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB205_1
; RV32IXCHERI-NEXT:  .LBB205_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB205_2
; RV32IAXCHERI-NEXT:  .LBB205_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    mv a4, zero
; RV32IAXCHERI-NEXT:    mv a5, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB205_7
; RV32IAXCHERI-NEXT:  .LBB205_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB205_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB205_5
; RV32IAXCHERI-NEXT:  .LBB205_4: # in Loop: Header=BB205_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB205_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB205_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB205_1
; RV32IAXCHERI-NEXT:  .LBB205_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB205_2
; RV64IXCHERI-NEXT:  .LBB205_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB205_4
; RV64IXCHERI-NEXT:  .LBB205_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bge s1, a3, .LBB205_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB205_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB205_1
; RV64IXCHERI-NEXT:  .LBB205_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_min_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB206_2
; RV32IXCHERI-NEXT:  .LBB206_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    addi a5, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB206_7
; RV32IXCHERI-NEXT:  .LBB206_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB206_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB206_5
; RV32IXCHERI-NEXT:  .LBB206_4: # in Loop: Header=BB206_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB206_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB206_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB206_1
; RV32IXCHERI-NEXT:  .LBB206_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB206_2
; RV32IAXCHERI-NEXT:  .LBB206_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 2
; RV32IAXCHERI-NEXT:    addi a5, zero, 2
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB206_7
; RV32IAXCHERI-NEXT:  .LBB206_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB206_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB206_5
; RV32IAXCHERI-NEXT:  .LBB206_4: # in Loop: Header=BB206_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB206_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB206_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB206_1
; RV32IAXCHERI-NEXT:  .LBB206_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB206_2
; RV64IXCHERI-NEXT:  .LBB206_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB206_4
; RV64IXCHERI-NEXT:  .LBB206_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bge s1, a3, .LBB206_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB206_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB206_1
; RV64IXCHERI-NEXT:  .LBB206_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_min_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB207_2
; RV32IXCHERI-NEXT:  .LBB207_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a5, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB207_7
; RV32IXCHERI-NEXT:  .LBB207_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB207_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB207_5
; RV32IXCHERI-NEXT:  .LBB207_4: # in Loop: Header=BB207_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB207_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB207_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB207_1
; RV32IXCHERI-NEXT:  .LBB207_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB207_2
; RV32IAXCHERI-NEXT:  .LBB207_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 3
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    mv a5, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB207_7
; RV32IAXCHERI-NEXT:  .LBB207_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB207_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB207_5
; RV32IAXCHERI-NEXT:  .LBB207_4: # in Loop: Header=BB207_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB207_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB207_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB207_1
; RV32IAXCHERI-NEXT:  .LBB207_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB207_2
; RV64IXCHERI-NEXT:  .LBB207_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB207_4
; RV64IXCHERI-NEXT:  .LBB207_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bge s1, a3, .LBB207_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB207_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB207_1
; RV64IXCHERI-NEXT:  .LBB207_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_min_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB208_2
; RV32IXCHERI-NEXT:  .LBB208_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 4
; RV32IXCHERI-NEXT:    addi a5, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB208_7
; RV32IXCHERI-NEXT:  .LBB208_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB208_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB208_5
; RV32IXCHERI-NEXT:  .LBB208_4: # in Loop: Header=BB208_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB208_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB208_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB208_1
; RV32IXCHERI-NEXT:  .LBB208_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB208_2
; RV32IAXCHERI-NEXT:  .LBB208_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 4
; RV32IAXCHERI-NEXT:    addi a5, zero, 2
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB208_7
; RV32IAXCHERI-NEXT:  .LBB208_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB208_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB208_5
; RV32IAXCHERI-NEXT:  .LBB208_4: # in Loop: Header=BB208_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB208_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB208_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB208_1
; RV32IAXCHERI-NEXT:  .LBB208_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB208_2
; RV64IXCHERI-NEXT:  .LBB208_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB208_4
; RV64IXCHERI-NEXT:  .LBB208_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bge s1, a3, .LBB208_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB208_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB208_1
; RV64IXCHERI-NEXT:  .LBB208_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_min_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_min_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB209_2
; RV32IXCHERI-NEXT:  .LBB209_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    addi a5, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB209_7
; RV32IXCHERI-NEXT:  .LBB209_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB209_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV32IXCHERI-NEXT:    slt a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB209_5
; RV32IXCHERI-NEXT:  .LBB209_4: # in Loop: Header=BB209_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB209_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB209_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB209_1
; RV32IXCHERI-NEXT:  .LBB209_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_min_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB209_2
; RV32IAXCHERI-NEXT:  .LBB209_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 5
; RV32IAXCHERI-NEXT:    addi a5, zero, 5
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB209_7
; RV32IAXCHERI-NEXT:  .LBB209_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB209_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV32IAXCHERI-NEXT:    slt a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB209_5
; RV32IAXCHERI-NEXT:  .LBB209_4: # in Loop: Header=BB209_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB209_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB209_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB209_1
; RV32IAXCHERI-NEXT:  .LBB209_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_min_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB209_2
; RV64IXCHERI-NEXT:  .LBB209_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB209_4
; RV64IXCHERI-NEXT:  .LBB209_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bge s1, a3, .LBB209_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB209_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB209_1
; RV64IXCHERI-NEXT:  .LBB209_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_min_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomin.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw min i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_umax_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB210_2
; RV32IXCHERI-NEXT:  .LBB210_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    mv a5, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB210_7
; RV32IXCHERI-NEXT:  .LBB210_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB210_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB210_5
; RV32IXCHERI-NEXT:  .LBB210_4: # in Loop: Header=BB210_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB210_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB210_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB210_1
; RV32IXCHERI-NEXT:  .LBB210_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB210_2
; RV32IAXCHERI-NEXT:  .LBB210_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    mv a4, zero
; RV32IAXCHERI-NEXT:    mv a5, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB210_7
; RV32IAXCHERI-NEXT:  .LBB210_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB210_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB210_5
; RV32IAXCHERI-NEXT:  .LBB210_4: # in Loop: Header=BB210_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB210_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB210_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB210_1
; RV32IAXCHERI-NEXT:  .LBB210_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB210_2
; RV64IXCHERI-NEXT:  .LBB210_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB210_4
; RV64IXCHERI-NEXT:  .LBB210_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bltu s1, a3, .LBB210_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB210_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB210_1
; RV64IXCHERI-NEXT:  .LBB210_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_umax_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB211_2
; RV32IXCHERI-NEXT:  .LBB211_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    addi a5, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB211_7
; RV32IXCHERI-NEXT:  .LBB211_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB211_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB211_5
; RV32IXCHERI-NEXT:  .LBB211_4: # in Loop: Header=BB211_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB211_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB211_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB211_1
; RV32IXCHERI-NEXT:  .LBB211_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB211_2
; RV32IAXCHERI-NEXT:  .LBB211_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 2
; RV32IAXCHERI-NEXT:    addi a5, zero, 2
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB211_7
; RV32IAXCHERI-NEXT:  .LBB211_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB211_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB211_5
; RV32IAXCHERI-NEXT:  .LBB211_4: # in Loop: Header=BB211_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB211_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB211_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB211_1
; RV32IAXCHERI-NEXT:  .LBB211_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB211_2
; RV64IXCHERI-NEXT:  .LBB211_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB211_4
; RV64IXCHERI-NEXT:  .LBB211_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bltu s1, a3, .LBB211_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB211_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB211_1
; RV64IXCHERI-NEXT:  .LBB211_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_umax_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB212_2
; RV32IXCHERI-NEXT:  .LBB212_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a5, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB212_7
; RV32IXCHERI-NEXT:  .LBB212_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB212_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB212_5
; RV32IXCHERI-NEXT:  .LBB212_4: # in Loop: Header=BB212_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB212_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB212_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB212_1
; RV32IXCHERI-NEXT:  .LBB212_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB212_2
; RV32IAXCHERI-NEXT:  .LBB212_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 3
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    mv a5, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB212_7
; RV32IAXCHERI-NEXT:  .LBB212_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB212_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB212_5
; RV32IAXCHERI-NEXT:  .LBB212_4: # in Loop: Header=BB212_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB212_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB212_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB212_1
; RV32IAXCHERI-NEXT:  .LBB212_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB212_2
; RV64IXCHERI-NEXT:  .LBB212_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB212_4
; RV64IXCHERI-NEXT:  .LBB212_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bltu s1, a3, .LBB212_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB212_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB212_1
; RV64IXCHERI-NEXT:  .LBB212_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_umax_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB213_2
; RV32IXCHERI-NEXT:  .LBB213_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 4
; RV32IXCHERI-NEXT:    addi a5, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB213_7
; RV32IXCHERI-NEXT:  .LBB213_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB213_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB213_5
; RV32IXCHERI-NEXT:  .LBB213_4: # in Loop: Header=BB213_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB213_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB213_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB213_1
; RV32IXCHERI-NEXT:  .LBB213_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB213_2
; RV32IAXCHERI-NEXT:  .LBB213_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 4
; RV32IAXCHERI-NEXT:    addi a5, zero, 2
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB213_7
; RV32IAXCHERI-NEXT:  .LBB213_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB213_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB213_5
; RV32IAXCHERI-NEXT:  .LBB213_4: # in Loop: Header=BB213_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB213_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB213_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB213_1
; RV32IAXCHERI-NEXT:  .LBB213_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB213_2
; RV64IXCHERI-NEXT:  .LBB213_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB213_4
; RV64IXCHERI-NEXT:  .LBB213_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bltu s1, a3, .LBB213_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB213_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB213_1
; RV64IXCHERI-NEXT:  .LBB213_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_umax_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umax_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB214_2
; RV32IXCHERI-NEXT:  .LBB214_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    addi a5, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB214_7
; RV32IXCHERI-NEXT:  .LBB214_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB214_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB214_5
; RV32IXCHERI-NEXT:  .LBB214_4: # in Loop: Header=BB214_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB214_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB214_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB214_1
; RV32IXCHERI-NEXT:  .LBB214_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umax_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB214_2
; RV32IAXCHERI-NEXT:  .LBB214_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 5
; RV32IAXCHERI-NEXT:    addi a5, zero, 5
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB214_7
; RV32IAXCHERI-NEXT:  .LBB214_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB214_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB214_5
; RV32IAXCHERI-NEXT:  .LBB214_4: # in Loop: Header=BB214_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB214_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB214_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB214_1
; RV32IAXCHERI-NEXT:  .LBB214_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umax_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB214_2
; RV64IXCHERI-NEXT:  .LBB214_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB214_4
; RV64IXCHERI-NEXT:  .LBB214_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bltu s1, a3, .LBB214_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB214_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB214_1
; RV64IXCHERI-NEXT:  .LBB214_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umax_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camomaxu.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umax i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i64 @atomicrmw_umin_i64_monotonic(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i64_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB215_2
; RV32IXCHERI-NEXT:  .LBB215_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    mv a5, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB215_7
; RV32IXCHERI-NEXT:  .LBB215_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB215_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB215_5
; RV32IXCHERI-NEXT:  .LBB215_4: # in Loop: Header=BB215_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB215_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB215_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB215_1
; RV32IXCHERI-NEXT:  .LBB215_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i64_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB215_2
; RV32IAXCHERI-NEXT:  .LBB215_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    mv a4, zero
; RV32IAXCHERI-NEXT:    mv a5, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB215_7
; RV32IAXCHERI-NEXT:  .LBB215_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB215_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB215_5
; RV32IAXCHERI-NEXT:  .LBB215_4: # in Loop: Header=BB215_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB215_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB215_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB215_1
; RV32IAXCHERI-NEXT:  .LBB215_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i64_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB215_2
; RV64IXCHERI-NEXT:  .LBB215_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB215_4
; RV64IXCHERI-NEXT:  .LBB215_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bgeu s1, a3, .LBB215_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB215_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB215_1
; RV64IXCHERI-NEXT:  .LBB215_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i64_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.d a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i64 addrspace(200)* %a, i64 %b monotonic
  ret i64 %1
}

define i64 @atomicrmw_umin_i64_acquire(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i64_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB216_2
; RV32IXCHERI-NEXT:  .LBB216_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    addi a5, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB216_7
; RV32IXCHERI-NEXT:  .LBB216_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB216_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB216_5
; RV32IXCHERI-NEXT:  .LBB216_4: # in Loop: Header=BB216_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB216_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB216_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB216_1
; RV32IXCHERI-NEXT:  .LBB216_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i64_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB216_2
; RV32IAXCHERI-NEXT:  .LBB216_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 2
; RV32IAXCHERI-NEXT:    addi a5, zero, 2
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB216_7
; RV32IAXCHERI-NEXT:  .LBB216_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB216_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB216_5
; RV32IAXCHERI-NEXT:  .LBB216_4: # in Loop: Header=BB216_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB216_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB216_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB216_1
; RV32IAXCHERI-NEXT:  .LBB216_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i64_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB216_2
; RV64IXCHERI-NEXT:  .LBB216_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB216_4
; RV64IXCHERI-NEXT:  .LBB216_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bgeu s1, a3, .LBB216_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB216_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB216_1
; RV64IXCHERI-NEXT:  .LBB216_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i64_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.d.aq a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i64 addrspace(200)* %a, i64 %b acquire
  ret i64 %1
}

define i64 @atomicrmw_umin_i64_release(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i64_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB217_2
; RV32IXCHERI-NEXT:  .LBB217_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 3
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    mv a5, zero
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB217_7
; RV32IXCHERI-NEXT:  .LBB217_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB217_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB217_5
; RV32IXCHERI-NEXT:  .LBB217_4: # in Loop: Header=BB217_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB217_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB217_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB217_1
; RV32IXCHERI-NEXT:  .LBB217_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i64_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB217_2
; RV32IAXCHERI-NEXT:  .LBB217_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 3
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    mv a5, zero
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB217_7
; RV32IAXCHERI-NEXT:  .LBB217_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB217_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB217_5
; RV32IAXCHERI-NEXT:  .LBB217_4: # in Loop: Header=BB217_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB217_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB217_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB217_1
; RV32IAXCHERI-NEXT:  .LBB217_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i64_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB217_2
; RV64IXCHERI-NEXT:  .LBB217_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB217_4
; RV64IXCHERI-NEXT:  .LBB217_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bgeu s1, a3, .LBB217_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB217_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB217_1
; RV64IXCHERI-NEXT:  .LBB217_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i64_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.d.rl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i64 addrspace(200)* %a, i64 %b release
  ret i64 %1
}

define i64 @atomicrmw_umin_i64_acq_rel(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i64_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB218_2
; RV32IXCHERI-NEXT:  .LBB218_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 4
; RV32IXCHERI-NEXT:    addi a5, zero, 2
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB218_7
; RV32IXCHERI-NEXT:  .LBB218_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB218_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB218_5
; RV32IXCHERI-NEXT:  .LBB218_4: # in Loop: Header=BB218_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB218_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB218_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB218_1
; RV32IXCHERI-NEXT:  .LBB218_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i64_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB218_2
; RV32IAXCHERI-NEXT:  .LBB218_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 4
; RV32IAXCHERI-NEXT:    addi a5, zero, 2
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB218_7
; RV32IAXCHERI-NEXT:  .LBB218_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB218_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB218_5
; RV32IAXCHERI-NEXT:  .LBB218_4: # in Loop: Header=BB218_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB218_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB218_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB218_1
; RV32IAXCHERI-NEXT:  .LBB218_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i64_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB218_2
; RV64IXCHERI-NEXT:  .LBB218_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB218_4
; RV64IXCHERI-NEXT:  .LBB218_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bgeu s1, a3, .LBB218_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB218_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB218_1
; RV64IXCHERI-NEXT:  .LBB218_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i64_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i64 addrspace(200)* %a, i64 %b acq_rel
  ret i64 %1
}

define i64 @atomicrmw_umin_i64_seq_cst(i64 addrspace(200)* %a, i64 %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_umin_i64_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    cmove cs3, ca0
; RV32IXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IXCHERI-NEXT:    mv s1, a2
; RV32IXCHERI-NEXT:    mv s2, a1
; RV32IXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IXCHERI-NEXT:    j .LBB219_2
; RV32IXCHERI-NEXT:  .LBB219_1: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV32IXCHERI-NEXT:    csw a4, 0(csp)
; RV32IXCHERI-NEXT:    csw a5, 4(csp)
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    addi a5, zero, 5
; RV32IXCHERI-NEXT:    cmove ca0, cs3
; RV32IXCHERI-NEXT:    cmove ca1, cs0
; RV32IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IXCHERI-NEXT:    clw a5, 4(csp)
; RV32IXCHERI-NEXT:    clw a4, 0(csp)
; RV32IXCHERI-NEXT:    bnez a0, .LBB219_7
; RV32IXCHERI-NEXT:  .LBB219_2: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IXCHERI-NEXT:    beq a5, s1, .LBB219_4
; RV32IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s1, a5
; RV32IXCHERI-NEXT:    j .LBB219_5
; RV32IXCHERI-NEXT:  .LBB219_4: # in Loop: Header=BB219_2 Depth=1
; RV32IXCHERI-NEXT:    sltu a0, s2, a4
; RV32IXCHERI-NEXT:  .LBB219_5: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV32IXCHERI-NEXT:    xori a0, a0, 1
; RV32IXCHERI-NEXT:    mv a2, a4
; RV32IXCHERI-NEXT:    mv a3, a5
; RV32IXCHERI-NEXT:    bnez a0, .LBB219_1
; RV32IXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV32IXCHERI-NEXT:    mv a2, s2
; RV32IXCHERI-NEXT:    mv a3, s1
; RV32IXCHERI-NEXT:    j .LBB219_1
; RV32IXCHERI-NEXT:  .LBB219_7: # %atomicrmw.end
; RV32IXCHERI-NEXT:    mv a0, a4
; RV32IXCHERI-NEXT:    mv a1, a5
; RV32IXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_umin_i64_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, -48
; RV32IAXCHERI-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; RV32IAXCHERI-NEXT:    cmove cs3, ca0
; RV32IAXCHERI-NEXT:    clw a5, 4(ca0)
; RV32IAXCHERI-NEXT:    clw a4, 0(ca0)
; RV32IAXCHERI-NEXT:    mv s1, a2
; RV32IAXCHERI-NEXT:    mv s2, a1
; RV32IAXCHERI-NEXT:    cincoffset ca0, csp, 0
; RV32IAXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV32IAXCHERI-NEXT:    j .LBB219_2
; RV32IAXCHERI-NEXT:  .LBB219_1: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV32IAXCHERI-NEXT:    csw a4, 0(csp)
; RV32IAXCHERI-NEXT:    csw a5, 4(csp)
; RV32IAXCHERI-NEXT:    addi a4, zero, 5
; RV32IAXCHERI-NEXT:    addi a5, zero, 5
; RV32IAXCHERI-NEXT:    cmove ca0, cs3
; RV32IAXCHERI-NEXT:    cmove ca1, cs0
; RV32IAXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV32IAXCHERI-NEXT:    clw a5, 4(csp)
; RV32IAXCHERI-NEXT:    clw a4, 0(csp)
; RV32IAXCHERI-NEXT:    bnez a0, .LBB219_7
; RV32IAXCHERI-NEXT:  .LBB219_2: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    beq a5, s1, .LBB219_4
; RV32IAXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s1, a5
; RV32IAXCHERI-NEXT:    j .LBB219_5
; RV32IAXCHERI-NEXT:  .LBB219_4: # in Loop: Header=BB219_2 Depth=1
; RV32IAXCHERI-NEXT:    sltu a0, s2, a4
; RV32IAXCHERI-NEXT:  .LBB219_5: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV32IAXCHERI-NEXT:    xori a0, a0, 1
; RV32IAXCHERI-NEXT:    mv a2, a4
; RV32IAXCHERI-NEXT:    mv a3, a5
; RV32IAXCHERI-NEXT:    bnez a0, .LBB219_1
; RV32IAXCHERI-NEXT:  # %bb.6: # %atomicrmw.start
; RV32IAXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV32IAXCHERI-NEXT:    mv a2, s2
; RV32IAXCHERI-NEXT:    mv a3, s1
; RV32IAXCHERI-NEXT:    j .LBB219_1
; RV32IAXCHERI-NEXT:  .LBB219_7: # %atomicrmw.end
; RV32IAXCHERI-NEXT:    mv a0, a4
; RV32IAXCHERI-NEXT:    mv a1, a5
; RV32IAXCHERI-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; RV32IAXCHERI-NEXT:    cincoffset csp, csp, 48
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_umin_i64_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -80
; RV64IXCHERI-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs1, 32(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    csc cs2, 16(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    cmove cs2, ca0
; RV64IXCHERI-NEXT:    cld a3, 0(ca0)
; RV64IXCHERI-NEXT:    mv s1, a1
; RV64IXCHERI-NEXT:    cincoffset ca0, csp, 8
; RV64IXCHERI-NEXT:    csetbounds cs0, ca0, 8
; RV64IXCHERI-NEXT:    j .LBB219_2
; RV64IXCHERI-NEXT:  .LBB219_1: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV64IXCHERI-NEXT:    csd a3, 8(csp)
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    cmove ca0, cs2
; RV64IXCHERI-NEXT:    cmove ca1, cs0
; RV64IXCHERI-NEXT:    ccall __atomic_compare_exchange_8
; RV64IXCHERI-NEXT:    cld a3, 8(csp)
; RV64IXCHERI-NEXT:    bnez a0, .LBB219_4
; RV64IXCHERI-NEXT:  .LBB219_2: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64IXCHERI-NEXT:    mv a2, a3
; RV64IXCHERI-NEXT:    bgeu s1, a3, .LBB219_1
; RV64IXCHERI-NEXT:  # %bb.3: # %atomicrmw.start
; RV64IXCHERI-NEXT:    # in Loop: Header=BB219_2 Depth=1
; RV64IXCHERI-NEXT:    mv a2, s1
; RV64IXCHERI-NEXT:    j .LBB219_1
; RV64IXCHERI-NEXT:  .LBB219_4: # %atomicrmw.end
; RV64IXCHERI-NEXT:    mv a0, a3
; RV64IXCHERI-NEXT:    clc cs2, 16(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs1, 32(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 80
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_umin_i64_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camominu.d.aqrl a0, a1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw umin i64 addrspace(200)* %a, i64 %b seq_cst
  ret i64 %1
}

define i8 addrspace(200)* @atomicrmw_xchg_cap_monotonic(i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_cap_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    mv a2, zero
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_cap_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.c ca0, ca1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_cap_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    mv a2, zero
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_cap_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.c ca0, ca1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b monotonic
  ret i8 addrspace(200)* %1
}

define i8 addrspace(200)* @atomicrmw_xchg_cap_acquire(i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_cap_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 2
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_cap_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.c.aq ca0, ca1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_cap_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 2
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_cap_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.c.aq ca0, ca1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b acquire
  ret i8 addrspace(200)* %1
}

define i8 addrspace(200)* @atomicrmw_xchg_cap_release(i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_cap_release:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 3
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_cap_release:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.c.rl ca0, ca1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_cap_release:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 3
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_cap_release:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.c.rl ca0, ca1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b release
  ret i8 addrspace(200)* %1
}

define i8 addrspace(200)* @atomicrmw_xchg_cap_acq_rel(i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_cap_acq_rel:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 4
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_cap_acq_rel:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.c.aqrl ca0, ca1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_cap_acq_rel:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 4
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_cap_acq_rel:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.c.aqrl ca0, ca1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b acq_rel
  ret i8 addrspace(200)* %1
}

define i8 addrspace(200)* @atomicrmw_xchg_cap_seq_cst(i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b) nounwind {
; RV32IXCHERI-LABEL: atomicrmw_xchg_cap_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV32IXCHERI-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; RV32IXCHERI-NEXT:    addi a2, zero, 5
; RV32IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV32IXCHERI-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; RV32IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV32IXCHERI-NEXT:    cret
;
; RV32IAXCHERI-LABEL: atomicrmw_xchg_cap_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:    camoswap.c.aqrl ca0, ca1, (ca0)
; RV32IAXCHERI-NEXT:    cret
;
; RV64IXCHERI-LABEL: atomicrmw_xchg_cap_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    cincoffset csp, csp, -16
; RV64IXCHERI-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; RV64IXCHERI-NEXT:    addi a2, zero, 5
; RV64IXCHERI-NEXT:    ccall __atomic_exchange_cap
; RV64IXCHERI-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; RV64IXCHERI-NEXT:    cincoffset csp, csp, 16
; RV64IXCHERI-NEXT:    cret
;
; RV64IAXCHERI-LABEL: atomicrmw_xchg_cap_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:    camoswap.c.aqrl ca0, ca1, (ca0)
; RV64IAXCHERI-NEXT:    cret
  %1 = atomicrmw xchg i8 addrspace(200)* addrspace(200)* %a, i8 addrspace(200)* %b seq_cst
  ret i8 addrspace(200)* %1
}
