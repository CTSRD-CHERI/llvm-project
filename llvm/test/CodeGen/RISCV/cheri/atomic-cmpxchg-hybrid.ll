; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: %riscv32_cheri_llc -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV32IXCHERI %s
; RUN: %riscv32_cheri_llc -mattr=+a -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV32IAXCHERI %s
; RUN: %riscv64_cheri_llc -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV64IXCHERI %s
; RUN: %riscv64_cheri_llc -mattr=+a -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV64IAXCHERI %s

define void @cmpxchg_cap_monotonic_monotonic(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_monotonic_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    mv a3, zero
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_monotonic_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB0_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB0_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB0_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB0_1
; RV32IAXCHERI-NEXT:  .LBB0_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_monotonic_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    mv a3, zero
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_monotonic_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB0_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB0_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB0_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB0_1
; RV64IAXCHERI-NEXT:  .LBB0_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val monotonic monotonic
  ret void
}

define void @cmpxchg_cap_acquire_monotonic(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_acquire_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_acquire_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB1_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB1_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB1_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB1_1
; RV32IAXCHERI-NEXT:  .LBB1_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_acquire_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_acquire_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB1_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB1_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB1_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB1_1
; RV64IAXCHERI-NEXT:  .LBB1_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val acquire monotonic
  ret void
}

define void @cmpxchg_cap_acquire_acquire(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_acquire_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    addi a3, zero, 2
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_acquire_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB2_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB2_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB2_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB2_1
; RV32IAXCHERI-NEXT:  .LBB2_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_acquire_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    addi a3, zero, 2
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_acquire_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB2_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB2_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB2_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB2_1
; RV64IAXCHERI-NEXT:  .LBB2_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val acquire acquire
  ret void
}

define void @cmpxchg_cap_release_monotonic(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_release_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_release_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB3_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c.rl ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB3_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB3_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB3_1
; RV32IAXCHERI-NEXT:  .LBB3_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_release_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_release_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB3_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c.rl ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB3_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB3_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB3_1
; RV64IAXCHERI-NEXT:  .LBB3_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val release monotonic
  ret void
}

define void @cmpxchg_cap_release_acquire(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_release_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    addi a3, zero, 3
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_release_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB4_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB4_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB4_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB4_1
; RV32IAXCHERI-NEXT:  .LBB4_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_release_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    addi a3, zero, 3
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_release_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB4_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB4_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB4_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB4_1
; RV64IAXCHERI-NEXT:  .LBB4_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val release acquire
  ret void
}

define void @cmpxchg_cap_acq_rel_monotonic(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_acq_rel_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_acq_rel_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB5_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB5_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB5_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB5_1
; RV32IAXCHERI-NEXT:  .LBB5_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_acq_rel_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_acq_rel_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB5_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB5_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB5_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB5_1
; RV64IAXCHERI-NEXT:  .LBB5_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val acq_rel monotonic
  ret void
}

define void @cmpxchg_cap_acq_rel_acquire(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_acq_rel_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    addi a3, zero, 4
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_acq_rel_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB6_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB6_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB6_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB6_1
; RV32IAXCHERI-NEXT:  .LBB6_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_acq_rel_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    addi a3, zero, 4
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_acq_rel_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB6_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c.aq ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB6_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB6_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c.aq a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB6_1
; RV64IAXCHERI-NEXT:  .LBB6_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val acq_rel acquire
  ret void
}

define void @cmpxchg_cap_seq_cst_monotonic(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_seq_cst_monotonic:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    mv a4, zero
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_seq_cst_monotonic:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB7_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c.aqrl ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB7_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB7_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c.aqrl a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB7_1
; RV32IAXCHERI-NEXT:  .LBB7_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_seq_cst_monotonic:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    mv a4, zero
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_seq_cst_monotonic:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB7_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c.aqrl ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB7_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB7_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c.aqrl a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB7_1
; RV64IAXCHERI-NEXT:  .LBB7_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val seq_cst monotonic
  ret void
}

define void @cmpxchg_cap_seq_cst_acquire(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_seq_cst_acquire:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 2
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_seq_cst_acquire:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB8_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c.aqrl ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB8_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB8_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c.aqrl a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB8_1
; RV32IAXCHERI-NEXT:  .LBB8_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_seq_cst_acquire:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 2
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_seq_cst_acquire:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB8_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c.aqrl ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB8_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB8_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c.aqrl a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB8_1
; RV64IAXCHERI-NEXT:  .LBB8_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val seq_cst acquire
  ret void
}

define void @cmpxchg_cap_seq_cst_seq_cst(i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val) nounwind {
; RV32IXCHERI-LABEL: cmpxchg_cap_seq_cst_seq_cst:
; RV32IXCHERI:       # %bb.0:
; RV32IXCHERI-NEXT:    addi sp, sp, -16
; RV32IXCHERI-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IXCHERI-NEXT:    sc ca1, 0(sp)
; RV32IXCHERI-NEXT:    mv a1, sp
; RV32IXCHERI-NEXT:    addi a3, zero, 5
; RV32IXCHERI-NEXT:    addi a4, zero, 5
; RV32IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV32IXCHERI-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IXCHERI-NEXT:    addi sp, sp, 16
; RV32IXCHERI-NEXT:    ret
;
; RV32IAXCHERI-LABEL: cmpxchg_cap_seq_cst_seq_cst:
; RV32IAXCHERI:       # %bb.0:
; RV32IAXCHERI-NEXT:  .LBB9_1: # =>This Inner Loop Header: Depth=1
; RV32IAXCHERI-NEXT:    lr.c.aqrl ca3, (a0)
; RV32IAXCHERI-NEXT:    bne a3, a1, .LBB9_3
; RV32IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB9_1 Depth=1
; RV32IAXCHERI-NEXT:    sc.c.aqrl a4, ca2, (a0)
; RV32IAXCHERI-NEXT:    bnez a4, .LBB9_1
; RV32IAXCHERI-NEXT:  .LBB9_3:
; RV32IAXCHERI-NEXT:    ret
;
; RV64IXCHERI-LABEL: cmpxchg_cap_seq_cst_seq_cst:
; RV64IXCHERI:       # %bb.0:
; RV64IXCHERI-NEXT:    addi sp, sp, -32
; RV64IXCHERI-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64IXCHERI-NEXT:    sc ca1, 0(sp)
; RV64IXCHERI-NEXT:    mv a1, sp
; RV64IXCHERI-NEXT:    addi a3, zero, 5
; RV64IXCHERI-NEXT:    addi a4, zero, 5
; RV64IXCHERI-NEXT:    call __atomic_compare_exchange_cap@plt
; RV64IXCHERI-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64IXCHERI-NEXT:    addi sp, sp, 32
; RV64IXCHERI-NEXT:    ret
;
; RV64IAXCHERI-LABEL: cmpxchg_cap_seq_cst_seq_cst:
; RV64IAXCHERI:       # %bb.0:
; RV64IAXCHERI-NEXT:  .LBB9_1: # =>This Inner Loop Header: Depth=1
; RV64IAXCHERI-NEXT:    lr.c.aqrl ca3, (a0)
; RV64IAXCHERI-NEXT:    bne a3, a1, .LBB9_3
; RV64IAXCHERI-NEXT:  # %bb.2: # in Loop: Header=BB9_1 Depth=1
; RV64IAXCHERI-NEXT:    sc.c.aqrl a4, ca2, (a0)
; RV64IAXCHERI-NEXT:    bnez a4, .LBB9_1
; RV64IAXCHERI-NEXT:  .LBB9_3:
; RV64IAXCHERI-NEXT:    ret
  %res = cmpxchg i8 addrspace(200)** %ptr, i8 addrspace(200)* %cmp, i8 addrspace(200)* %val seq_cst seq_cst
  ret void
}
