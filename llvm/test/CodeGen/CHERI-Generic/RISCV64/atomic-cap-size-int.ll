; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/atomic-cap-size-int.ll
;; Check that we can atomically update i128 (i64 for 32-bit systems)
;; For systems without double-width atomics (RISC-V, MIPS) we can use capability atomics
;; This is needed so we can report true for __atomic_always_lock_free(sizeof(uintptr_t), 0)
; RUN: opt -data-layout="e-m:e-pf200:128:128:128:64-p:64:64-i64:64-i128:128-n64-S128-A200-P200-G200" -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -atomic-expand -S -mattr=+a < %s | FileCheck %s --check-prefix=PURECAP-IR
; RUN: opt -data-layout="e-m:e-pf200:128:128:128:64-p:64:64-i64:64-i128:128-n64-S128" -mtriple=riscv64 --relocation-model=pic -target-abi lp64d -mattr=+xcheri,+f,+d -atomic-expand -S -mattr=+a < %s | FileCheck %s --check-prefix=HYBRID-IR
; RUN: llc -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -mattr=+a < %s | FileCheck %s --check-prefixes=PURECAP,PURECAP-ATOMICS --allow-unused-prefixes
; RUN: llc -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -mattr=-a < %s | FileCheck %s --check-prefixes=PURECAP,PURECAP-LIBCALLS --allow-unused-prefixes
; RUN: sed 's/addrspace(200)/addrspace(0)/g' %s | llc -mtriple=riscv64 --relocation-model=pic -target-abi lp64d -mattr=+xcheri,+f,+d -mattr=+a | FileCheck %s --check-prefixes=HYBRID,HYBRID-ATOMICS --allow-unused-prefixes
; RUN: sed 's/addrspace(200)/addrspace(0)/g' %s | llc -mtriple=riscv64 --relocation-model=pic -target-abi lp64d -mattr=+xcheri,+f,+d -mattr=-a | FileCheck %s --check-prefixes=HYBRID,HYBRID-LIBCALLS --allow-unused-prefixes
; RUN: llc -mtriple=riscv64 --relocation-model=pic -target-abi lp64d -mattr=+xcheri,+f,+d -mattr=+a < %s | FileCheck %s --check-prefixes=HYBRID-CAP-PTR,HYBRID-CAP-PTR-ATOMICS --allow-unused-prefixes
; RUN: llc -mtriple=riscv64 --relocation-model=pic -target-abi lp64d -mattr=+xcheri,+f,+d -mattr=-a < %s | FileCheck %s --check-prefixes=HYBRID-CAP-PTR,HYBRID-CAP-PTR-LIBCALLS --allow-unused-prefixes

define i128 @store(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: store:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -48
; PURECAP-NEXT:    csc cra, 32(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 16(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 0(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    mv s0, a2
; PURECAP-NEXT:    mv s1, a1
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_store_16
; PURECAP-NEXT:    mv a0, s1
; PURECAP-NEXT:    mv a1, s0
; PURECAP-NEXT:    clc cra, 32(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 16(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 0(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 48
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: store:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -32
; HYBRID-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s0, 16(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s1, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    mv s0, a2
; HYBRID-NEXT:    mv s1, a1
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_store_16@plt
; HYBRID-NEXT:    mv a0, s1
; HYBRID-NEXT:    mv a1, s0
; HYBRID-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s0, 16(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s1, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 32
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: store:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -32
; HYBRID-CAP-PTR-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s0, 16(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s1, 8(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_store_16_c@plt
; HYBRID-CAP-PTR-NEXT:    mv a0, s1
; HYBRID-CAP-PTR-NEXT:    mv a1, s0
; HYBRID-CAP-PTR-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s0, 16(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s1, 8(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 32
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@store
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0:[0-9]+]] {
; PURECAP-IR-NEXT:    call void @__atomic_store_16(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i128 [[VAL]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@store
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0:[0-9]+]] {
; HYBRID-IR-NEXT:    call void @__atomic_store_16_c(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i128 [[VAL]]
;
  store atomic i128 %val, ptr addrspace(200) %ptr seq_cst, align 16
  ret i128 %val
}

define i128 @load(ptr addrspace(200) %ptr) nounwind {
; PURECAP-LABEL: load:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    li a1, 5
; PURECAP-NEXT:    ccall __atomic_load_16
; PURECAP-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: load:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    li a1, 5
; HYBRID-NEXT:    call __atomic_load_16@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: load:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a1, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_load_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@load
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_load_16(ptr addrspace(200) [[PTR]], i32 5)
; PURECAP-IR-NEXT:    ret i128 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@load
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_load_16_c(ptr addrspace(200) [[PTR]], i32 5)
; HYBRID-IR-NEXT:    ret i128 [[TMP1]]
;
  %val = load atomic i128, ptr addrspace(200) %ptr seq_cst, align 16
  ret i128 %val
}

define i128 @atomic_xchg(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_xchg:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_exchange_16
; PURECAP-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_xchg:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_exchange_16@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_xchg:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_exchange_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_xchg
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_exchange_16(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i128 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_xchg
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_exchange_16_c(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i128 [[TMP1]]
;
  %tmp = atomicrmw xchg ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_add(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_add:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_add_16
; PURECAP-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_add:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_add_16@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_add:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_add_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_add
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_add_16(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i128 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_add
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_add_16_c(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i128 [[TMP1]]
;
  %tmp = atomicrmw add ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_sub(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_sub:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_sub_16
; PURECAP-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_sub:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_sub_16@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_sub:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_sub_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_sub
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_sub_16(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i128 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_sub
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_sub_16_c(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i128 [[TMP1]]
;
  %tmp = atomicrmw sub ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_and(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_and:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_and_16
; PURECAP-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_and:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_and_16@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_and:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_and_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_and
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_and_16(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i128 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_and
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_and_16_c(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i128 [[TMP1]]
;
  %tmp = atomicrmw and ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_nand(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_nand:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_nand_16
; PURECAP-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_nand:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_nand_16@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_nand:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_nand_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_nand
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_nand_16(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i128 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_nand
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_nand_16_c(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i128 [[TMP1]]
;
  %tmp = atomicrmw nand ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_or(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_or:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_or_16
; PURECAP-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_or:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_or_16@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_or:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_or_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_or
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_or_16(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i128 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_or
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_or_16_c(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i128 [[TMP1]]
;
  %tmp = atomicrmw or ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_xor(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_xor:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 0(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_xor_16
; PURECAP-NEXT:    clc cra, 0(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_xor:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_xor_16@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_xor:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_xor_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_xor
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_xor_16(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i128 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_xor
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i128 @__atomic_fetch_xor_16_c(ptr addrspace(200) [[PTR]], i128 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i128 [[TMP1]]
;
  %tmp = atomicrmw xor ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_max(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_max:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -96
; PURECAP-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    cmove cs3, ca0
; PURECAP-NEXT:    cld a5, 8(ca0)
; PURECAP-NEXT:    cld a4, 0(ca0)
; PURECAP-NEXT:    mv s1, a2
; PURECAP-NEXT:    mv s2, a1
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds cs0, ca0, 16
; PURECAP-NEXT:    j .LBB9_2
; PURECAP-NEXT:  .LBB9_1: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    csd a4, 0(csp)
; PURECAP-NEXT:    csd a5, 8(csp)
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, cs3
; PURECAP-NEXT:    cmove ca1, cs0
; PURECAP-NEXT:    ccall __atomic_compare_exchange_16
; PURECAP-NEXT:    cld a5, 8(csp)
; PURECAP-NEXT:    cld a4, 0(csp)
; PURECAP-NEXT:    bnez a0, .LBB9_7
; PURECAP-NEXT:  .LBB9_2: # %atomicrmw.start
; PURECAP-NEXT:    # =>This Inner Loop Header: Depth=1
; PURECAP-NEXT:    beq a5, s1, .LBB9_4
; PURECAP-NEXT:  # %bb.3: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    slt a0, s1, a5
; PURECAP-NEXT:    j .LBB9_5
; PURECAP-NEXT:  .LBB9_4: # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    sltu a0, s2, a4
; PURECAP-NEXT:  .LBB9_5: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    mv a2, a4
; PURECAP-NEXT:    mv a3, a5
; PURECAP-NEXT:    bnez a0, .LBB9_1
; PURECAP-NEXT:  # %bb.6: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    mv a2, s2
; PURECAP-NEXT:    mv a3, s1
; PURECAP-NEXT:    j .LBB9_1
; PURECAP-NEXT:  .LBB9_7: # %atomicrmw.end
; PURECAP-NEXT:    mv a0, a4
; PURECAP-NEXT:    mv a1, a5
; PURECAP-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 96
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_max:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -48
; HYBRID-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s2, 16(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    ld a5, 8(a0)
; HYBRID-NEXT:    ld a4, 0(a0)
; HYBRID-NEXT:    mv s1, a2
; HYBRID-NEXT:    mv s2, a1
; HYBRID-NEXT:    j .LBB9_2
; HYBRID-NEXT:  .LBB9_1: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    sd a4, 0(sp)
; HYBRID-NEXT:    sd a5, 8(sp)
; HYBRID-NEXT:    mv a1, sp
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, s0
; HYBRID-NEXT:    call __atomic_compare_exchange_16@plt
; HYBRID-NEXT:    ld a5, 8(sp)
; HYBRID-NEXT:    ld a4, 0(sp)
; HYBRID-NEXT:    bnez a0, .LBB9_7
; HYBRID-NEXT:  .LBB9_2: # %atomicrmw.start
; HYBRID-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-NEXT:    beq a5, s1, .LBB9_4
; HYBRID-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    slt a0, s1, a5
; HYBRID-NEXT:    j .LBB9_5
; HYBRID-NEXT:  .LBB9_4: # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    sltu a0, s2, a4
; HYBRID-NEXT:  .LBB9_5: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    mv a2, a4
; HYBRID-NEXT:    mv a3, a5
; HYBRID-NEXT:    bnez a0, .LBB9_1
; HYBRID-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    mv a2, s2
; HYBRID-NEXT:    mv a3, s1
; HYBRID-NEXT:    j .LBB9_1
; HYBRID-NEXT:  .LBB9_7: # %atomicrmw.end
; HYBRID-NEXT:    mv a0, a4
; HYBRID-NEXT:    mv a1, a5
; HYBRID-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s2, 16(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 48
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_max:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -64
; HYBRID-CAP-PTR-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    ld.cap a4, (ca0)
; HYBRID-CAP-PTR-NEXT:    sc ca0, 0(sp) # 16-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    cincoffset ca0, ca0, 8
; HYBRID-CAP-PTR-NEXT:    ld.cap a5, (ca0)
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    j .LBB9_2
; HYBRID-CAP-PTR-NEXT:  .LBB9_1: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sd a4, 16(sp)
; HYBRID-CAP-PTR-NEXT:    sd a5, 24(sp)
; HYBRID-CAP-PTR-NEXT:    addi a1, sp, 16
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    lc ca0, 0(sp) # 16-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld a5, 24(sp)
; HYBRID-CAP-PTR-NEXT:    ld a4, 16(sp)
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB9_7
; HYBRID-CAP-PTR-NEXT:  .LBB9_2: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-CAP-PTR-NEXT:    beq a5, s0, .LBB9_4
; HYBRID-CAP-PTR-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    slt a0, s0, a5
; HYBRID-CAP-PTR-NEXT:    j .LBB9_5
; HYBRID-CAP-PTR-NEXT:  .LBB9_4: # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s1, a4
; HYBRID-CAP-PTR-NEXT:  .LBB9_5: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, a4
; HYBRID-CAP-PTR-NEXT:    mv a3, a5
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB9_1
; HYBRID-CAP-PTR-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, s1
; HYBRID-CAP-PTR-NEXT:    mv a3, s0
; HYBRID-CAP-PTR-NEXT:    j .LBB9_1
; HYBRID-CAP-PTR-NEXT:  .LBB9_7: # %atomicrmw.end
; HYBRID-CAP-PTR-NEXT:    mv a0, a4
; HYBRID-CAP-PTR-NEXT:    mv a1, a5
; HYBRID-CAP-PTR-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 64
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_max
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16, addrspace(200)
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = load i128, ptr addrspace(200) [[PTR]], align 16
; PURECAP-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; PURECAP-IR:       atomicrmw.start:
; PURECAP-IR-NEXT:    [[LOADED:%.*]] = phi i128 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = icmp sgt i128 [[LOADED]], [[VAL]]
; PURECAP-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i128 [[LOADED]], i128 [[VAL]]
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i128 [[LOADED]], ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_16(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = load i128, ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP5]], 0
; PURECAP-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i128, i1 } [[TMP6]], i1 [[TMP4]], 1
; PURECAP-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i128, i1 } [[TMP7]], 1
; PURECAP-IR-NEXT:    [[NEWLOADED]] = extractvalue { i128, i1 } [[TMP7]], 0
; PURECAP-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; PURECAP-IR:       atomicrmw.end:
; PURECAP-IR-NEXT:    ret i128 [[NEWLOADED]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_max
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = load i128, ptr addrspace(200) [[PTR]], align 16
; HYBRID-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; HYBRID-IR:       atomicrmw.start:
; HYBRID-IR-NEXT:    [[LOADED:%.*]] = phi i128 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = icmp sgt i128 [[LOADED]], [[VAL]]
; HYBRID-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i128 [[LOADED]], i128 [[VAL]]
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i128 [[LOADED]], ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_16_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = load i128, ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP5]], 0
; HYBRID-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i128, i1 } [[TMP6]], i1 [[TMP4]], 1
; HYBRID-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i128, i1 } [[TMP7]], 1
; HYBRID-IR-NEXT:    [[NEWLOADED]] = extractvalue { i128, i1 } [[TMP7]], 0
; HYBRID-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; HYBRID-IR:       atomicrmw.end:
; HYBRID-IR-NEXT:    ret i128 [[NEWLOADED]]
;
  %tmp = atomicrmw max ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_min(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_min:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -96
; PURECAP-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    cmove cs3, ca0
; PURECAP-NEXT:    cld a5, 8(ca0)
; PURECAP-NEXT:    cld a4, 0(ca0)
; PURECAP-NEXT:    mv s1, a2
; PURECAP-NEXT:    mv s2, a1
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds cs0, ca0, 16
; PURECAP-NEXT:    j .LBB10_2
; PURECAP-NEXT:  .LBB10_1: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    csd a4, 0(csp)
; PURECAP-NEXT:    csd a5, 8(csp)
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, cs3
; PURECAP-NEXT:    cmove ca1, cs0
; PURECAP-NEXT:    ccall __atomic_compare_exchange_16
; PURECAP-NEXT:    cld a5, 8(csp)
; PURECAP-NEXT:    cld a4, 0(csp)
; PURECAP-NEXT:    bnez a0, .LBB10_7
; PURECAP-NEXT:  .LBB10_2: # %atomicrmw.start
; PURECAP-NEXT:    # =>This Inner Loop Header: Depth=1
; PURECAP-NEXT:    beq a5, s1, .LBB10_4
; PURECAP-NEXT:  # %bb.3: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    slt a0, s1, a5
; PURECAP-NEXT:    j .LBB10_5
; PURECAP-NEXT:  .LBB10_4: # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    sltu a0, s2, a4
; PURECAP-NEXT:  .LBB10_5: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    xori a0, a0, 1
; PURECAP-NEXT:    mv a2, a4
; PURECAP-NEXT:    mv a3, a5
; PURECAP-NEXT:    bnez a0, .LBB10_1
; PURECAP-NEXT:  # %bb.6: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    mv a2, s2
; PURECAP-NEXT:    mv a3, s1
; PURECAP-NEXT:    j .LBB10_1
; PURECAP-NEXT:  .LBB10_7: # %atomicrmw.end
; PURECAP-NEXT:    mv a0, a4
; PURECAP-NEXT:    mv a1, a5
; PURECAP-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 96
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_min:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -48
; HYBRID-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s2, 16(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    ld a5, 8(a0)
; HYBRID-NEXT:    ld a4, 0(a0)
; HYBRID-NEXT:    mv s1, a2
; HYBRID-NEXT:    mv s2, a1
; HYBRID-NEXT:    j .LBB10_2
; HYBRID-NEXT:  .LBB10_1: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    sd a4, 0(sp)
; HYBRID-NEXT:    sd a5, 8(sp)
; HYBRID-NEXT:    mv a1, sp
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, s0
; HYBRID-NEXT:    call __atomic_compare_exchange_16@plt
; HYBRID-NEXT:    ld a5, 8(sp)
; HYBRID-NEXT:    ld a4, 0(sp)
; HYBRID-NEXT:    bnez a0, .LBB10_7
; HYBRID-NEXT:  .LBB10_2: # %atomicrmw.start
; HYBRID-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-NEXT:    beq a5, s1, .LBB10_4
; HYBRID-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    slt a0, s1, a5
; HYBRID-NEXT:    j .LBB10_5
; HYBRID-NEXT:  .LBB10_4: # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    sltu a0, s2, a4
; HYBRID-NEXT:  .LBB10_5: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    xori a0, a0, 1
; HYBRID-NEXT:    mv a2, a4
; HYBRID-NEXT:    mv a3, a5
; HYBRID-NEXT:    bnez a0, .LBB10_1
; HYBRID-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    mv a2, s2
; HYBRID-NEXT:    mv a3, s1
; HYBRID-NEXT:    j .LBB10_1
; HYBRID-NEXT:  .LBB10_7: # %atomicrmw.end
; HYBRID-NEXT:    mv a0, a4
; HYBRID-NEXT:    mv a1, a5
; HYBRID-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s2, 16(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 48
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_min:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -64
; HYBRID-CAP-PTR-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    ld.cap a4, (ca0)
; HYBRID-CAP-PTR-NEXT:    sc ca0, 0(sp) # 16-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    cincoffset ca0, ca0, 8
; HYBRID-CAP-PTR-NEXT:    ld.cap a5, (ca0)
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    j .LBB10_2
; HYBRID-CAP-PTR-NEXT:  .LBB10_1: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sd a4, 16(sp)
; HYBRID-CAP-PTR-NEXT:    sd a5, 24(sp)
; HYBRID-CAP-PTR-NEXT:    addi a1, sp, 16
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    lc ca0, 0(sp) # 16-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld a5, 24(sp)
; HYBRID-CAP-PTR-NEXT:    ld a4, 16(sp)
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB10_7
; HYBRID-CAP-PTR-NEXT:  .LBB10_2: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-CAP-PTR-NEXT:    beq a5, s0, .LBB10_4
; HYBRID-CAP-PTR-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    slt a0, s0, a5
; HYBRID-CAP-PTR-NEXT:    j .LBB10_5
; HYBRID-CAP-PTR-NEXT:  .LBB10_4: # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s1, a4
; HYBRID-CAP-PTR-NEXT:  .LBB10_5: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    xori a0, a0, 1
; HYBRID-CAP-PTR-NEXT:    mv a2, a4
; HYBRID-CAP-PTR-NEXT:    mv a3, a5
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB10_1
; HYBRID-CAP-PTR-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, s1
; HYBRID-CAP-PTR-NEXT:    mv a3, s0
; HYBRID-CAP-PTR-NEXT:    j .LBB10_1
; HYBRID-CAP-PTR-NEXT:  .LBB10_7: # %atomicrmw.end
; HYBRID-CAP-PTR-NEXT:    mv a0, a4
; HYBRID-CAP-PTR-NEXT:    mv a1, a5
; HYBRID-CAP-PTR-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 64
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_min
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16, addrspace(200)
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = load i128, ptr addrspace(200) [[PTR]], align 16
; PURECAP-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; PURECAP-IR:       atomicrmw.start:
; PURECAP-IR-NEXT:    [[LOADED:%.*]] = phi i128 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = icmp sle i128 [[LOADED]], [[VAL]]
; PURECAP-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i128 [[LOADED]], i128 [[VAL]]
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i128 [[LOADED]], ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_16(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = load i128, ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP5]], 0
; PURECAP-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i128, i1 } [[TMP6]], i1 [[TMP4]], 1
; PURECAP-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i128, i1 } [[TMP7]], 1
; PURECAP-IR-NEXT:    [[NEWLOADED]] = extractvalue { i128, i1 } [[TMP7]], 0
; PURECAP-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; PURECAP-IR:       atomicrmw.end:
; PURECAP-IR-NEXT:    ret i128 [[NEWLOADED]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_min
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = load i128, ptr addrspace(200) [[PTR]], align 16
; HYBRID-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; HYBRID-IR:       atomicrmw.start:
; HYBRID-IR-NEXT:    [[LOADED:%.*]] = phi i128 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = icmp sle i128 [[LOADED]], [[VAL]]
; HYBRID-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i128 [[LOADED]], i128 [[VAL]]
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i128 [[LOADED]], ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_16_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = load i128, ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP5]], 0
; HYBRID-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i128, i1 } [[TMP6]], i1 [[TMP4]], 1
; HYBRID-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i128, i1 } [[TMP7]], 1
; HYBRID-IR-NEXT:    [[NEWLOADED]] = extractvalue { i128, i1 } [[TMP7]], 0
; HYBRID-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; HYBRID-IR:       atomicrmw.end:
; HYBRID-IR-NEXT:    ret i128 [[NEWLOADED]]
;
  %tmp = atomicrmw min ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_umax(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_umax:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -96
; PURECAP-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    cmove cs3, ca0
; PURECAP-NEXT:    cld a5, 8(ca0)
; PURECAP-NEXT:    cld a4, 0(ca0)
; PURECAP-NEXT:    mv s1, a2
; PURECAP-NEXT:    mv s2, a1
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds cs0, ca0, 16
; PURECAP-NEXT:    j .LBB11_2
; PURECAP-NEXT:  .LBB11_1: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    csd a4, 0(csp)
; PURECAP-NEXT:    csd a5, 8(csp)
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, cs3
; PURECAP-NEXT:    cmove ca1, cs0
; PURECAP-NEXT:    ccall __atomic_compare_exchange_16
; PURECAP-NEXT:    cld a5, 8(csp)
; PURECAP-NEXT:    cld a4, 0(csp)
; PURECAP-NEXT:    bnez a0, .LBB11_7
; PURECAP-NEXT:  .LBB11_2: # %atomicrmw.start
; PURECAP-NEXT:    # =>This Inner Loop Header: Depth=1
; PURECAP-NEXT:    beq a5, s1, .LBB11_4
; PURECAP-NEXT:  # %bb.3: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    sltu a0, s1, a5
; PURECAP-NEXT:    j .LBB11_5
; PURECAP-NEXT:  .LBB11_4: # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    sltu a0, s2, a4
; PURECAP-NEXT:  .LBB11_5: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    mv a2, a4
; PURECAP-NEXT:    mv a3, a5
; PURECAP-NEXT:    bnez a0, .LBB11_1
; PURECAP-NEXT:  # %bb.6: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    mv a2, s2
; PURECAP-NEXT:    mv a3, s1
; PURECAP-NEXT:    j .LBB11_1
; PURECAP-NEXT:  .LBB11_7: # %atomicrmw.end
; PURECAP-NEXT:    mv a0, a4
; PURECAP-NEXT:    mv a1, a5
; PURECAP-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 96
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_umax:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -48
; HYBRID-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s2, 16(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    ld a5, 8(a0)
; HYBRID-NEXT:    ld a4, 0(a0)
; HYBRID-NEXT:    mv s1, a2
; HYBRID-NEXT:    mv s2, a1
; HYBRID-NEXT:    j .LBB11_2
; HYBRID-NEXT:  .LBB11_1: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    sd a4, 0(sp)
; HYBRID-NEXT:    sd a5, 8(sp)
; HYBRID-NEXT:    mv a1, sp
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, s0
; HYBRID-NEXT:    call __atomic_compare_exchange_16@plt
; HYBRID-NEXT:    ld a5, 8(sp)
; HYBRID-NEXT:    ld a4, 0(sp)
; HYBRID-NEXT:    bnez a0, .LBB11_7
; HYBRID-NEXT:  .LBB11_2: # %atomicrmw.start
; HYBRID-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-NEXT:    beq a5, s1, .LBB11_4
; HYBRID-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    sltu a0, s1, a5
; HYBRID-NEXT:    j .LBB11_5
; HYBRID-NEXT:  .LBB11_4: # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    sltu a0, s2, a4
; HYBRID-NEXT:  .LBB11_5: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    mv a2, a4
; HYBRID-NEXT:    mv a3, a5
; HYBRID-NEXT:    bnez a0, .LBB11_1
; HYBRID-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    mv a2, s2
; HYBRID-NEXT:    mv a3, s1
; HYBRID-NEXT:    j .LBB11_1
; HYBRID-NEXT:  .LBB11_7: # %atomicrmw.end
; HYBRID-NEXT:    mv a0, a4
; HYBRID-NEXT:    mv a1, a5
; HYBRID-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s2, 16(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 48
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_umax:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -64
; HYBRID-CAP-PTR-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    ld.cap a4, (ca0)
; HYBRID-CAP-PTR-NEXT:    sc ca0, 0(sp) # 16-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    cincoffset ca0, ca0, 8
; HYBRID-CAP-PTR-NEXT:    ld.cap a5, (ca0)
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    j .LBB11_2
; HYBRID-CAP-PTR-NEXT:  .LBB11_1: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sd a4, 16(sp)
; HYBRID-CAP-PTR-NEXT:    sd a5, 24(sp)
; HYBRID-CAP-PTR-NEXT:    addi a1, sp, 16
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    lc ca0, 0(sp) # 16-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld a5, 24(sp)
; HYBRID-CAP-PTR-NEXT:    ld a4, 16(sp)
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB11_7
; HYBRID-CAP-PTR-NEXT:  .LBB11_2: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-CAP-PTR-NEXT:    beq a5, s0, .LBB11_4
; HYBRID-CAP-PTR-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s0, a5
; HYBRID-CAP-PTR-NEXT:    j .LBB11_5
; HYBRID-CAP-PTR-NEXT:  .LBB11_4: # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s1, a4
; HYBRID-CAP-PTR-NEXT:  .LBB11_5: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, a4
; HYBRID-CAP-PTR-NEXT:    mv a3, a5
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB11_1
; HYBRID-CAP-PTR-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, s1
; HYBRID-CAP-PTR-NEXT:    mv a3, s0
; HYBRID-CAP-PTR-NEXT:    j .LBB11_1
; HYBRID-CAP-PTR-NEXT:  .LBB11_7: # %atomicrmw.end
; HYBRID-CAP-PTR-NEXT:    mv a0, a4
; HYBRID-CAP-PTR-NEXT:    mv a1, a5
; HYBRID-CAP-PTR-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 64
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_umax
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16, addrspace(200)
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = load i128, ptr addrspace(200) [[PTR]], align 16
; PURECAP-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; PURECAP-IR:       atomicrmw.start:
; PURECAP-IR-NEXT:    [[LOADED:%.*]] = phi i128 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = icmp ugt i128 [[LOADED]], [[VAL]]
; PURECAP-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i128 [[LOADED]], i128 [[VAL]]
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i128 [[LOADED]], ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_16(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = load i128, ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP5]], 0
; PURECAP-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i128, i1 } [[TMP6]], i1 [[TMP4]], 1
; PURECAP-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i128, i1 } [[TMP7]], 1
; PURECAP-IR-NEXT:    [[NEWLOADED]] = extractvalue { i128, i1 } [[TMP7]], 0
; PURECAP-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; PURECAP-IR:       atomicrmw.end:
; PURECAP-IR-NEXT:    ret i128 [[NEWLOADED]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_umax
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = load i128, ptr addrspace(200) [[PTR]], align 16
; HYBRID-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; HYBRID-IR:       atomicrmw.start:
; HYBRID-IR-NEXT:    [[LOADED:%.*]] = phi i128 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = icmp ugt i128 [[LOADED]], [[VAL]]
; HYBRID-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i128 [[LOADED]], i128 [[VAL]]
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i128 [[LOADED]], ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_16_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = load i128, ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP5]], 0
; HYBRID-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i128, i1 } [[TMP6]], i1 [[TMP4]], 1
; HYBRID-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i128, i1 } [[TMP7]], 1
; HYBRID-IR-NEXT:    [[NEWLOADED]] = extractvalue { i128, i1 } [[TMP7]], 0
; HYBRID-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; HYBRID-IR:       atomicrmw.end:
; HYBRID-IR-NEXT:    ret i128 [[NEWLOADED]]
;
  %tmp = atomicrmw umax ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define i128 @atomic_umin(ptr addrspace(200) %ptr, i128 %val) nounwind {
; PURECAP-LABEL: atomic_umin:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -96
; PURECAP-NEXT:    csc cra, 80(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 64(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 48(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs2, 32(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs3, 16(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    cmove cs3, ca0
; PURECAP-NEXT:    cld a5, 8(ca0)
; PURECAP-NEXT:    cld a4, 0(ca0)
; PURECAP-NEXT:    mv s1, a2
; PURECAP-NEXT:    mv s2, a1
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds cs0, ca0, 16
; PURECAP-NEXT:    j .LBB12_2
; PURECAP-NEXT:  .LBB12_1: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    csd a4, 0(csp)
; PURECAP-NEXT:    csd a5, 8(csp)
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, cs3
; PURECAP-NEXT:    cmove ca1, cs0
; PURECAP-NEXT:    ccall __atomic_compare_exchange_16
; PURECAP-NEXT:    cld a5, 8(csp)
; PURECAP-NEXT:    cld a4, 0(csp)
; PURECAP-NEXT:    bnez a0, .LBB12_7
; PURECAP-NEXT:  .LBB12_2: # %atomicrmw.start
; PURECAP-NEXT:    # =>This Inner Loop Header: Depth=1
; PURECAP-NEXT:    beq a5, s1, .LBB12_4
; PURECAP-NEXT:  # %bb.3: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    sltu a0, s1, a5
; PURECAP-NEXT:    j .LBB12_5
; PURECAP-NEXT:  .LBB12_4: # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    sltu a0, s2, a4
; PURECAP-NEXT:  .LBB12_5: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    xori a0, a0, 1
; PURECAP-NEXT:    mv a2, a4
; PURECAP-NEXT:    mv a3, a5
; PURECAP-NEXT:    bnez a0, .LBB12_1
; PURECAP-NEXT:  # %bb.6: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    mv a2, s2
; PURECAP-NEXT:    mv a3, s1
; PURECAP-NEXT:    j .LBB12_1
; PURECAP-NEXT:  .LBB12_7: # %atomicrmw.end
; PURECAP-NEXT:    mv a0, a4
; PURECAP-NEXT:    mv a1, a5
; PURECAP-NEXT:    clc cra, 80(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 64(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 48(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs2, 32(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs3, 16(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 96
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_umin:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -48
; HYBRID-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s2, 16(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    ld a5, 8(a0)
; HYBRID-NEXT:    ld a4, 0(a0)
; HYBRID-NEXT:    mv s1, a2
; HYBRID-NEXT:    mv s2, a1
; HYBRID-NEXT:    j .LBB12_2
; HYBRID-NEXT:  .LBB12_1: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    sd a4, 0(sp)
; HYBRID-NEXT:    sd a5, 8(sp)
; HYBRID-NEXT:    mv a1, sp
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, s0
; HYBRID-NEXT:    call __atomic_compare_exchange_16@plt
; HYBRID-NEXT:    ld a5, 8(sp)
; HYBRID-NEXT:    ld a4, 0(sp)
; HYBRID-NEXT:    bnez a0, .LBB12_7
; HYBRID-NEXT:  .LBB12_2: # %atomicrmw.start
; HYBRID-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-NEXT:    beq a5, s1, .LBB12_4
; HYBRID-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    sltu a0, s1, a5
; HYBRID-NEXT:    j .LBB12_5
; HYBRID-NEXT:  .LBB12_4: # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    sltu a0, s2, a4
; HYBRID-NEXT:  .LBB12_5: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    xori a0, a0, 1
; HYBRID-NEXT:    mv a2, a4
; HYBRID-NEXT:    mv a3, a5
; HYBRID-NEXT:    bnez a0, .LBB12_1
; HYBRID-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    mv a2, s2
; HYBRID-NEXT:    mv a3, s1
; HYBRID-NEXT:    j .LBB12_1
; HYBRID-NEXT:  .LBB12_7: # %atomicrmw.end
; HYBRID-NEXT:    mv a0, a4
; HYBRID-NEXT:    mv a1, a5
; HYBRID-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s2, 16(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 48
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_umin:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -64
; HYBRID-CAP-PTR-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    ld.cap a4, (ca0)
; HYBRID-CAP-PTR-NEXT:    sc ca0, 0(sp) # 16-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    cincoffset ca0, ca0, 8
; HYBRID-CAP-PTR-NEXT:    ld.cap a5, (ca0)
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    j .LBB12_2
; HYBRID-CAP-PTR-NEXT:  .LBB12_1: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sd a4, 16(sp)
; HYBRID-CAP-PTR-NEXT:    sd a5, 24(sp)
; HYBRID-CAP-PTR-NEXT:    addi a1, sp, 16
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    lc ca0, 0(sp) # 16-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld a5, 24(sp)
; HYBRID-CAP-PTR-NEXT:    ld a4, 16(sp)
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB12_7
; HYBRID-CAP-PTR-NEXT:  .LBB12_2: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-CAP-PTR-NEXT:    beq a5, s0, .LBB12_4
; HYBRID-CAP-PTR-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s0, a5
; HYBRID-CAP-PTR-NEXT:    j .LBB12_5
; HYBRID-CAP-PTR-NEXT:  .LBB12_4: # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s1, a4
; HYBRID-CAP-PTR-NEXT:  .LBB12_5: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    xori a0, a0, 1
; HYBRID-CAP-PTR-NEXT:    mv a2, a4
; HYBRID-CAP-PTR-NEXT:    mv a3, a5
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB12_1
; HYBRID-CAP-PTR-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, s1
; HYBRID-CAP-PTR-NEXT:    mv a3, s0
; HYBRID-CAP-PTR-NEXT:    j .LBB12_1
; HYBRID-CAP-PTR-NEXT:  .LBB12_7: # %atomicrmw.end
; HYBRID-CAP-PTR-NEXT:    mv a0, a4
; HYBRID-CAP-PTR-NEXT:    mv a1, a5
; HYBRID-CAP-PTR-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 64
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_umin
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16, addrspace(200)
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = load i128, ptr addrspace(200) [[PTR]], align 16
; PURECAP-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; PURECAP-IR:       atomicrmw.start:
; PURECAP-IR-NEXT:    [[LOADED:%.*]] = phi i128 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = icmp ule i128 [[LOADED]], [[VAL]]
; PURECAP-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i128 [[LOADED]], i128 [[VAL]]
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i128 [[LOADED]], ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_16(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = load i128, ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP5]], 0
; PURECAP-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i128, i1 } [[TMP6]], i1 [[TMP4]], 1
; PURECAP-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i128, i1 } [[TMP7]], 1
; PURECAP-IR-NEXT:    [[NEWLOADED]] = extractvalue { i128, i1 } [[TMP7]], 0
; PURECAP-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; PURECAP-IR:       atomicrmw.end:
; PURECAP-IR-NEXT:    ret i128 [[NEWLOADED]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_umin
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = load i128, ptr addrspace(200) [[PTR]], align 16
; HYBRID-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; HYBRID-IR:       atomicrmw.start:
; HYBRID-IR-NEXT:    [[LOADED:%.*]] = phi i128 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = icmp ule i128 [[LOADED]], [[VAL]]
; HYBRID-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i128 [[LOADED]], i128 [[VAL]]
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i128 [[LOADED]], ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_16_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = load i128, ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP5]], 0
; HYBRID-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i128, i1 } [[TMP6]], i1 [[TMP4]], 1
; HYBRID-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i128, i1 } [[TMP7]], 1
; HYBRID-IR-NEXT:    [[NEWLOADED]] = extractvalue { i128, i1 } [[TMP7]], 0
; HYBRID-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; HYBRID-IR:       atomicrmw.end:
; HYBRID-IR-NEXT:    ret i128 [[NEWLOADED]]
;
  %tmp = atomicrmw umin ptr addrspace(200) %ptr, i128 %val seq_cst
  ret i128 %tmp
}

define { i128, i1 } @cmpxchg_weak(ptr addrspace(200) %ptr, i128 %exp, i128 %new) nounwind {
; PURECAP-LABEL: cmpxchg_weak:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -48
; PURECAP-NEXT:    csc cra, 32(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 16(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    mv a6, a5
; PURECAP-NEXT:    mv a7, a4
; PURECAP-NEXT:    cmove ct0, ca1
; PURECAP-NEXT:    cmove cs0, ca0
; PURECAP-NEXT:    csd a3, 8(csp)
; PURECAP-NEXT:    csd a2, 0(csp)
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds ca1, ca0, 16
; PURECAP-NEXT:    li a4, 4
; PURECAP-NEXT:    li a5, 2
; PURECAP-NEXT:    cmove ca0, ct0
; PURECAP-NEXT:    mv a2, a7
; PURECAP-NEXT:    mv a3, a6
; PURECAP-NEXT:    ccall __atomic_compare_exchange_16
; PURECAP-NEXT:    cld a1, 8(csp)
; PURECAP-NEXT:    cld a2, 0(csp)
; PURECAP-NEXT:    csd a1, 8(cs0)
; PURECAP-NEXT:    csd a2, 0(cs0)
; PURECAP-NEXT:    csb a0, 16(cs0)
; PURECAP-NEXT:    clc cra, 32(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 16(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 48
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: cmpxchg_weak:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -32
; HYBRID-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s0, 16(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    mv a6, a5
; HYBRID-NEXT:    mv a7, a4
; HYBRID-NEXT:    mv t0, a1
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    sd a3, 8(sp)
; HYBRID-NEXT:    sd a2, 0(sp)
; HYBRID-NEXT:    mv a1, sp
; HYBRID-NEXT:    li a4, 4
; HYBRID-NEXT:    li a5, 2
; HYBRID-NEXT:    mv a0, t0
; HYBRID-NEXT:    mv a2, a7
; HYBRID-NEXT:    mv a3, a6
; HYBRID-NEXT:    call __atomic_compare_exchange_16@plt
; HYBRID-NEXT:    ld a1, 8(sp)
; HYBRID-NEXT:    ld a2, 0(sp)
; HYBRID-NEXT:    sd a1, 8(s0)
; HYBRID-NEXT:    sd a2, 0(s0)
; HYBRID-NEXT:    sb a0, 16(s0)
; HYBRID-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s0, 16(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 32
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: cmpxchg_weak:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -32
; HYBRID-CAP-PTR-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s0, 16(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    mv a6, a5
; HYBRID-CAP-PTR-NEXT:    mv a7, a4
; HYBRID-CAP-PTR-NEXT:    cmove ct0, ca1
; HYBRID-CAP-PTR-NEXT:    mv s0, a0
; HYBRID-CAP-PTR-NEXT:    sd a3, 8(sp)
; HYBRID-CAP-PTR-NEXT:    sd a2, 0(sp)
; HYBRID-CAP-PTR-NEXT:    mv a1, sp
; HYBRID-CAP-PTR-NEXT:    li a4, 4
; HYBRID-CAP-PTR-NEXT:    li a5, 2
; HYBRID-CAP-PTR-NEXT:    cmove ca0, ct0
; HYBRID-CAP-PTR-NEXT:    mv a2, a7
; HYBRID-CAP-PTR-NEXT:    mv a3, a6
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld a1, 8(sp)
; HYBRID-CAP-PTR-NEXT:    ld a2, 0(sp)
; HYBRID-CAP-PTR-NEXT:    sd a1, 8(s0)
; HYBRID-CAP-PTR-NEXT:    sd a2, 0(s0)
; HYBRID-CAP-PTR-NEXT:    sb a0, 16(s0)
; HYBRID-CAP-PTR-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s0, 16(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 32
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@cmpxchg_weak
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[EXP:%.*]], i128 [[NEW:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16, addrspace(200)
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i128 [[EXP]], ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = call zeroext i1 @__atomic_compare_exchange_16(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i128 [[NEW]], i32 4, i32 2)
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = load i128, ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP3]], 0
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = insertvalue { i128, i1 } [[TMP4]], i1 [[TMP2]], 1
; PURECAP-IR-NEXT:    ret { i128, i1 } [[TMP5]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@cmpxchg_weak
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[EXP:%.*]], i128 [[NEW:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i128 [[EXP]], ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = call zeroext i1 @__atomic_compare_exchange_16_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i128 [[NEW]], i32 4, i32 2)
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = load i128, ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP3]], 0
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = insertvalue { i128, i1 } [[TMP4]], i1 [[TMP2]], 1
; HYBRID-IR-NEXT:    ret { i128, i1 } [[TMP5]]
;
  %1 = cmpxchg weak ptr addrspace(200) %ptr, i128 %exp, i128 %new acq_rel acquire
  ret { i128, i1 } %1
}

define { i128, i1 } @cmpxchg_strong(ptr addrspace(200) %ptr, i128 %exp, i128 %new) nounwind {
; PURECAP-LABEL: cmpxchg_strong:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -48
; PURECAP-NEXT:    csc cra, 32(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 16(csp) # 16-byte Folded Spill
; PURECAP-NEXT:    mv a6, a5
; PURECAP-NEXT:    mv a7, a4
; PURECAP-NEXT:    cmove ct0, ca1
; PURECAP-NEXT:    cmove cs0, ca0
; PURECAP-NEXT:    csd a3, 8(csp)
; PURECAP-NEXT:    csd a2, 0(csp)
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds ca1, ca0, 16
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, ct0
; PURECAP-NEXT:    mv a2, a7
; PURECAP-NEXT:    mv a3, a6
; PURECAP-NEXT:    ccall __atomic_compare_exchange_16
; PURECAP-NEXT:    cld a1, 8(csp)
; PURECAP-NEXT:    cld a2, 0(csp)
; PURECAP-NEXT:    csd a1, 8(cs0)
; PURECAP-NEXT:    csd a2, 0(cs0)
; PURECAP-NEXT:    csb a0, 16(cs0)
; PURECAP-NEXT:    clc cra, 32(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 16(csp) # 16-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 48
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: cmpxchg_strong:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -32
; HYBRID-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    sd s0, 16(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    mv a6, a5
; HYBRID-NEXT:    mv a7, a4
; HYBRID-NEXT:    mv t0, a1
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    sd a3, 8(sp)
; HYBRID-NEXT:    sd a2, 0(sp)
; HYBRID-NEXT:    mv a1, sp
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, t0
; HYBRID-NEXT:    mv a2, a7
; HYBRID-NEXT:    mv a3, a6
; HYBRID-NEXT:    call __atomic_compare_exchange_16@plt
; HYBRID-NEXT:    ld a1, 8(sp)
; HYBRID-NEXT:    ld a2, 0(sp)
; HYBRID-NEXT:    sd a1, 8(s0)
; HYBRID-NEXT:    sd a2, 0(s0)
; HYBRID-NEXT:    sb a0, 16(s0)
; HYBRID-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    ld s0, 16(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 32
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: cmpxchg_strong:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -32
; HYBRID-CAP-PTR-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sd s0, 16(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    mv a6, a5
; HYBRID-CAP-PTR-NEXT:    mv a7, a4
; HYBRID-CAP-PTR-NEXT:    cmove ct0, ca1
; HYBRID-CAP-PTR-NEXT:    mv s0, a0
; HYBRID-CAP-PTR-NEXT:    sd a3, 8(sp)
; HYBRID-CAP-PTR-NEXT:    sd a2, 0(sp)
; HYBRID-CAP-PTR-NEXT:    mv a1, sp
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    cmove ca0, ct0
; HYBRID-CAP-PTR-NEXT:    mv a2, a7
; HYBRID-CAP-PTR-NEXT:    mv a3, a6
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_16_c@plt
; HYBRID-CAP-PTR-NEXT:    ld a1, 8(sp)
; HYBRID-CAP-PTR-NEXT:    ld a2, 0(sp)
; HYBRID-CAP-PTR-NEXT:    sd a1, 8(s0)
; HYBRID-CAP-PTR-NEXT:    sd a2, 0(s0)
; HYBRID-CAP-PTR-NEXT:    sb a0, 16(s0)
; HYBRID-CAP-PTR-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    ld s0, 16(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 32
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@cmpxchg_strong
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[EXP:%.*]], i128 [[NEW:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16, addrspace(200)
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i128 [[EXP]], ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = call zeroext i1 @__atomic_compare_exchange_16(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = load i128, ptr addrspace(200) [[TMP1]], align 16
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 16, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP3]], 0
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = insertvalue { i128, i1 } [[TMP4]], i1 [[TMP2]], 1
; PURECAP-IR-NEXT:    ret { i128, i1 } [[TMP5]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@cmpxchg_strong
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i128 [[EXP:%.*]], i128 [[NEW:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i128, align 16
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i128 [[EXP]], ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = call zeroext i1 @__atomic_compare_exchange_16_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i128 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = load i128, ptr [[TMP1]], align 16
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = insertvalue { i128, i1 } undef, i128 [[TMP3]], 0
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = insertvalue { i128, i1 } [[TMP4]], i1 [[TMP2]], 1
; HYBRID-IR-NEXT:    ret { i128, i1 } [[TMP5]]
;
  %1 = cmpxchg ptr addrspace(200) %ptr, i128 %exp, i128 %new seq_cst seq_cst
  ret { i128, i1 } %1
}
