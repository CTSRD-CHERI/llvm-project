; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/stack-spill-unnecessary.c.ll
; The new CheriBoundedStackPseudo instruction lets us pretend that the incoffset+csetbounds
; is a single trivially rematerizable instruction so it can freely move it around to avoid stack spills.
; Previously we were moving the allocation of the register that is only used later to the beginning of
; the function and saving+restoring it instead of materializing it just before

; RUN: llc -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -O2 --cheri-stack-bounds-single-intrinsic-threshold=0 < %s | %cheri_FileCheck %s --check-prefixes=CHECK
; Always use a single intrinsic for the calls (should result in same codegen)
; RUN: llc -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -O2 --cheri-stack-bounds-single-intrinsic-threshold=0 < %s | %cheri_FileCheck %s --check-prefixes=CHECK
; RUN: sed 's/addrspace(200)/addrspace(0)/g' %s | llc -mtriple=riscv64 --relocation-model=pic -target-abi lp64d -mattr=+xcheri,+f,+d | FileCheck --check-prefix HYBRID %s


declare void @foo() addrspace(200)
declare void @one_arg(i32 addrspace(200)*) addrspace(200)
declare void @multi_arg(i32 addrspace(200)* %start, i32 addrspace(200)* %end, i8 addrspace(200)* %buf) addrspace(200)

define void @use_after_call() addrspace(200) nounwind {
; CHECK-LABEL: use_after_call:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    cincoffset csp, csp, -48
; CHECK-NEXT:    csc cra, 32(csp) # 16-byte Folded Spill
; CHECK-NEXT:    csc cs0, 16(csp) # 16-byte Folded Spill
; CHECK-NEXT:    cincoffset ca0, csp, 12
; CHECK-NEXT:    csetbounds cs0, ca0, 4
; CHECK-NEXT:    addi a0, zero, 123
; CHECK-NEXT:    csw a0, 12(csp)
; CHECK-NEXT:    ccall foo
; CHECK-NEXT:    cmove ca0, cs0
; CHECK-NEXT:    ccall one_arg
; CHECK-NEXT:    clc cs0, 16(csp) # 16-byte Folded Reload
; CHECK-NEXT:    clc cra, 32(csp) # 16-byte Folded Reload
; CHECK-NEXT:    cincoffset csp, csp, 48
; CHECK-NEXT:    cret
;
; HYBRID-LABEL: use_after_call:
; HYBRID:       # %bb.0: # %entry
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    addi a0, zero, 123
; HYBRID-NEXT:    sw a0, 4(sp)
; HYBRID-NEXT:    call foo@plt
; HYBRID-NEXT:    addi a0, sp, 4
; HYBRID-NEXT:    call one_arg@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
entry:
  %x = alloca i32, align 4, addrspace(200)
  store i32 123, i32 addrspace(200)* %x, align 4
  call void @foo()
  call void @one_arg(i32 addrspace(200)* %x)
  ret void
}


define void @use_after_call_no_store() addrspace(200) nounwind {
; CHECK-LABEL: use_after_call_no_store:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    cincoffset csp, csp, -64
; CHECK-NEXT:    csc cra, 48(csp) # 16-byte Folded Spill
; CHECK-NEXT:    csc cs0, 32(csp) # 16-byte Folded Spill
; CHECK-NEXT:    csc cs1, 16(csp) # 16-byte Folded Spill
; CHECK-NEXT:    cincoffset ca0, csp, 12
; CHECK-NEXT:    csetbounds cs0, ca0, 4
; CHECK-NEXT:    cincoffset ca0, csp, 8
; CHECK-NEXT:    csetbounds cs1, ca0, 4
; CHECK-NEXT:    ccall foo
; CHECK-NEXT:    cmove ca0, cs0
; CHECK-NEXT:    ccall one_arg
; CHECK-NEXT:    cmove ca0, cs1
; CHECK-NEXT:    ccall one_arg
; CHECK-NEXT:    clc cs1, 16(csp) # 16-byte Folded Reload
; CHECK-NEXT:    clc cs0, 32(csp) # 16-byte Folded Reload
; CHECK-NEXT:    clc cra, 48(csp) # 16-byte Folded Reload
; CHECK-NEXT:    cincoffset csp, csp, 64
; CHECK-NEXT:    cret
;
; HYBRID-LABEL: use_after_call_no_store:
; HYBRID:       # %bb.0: # %entry
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    call foo@plt
; HYBRID-NEXT:    addi a0, sp, 4
; HYBRID-NEXT:    call one_arg@plt
; HYBRID-NEXT:    mv a0, sp
; HYBRID-NEXT:    call one_arg@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
entry:
  %x = alloca i32, align 4, addrspace(200)
  %y = alloca i32, align 4, addrspace(200)
  call void @foo()
  call void @one_arg(i32 addrspace(200)* %x)
  call void @one_arg(i32 addrspace(200)* %y)
  ret void
}

define void @multi_use() addrspace(200) nounwind {
; CHECK-LABEL: multi_use:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    cincoffset csp, csp, -64
; CHECK-NEXT:    csc cra, 48(csp) # 16-byte Folded Spill
; CHECK-NEXT:    csc cs0, 32(csp) # 16-byte Folded Spill
; CHECK-NEXT:    csc cs1, 16(csp) # 16-byte Folded Spill
; CHECK-NEXT:    cincoffset ca0, csp, 12
; CHECK-NEXT:    csetbounds cs0, ca0, 4
; CHECK-NEXT:    cincoffset ca0, csp, 8
; CHECK-NEXT:    csetbounds cs1, ca0, 4
; CHECK-NEXT:    ccall foo
; CHECK-NEXT:    cincoffset ca1, cs1, 4
; CHECK-NEXT:    cincoffset ca2, cs1, 1
; CHECK-NEXT:    cmove ca0, cs1
; CHECK-NEXT:    ccall multi_arg
; CHECK-NEXT:    cmove ca0, cs0
; CHECK-NEXT:    ccall one_arg
; CHECK-NEXT:    cmove ca0, cs1
; CHECK-NEXT:    ccall one_arg
; CHECK-NEXT:    clc cs1, 16(csp) # 16-byte Folded Reload
; CHECK-NEXT:    clc cs0, 32(csp) # 16-byte Folded Reload
; CHECK-NEXT:    clc cra, 48(csp) # 16-byte Folded Reload
; CHECK-NEXT:    cincoffset csp, csp, 64
; CHECK-NEXT:    cret
;
; HYBRID-LABEL: multi_use:
; HYBRID:       # %bb.0: # %entry
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; HYBRID-NEXT:    call foo@plt
; HYBRID-NEXT:    addi a1, sp, 4
; HYBRID-NEXT:    addi a2, sp, 1
; HYBRID-NEXT:    mv a0, sp
; HYBRID-NEXT:    call multi_arg@plt
; HYBRID-NEXT:    addi a0, sp, 4
; HYBRID-NEXT:    call one_arg@plt
; HYBRID-NEXT:    mv a0, sp
; HYBRID-NEXT:    call one_arg@plt
; HYBRID-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
entry:
  %y = alloca i32, align 4, addrspace(200)
  %x = alloca i32, align 4, addrspace(200)
  call void @foo()
  %x_plus0 = getelementptr inbounds i32, i32 addrspace(200)* %x, i32 0
  %x_plus1 = getelementptr i32, i32 addrspace(200)* %x, i32 1
  %x_i8 = bitcast i32 addrspace(200)* %x to i8 addrspace(200)*
  %x_i8_plus_1 = getelementptr inbounds i8, i8 addrspace(200)* %x_i8, i32 1
  call void @multi_arg(i32 addrspace(200)* %x_plus0, i32 addrspace(200)* %x_plus1, i8 addrspace(200)* %x_i8_plus_1)
  call void @one_arg(i32 addrspace(200)* %y)
  call void @one_arg(i32 addrspace(200)* %x)
  ret void
}
