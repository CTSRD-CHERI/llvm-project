; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/cheri-intrinsics-folding-broken-module-regression.ll
; This used to create a broken function.
; FIXME: the getoffset+add sequence should be folded to an increment
; REQUIRES: mips-registered-target
; RUN: opt -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -S -passes=instcombine %s -o - | FileCheck %s
; RUN: opt -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -S '-passes=default<O3>' %s | llc -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -O3 -o - | FileCheck %s --check-prefix ASM
target datalayout = "e-m:e-pf200:128:128:128:64-p:64:64-i64:64-i128:128-n64-S128-A200-P200-G200"

@d = common addrspace(200) global i64 0, align 4
@e = common addrspace(200) global i8 addrspace(200)* null, align 32

; C Source code:
;int d;
;void* e;
;void g(int x, int y) {
;  e = (__uintcap_t)&d + x + y;
;}

define void @g(i64 %x, i64 %y) addrspace(200) nounwind {
; ASM-LABEL: g:
; ASM:       # %bb.0:
; ASM-NEXT:  .LBB0_1: # Label of block must be emitted
; ASM-NEXT:    auipcc ca2, %captab_pcrel_hi(d)
; ASM-NEXT:    clc ca2, %pcrel_lo(.LBB0_1)(ca2)
; ASM-NEXT:    add a0, a1, a0
; ASM-NEXT:  .LBB0_2: # Label of block must be emitted
; ASM-NEXT:    auipcc ca1, %captab_pcrel_hi(e)
; ASM-NEXT:    clc ca1, %pcrel_lo(.LBB0_2)(ca1)
; ASM-NEXT:    cgetoffset a3, ca2
; ASM-NEXT:    add a0, a0, a3
; ASM-NEXT:    csetoffset ca0, ca2, a0
; ASM-NEXT:    csc ca0, 0(ca1)
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@g
; CHECK-SAME: (i64 [[X:%.*]], i64 [[Y:%.*]]) addrspace(200) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* nonnull bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*))
; CHECK-NEXT:    [[ADD:%.*]] = add i64 [[TMP3]], [[X]]
; CHECK-NEXT:    [[ADD1:%.*]] = add i64 [[ADD]], [[Y]]
; CHECK-NEXT:    [[TMP11:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* nonnull bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*), i64 [[ADD1]])
; CHECK-NEXT:    store i8 addrspace(200)* [[TMP11]], i8 addrspace(200)* addrspace(200)* @e, align 32
; CHECK-NEXT:    ret void
;
  %x.addr = alloca i64, align 4, addrspace(200)
  %y.addr = alloca i64, align 4, addrspace(200)
  store i64 %x, i64 addrspace(200)* %x.addr, align 4
  store i64 %y, i64 addrspace(200)* %y.addr, align 4
  %tmp1 = load i64, i64 addrspace(200)* %x.addr, align 4
  %tmp2 = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* null, i64 %tmp1)
  %tmp3 = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*))
  %tmp4 = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* %tmp2)
  %add = add i64 %tmp3, %tmp4
  %tmp5 = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*), i64 %add)
  %tmp7 = load i64, i64 addrspace(200)* %y.addr, align 4
  %tmp8 = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* null, i64 %tmp7)
  %tmp9 = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* %tmp5)
  %tmp10 = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* %tmp8)
  %add1 = add i64 %tmp9, %tmp10
  %tmp11 = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* %tmp5, i64 %add1)
  store i8 addrspace(200)* %tmp11, i8 addrspace(200)* addrspace(200)* @e, align 32
  ret void
}

; define void @g(i64 %x, i64 %y)  nounwind {
;   %tmp1 = tail call i8 addrspace(200)* @llvm.cheri.cap.offset.increment.i64(i8 addrspace(200)* bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*), i64 %x)
;   %tmp3 = tail call i8 addrspace(200)* @llvm.cheri.cap.offset.increment.i64(i8 addrspace(200)* %tmp1, i64 %y)
;   store i8 addrspace(200)* %tmp3, i8 addrspace(200)* addrspace(200)* @e, align 32
;   ret void
; }
;
declare i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)*, i64) addrspace(200)
declare i8 addrspace(200)* @llvm.cheri.cap.offset.increment.i64(i8 addrspace(200)*, i64) addrspace(200)
declare i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)*) addrspace(200)
