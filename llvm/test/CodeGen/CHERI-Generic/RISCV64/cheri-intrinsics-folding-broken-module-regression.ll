; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --scrub-attributes --version 2
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/cheri-intrinsics-folding-broken-module-regression.ll
; This used to create a broken function.
; RUN: opt -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -S -passes=instcombine %s -o - | FileCheck %s
; RUN: opt -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -S '-passes=default<O3>' %s | llc -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -O3 -o - | FileCheck %s --check-prefix ASM
target datalayout = "e-m:e-pf200:128:128:128:64-p:64:64-i64:64-i128:128-n64-S128-A200-P200-G200"

@d = common addrspace(200) global i64 0, align 4
@e = common addrspace(200) global ptr addrspace(200) null, align 32

; C Source code:
;int d;
;void* e;
;void g(int x, int y) {
;  e = (__uintcap_t)&d + x + y;
;}

define void @g(i64 %x, i64 %y) addrspace(200) nounwind {
; ASM-LABEL: g:
; ASM:       # %bb.0:
; ASM-NEXT:  .LBB0_1: # Label of block must be emitted
; ASM-NEXT:    auipcc ca2, %captab_pcrel_hi(d)
; ASM-NEXT:    lc ca2, %pcrel_lo(.LBB0_1)(ca2)
; ASM-NEXT:  .LBB0_2: # Label of block must be emitted
; ASM-NEXT:    auipcc ca3, %captab_pcrel_hi(e)
; ASM-NEXT:    lc ca3, %pcrel_lo(.LBB0_2)(ca3)
; ASM-NEXT:    cincoffset ca0, ca2, a0
; ASM-NEXT:    cincoffset ca0, ca0, a1
; ASM-NEXT:    sc ca0, 0(ca3)
; ASM-NEXT:    ret
; CHECK-LABEL: define void @g
; CHECK-SAME: (i64 [[X:%.*]], i64 [[Y:%.*]]) addrspace(200) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr addrspace(200) @d, i64 [[X]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr i8, ptr addrspace(200) [[TMP5]], i64 [[Y]]
; CHECK-NEXT:    store ptr addrspace(200) [[TMP11]], ptr addrspace(200) @e, align 32
; CHECK-NEXT:    ret void
;
  %x.addr = alloca i64, align 4, addrspace(200)
  %y.addr = alloca i64, align 4, addrspace(200)
  store i64 %x, ptr addrspace(200) %x.addr, align 4
  store i64 %y, ptr addrspace(200) %y.addr, align 4
  %tmp1 = load i64, ptr addrspace(200) %x.addr, align 4
  %tmp2 = call ptr addrspace(200) @llvm.cheri.cap.offset.set.i64(ptr addrspace(200) null, i64 %tmp1)
  %tmp3 = call i64 @llvm.cheri.cap.offset.get.i64(ptr addrspace(200) @d)
  %tmp4 = call i64 @llvm.cheri.cap.offset.get.i64(ptr addrspace(200) %tmp2)
  %add = add i64 %tmp3, %tmp4
  %tmp5 = call ptr addrspace(200) @llvm.cheri.cap.offset.set.i64(ptr addrspace(200) @d, i64 %add)
  %tmp7 = load i64, ptr addrspace(200) %y.addr, align 4
  %tmp8 = call ptr addrspace(200) @llvm.cheri.cap.offset.set.i64(ptr addrspace(200) null, i64 %tmp7)
  %tmp9 = call i64 @llvm.cheri.cap.offset.get.i64(ptr addrspace(200) %tmp5)
  %tmp10 = call i64 @llvm.cheri.cap.offset.get.i64(ptr addrspace(200) %tmp8)
  %add1 = add i64 %tmp9, %tmp10
  %tmp11 = call ptr addrspace(200) @llvm.cheri.cap.offset.set.i64(ptr addrspace(200) %tmp5, i64 %add1)
  store ptr addrspace(200) %tmp11, ptr addrspace(200) @e, align 32
  ret void
}

; define void @g(i64 %x, i64 %y)  nounwind {
;   %tmp1 = tail call i8 addrspace(200)* @llvm.cheri.cap.offset.increment.i64(i8 addrspace(200)* bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*), i64 %x)
;   %tmp3 = tail call i8 addrspace(200)* @llvm.cheri.cap.offset.increment.i64(i8 addrspace(200)* %tmp1, i64 %y)
;   store i8 addrspace(200)* %tmp3, i8 addrspace(200)* addrspace(200)* @e, align 32
;   ret void
; }
;
declare ptr addrspace(200) @llvm.cheri.cap.offset.set.i64(ptr addrspace(200), i64) addrspace(200)
declare i64 @llvm.cheri.cap.offset.get.i64(ptr addrspace(200)) addrspace(200)
