; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/subobject-bounds-redundant-setbounds.c.ll
; REQUIRES: asserts
; RUN: rm -f %t.dbg-opt %t.dbg-llc
; RUN: opt -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -cheri-bound-allocas -debug-only=cheri-bound-allocas -S -o - %s 2>%t.dbg-opt | FileCheck %s
; RUN: FileCheck %s -input-file=%t.dbg-opt -check-prefix DBG
; RUN: llc -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -debug-only=cheri-bound-allocas -o - %s 2>%t.dbg-llc | FileCheck %s -check-prefix ASM
; RUN: FileCheck %s -input-file=%t.dbg-llc -check-prefix DBG
target datalayout = "e-m:e-pf200:128:128:128:64-p:64:64-i64:64-i128:128-n64-S128-A200-P200-G200"

; created from the following C source code (when compiled with subobject bounds):
; void use(void* arg);
;
;void use_inline(int* arg) {
;  *arg = 2;
;}
;
;int stack_array() {
;  int array[10];
;  use(array);
;  return array[5];
;}
;
;int stack_int() {
;  int value = 1;
;  use(&value);
;  return value;
;}
;
;int stack_int_inlined() {
;  int value = 1;
;  use_inline(&value);
;  return value;
;}


define void @use_inline(i32 addrspace(200)* nocapture %arg) local_unnamed_addr addrspace(200) {
; ASM-LABEL: use_inline:
; ASM:       # %bb.0:
; ASM-NEXT:    li a1, 2
; ASM-NEXT:    csw a1, 0(ca0)
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@use_inline
; CHECK-SAME: (i32 addrspace(200)* nocapture [[ARG:%.*]]) local_unnamed_addr addrspace(200) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    store i32 2, i32 addrspace(200)* [[ARG]], align 4
; CHECK-NEXT:    ret void
;
  store i32 2, i32 addrspace(200)* %arg, align 4
  ret void
}

define signext i32 @stack_array() local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: stack_array:
; ASM:       # %bb.0:
; ASM-NEXT:    cincoffset csp, csp, -80
; ASM-NEXT:    csc cra, 64(csp) # 16-byte Folded Spill
; ASM-NEXT:    csc cs0, 48(csp) # 16-byte Folded Spill
; ASM-NEXT:    cincoffset ca0, csp, 8
; ASM-NEXT:    csetbounds cs0, ca0, 40
; ASM-NEXT:    cmove ca0, cs0
; ASM-NEXT:    ccall use
; ASM-NEXT:    clw a0, 20(cs0)
; ASM-NEXT:    clc cra, 64(csp) # 16-byte Folded Reload
; ASM-NEXT:    clc cs0, 48(csp) # 16-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 80
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@stack_array
; CHECK-SAME: () local_unnamed_addr addrspace(200) #[[ATTR1:[0-9]+]] {
; CHECK-NEXT:    [[ARRAY:%.*]] = alloca [10 x i32], align 4, addrspace(200)
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast [10 x i32] addrspace(200)* [[ARRAY]] to i8 addrspace(200)*
; CHECK-NEXT:    call void @llvm.lifetime.start.p200i8(i64 40, i8 addrspace(200)* nonnull [[TMP1]])
; CHECK-NEXT:    [[TMP2:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull [[TMP1]], i64 40)
; CHECK-NEXT:    call void @use(i8 addrspace(200)* [[TMP2]])
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, i8 addrspace(200)* [[TMP2]], i64 20
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[ARRAYIDX]] to i32 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = load i32, i32 addrspace(200)* [[TMP3]], align 4
; CHECK-NEXT:    call void @llvm.lifetime.end.p200i8(i64 40, i8 addrspace(200)* nonnull [[TMP1]])
; CHECK-NEXT:    ret i32 [[TMP4]]
;
  %array = alloca [10 x i32], align 4, addrspace(200)
  %1 = bitcast [10 x i32] addrspace(200)* %array to i8 addrspace(200)*
  call void @llvm.lifetime.start.p200i8(i64 40, i8 addrspace(200)* nonnull %1)
  %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 40)
  call void @use(i8 addrspace(200)* %2)
  %arrayidx = getelementptr inbounds i8, i8 addrspace(200)* %2, i64 20
  %3 = bitcast i8 addrspace(200)* %arrayidx to i32 addrspace(200)*
  %4 = load i32, i32 addrspace(200)* %3, align 4
  call void @llvm.lifetime.end.p200i8(i64 40, i8 addrspace(200)* nonnull %1)
  ret i32 %4
}

; DBG-LABEL: Checking function stack_array
; DBG-NEXT: cheri-bound-allocas:  -Checking if bitcast needs stack bounds:   %1 = bitcast [10 x i32] addrspace(200)* %array to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for lifetime_{start,end}:   call void @llvm.lifetime.end.p200i8(i64 40, i8 addrspace(200)* nonnull %1)
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for use in setbounds with smaller or equal size: original size=40, setbounds size=40 current offset=0:  %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 40)
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for lifetime_{start,end}:   call void @llvm.lifetime.start.p200i8(i64 40, i8 addrspace(200)* nonnull %1)
; DBG-NEXT: cheri-bound-allocas:  -no bitcast users need bounds:   %1 = bitcast [10 x i32] addrspace(200)* %array to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas: stack_array: 0 of 1 users need bounds for   %array = alloca [10 x i32], align 4, addrspace(200)
; DBG-NEXT: cheri-bound-allocas: No need to set bounds on stack alloca  %array = alloca [10 x i32], align 4, addrspace(200)
; DBG-EMPTY:

declare void @llvm.lifetime.start.p200i8(i64 immarg, i8 addrspace(200)* nocapture) addrspace(200)

declare void @use(i8 addrspace(200)*) local_unnamed_addr addrspace(200)

declare i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)*, i64) addrspace(200)

declare void @llvm.lifetime.end.p200i8(i64 immarg, i8 addrspace(200)* nocapture) addrspace(200)

define signext i32 @stack_int() local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: stack_int:
; ASM:       # %bb.0:
; ASM-NEXT:    cincoffset csp, csp, -32
; ASM-NEXT:    csc cra, 16(csp) # 16-byte Folded Spill
; ASM-NEXT:    li a0, 1
; ASM-NEXT:    csw a0, 12(csp)
; ASM-NEXT:    cincoffset ca0, csp, 12
; ASM-NEXT:    csetbounds ca0, ca0, 4
; ASM-NEXT:    ccall use
; ASM-NEXT:    clw a0, 12(csp)
; ASM-NEXT:    clc cra, 16(csp) # 16-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 32
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@stack_int
; CHECK-SAME: () local_unnamed_addr addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:    [[VALUE:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast i32 addrspace(200)* [[VALUE]] to i8 addrspace(200)*
; CHECK-NEXT:    call void @llvm.lifetime.start.p200i8(i64 4, i8 addrspace(200)* nonnull [[TMP1]])
; CHECK-NEXT:    store i32 1, i32 addrspace(200)* [[VALUE]], align 4
; CHECK-NEXT:    [[TMP2:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull [[TMP1]], i64 4)
; CHECK-NEXT:    call void @use(i8 addrspace(200)* [[TMP2]])
; CHECK-NEXT:    [[TMP3:%.*]] = load i32, i32 addrspace(200)* [[VALUE]], align 4
; CHECK-NEXT:    call void @llvm.lifetime.end.p200i8(i64 4, i8 addrspace(200)* nonnull [[TMP1]])
; CHECK-NEXT:    ret i32 [[TMP3]]
;
  %value = alloca i32, align 4, addrspace(200)
  %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
  call void @llvm.lifetime.start.p200i8(i64 4, i8 addrspace(200)* nonnull %1)
  store i32 1, i32 addrspace(200)* %value, align 4
  %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 4)
  call void @use(i8 addrspace(200)* %2)
  %3 = load i32, i32 addrspace(200)* %value, align 4
  call void @llvm.lifetime.end.p200i8(i64 4, i8 addrspace(200)* nonnull %1)
  ret i32 %3
}

; DBG-LABEL: Checking function stack_int
; DBG-NEXT: cheri-bound-allocas:  -Checking if load/store needs bounds (GEP offset is 0):   %3 = load i32, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:   -Load/store size=4, alloca size=4, current GEP offset=0 for i32
; DBG-NEXT: cheri-bound-allocas:   -Load/store is in bounds -> can reuse $csp for   %3 = load i32, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:  -Checking if load/store needs bounds (GEP offset is 0):   store i32 1, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:   -Load/store size=4, alloca size=4, current GEP offset=0 for i32
; DBG-NEXT: cheri-bound-allocas:   -Load/store is in bounds -> can reuse $csp for   store i32 1, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:  -Checking if bitcast needs stack bounds:   %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for lifetime_{start,end}:   call void @llvm.lifetime.end.p200i8(i64 4, i8 addrspace(200)* nonnull %1)
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for use in setbounds with smaller or equal size: original size=4, setbounds size=4 current offset=0:  %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 4)
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for lifetime_{start,end}:   call void @llvm.lifetime.start.p200i8(i64 4, i8 addrspace(200)* nonnull %1)
; DBG-NEXT: cheri-bound-allocas:  -no bitcast users need bounds:   %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas: stack_int: 0 of 3 users need bounds for   %value = alloca i32, align 4, addrspace(200)
; DBG-NEXT: cheri-bound-allocas: No need to set bounds on stack alloca  %value = alloca i32, align 4, addrspace(200)
; DBG-EMPTY:

define signext i32 @stack_int_inlined() local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: stack_int_inlined:
; ASM:       # %bb.0:
; ASM-NEXT:    cincoffset csp, csp, -16
; ASM-NEXT:    li a0, 1
; ASM-NEXT:    csw a0, 12(csp)
; ASM-NEXT:    cincoffset ca0, csp, 12
; ASM-NEXT:    csetbounds ca0, ca0, 4
; ASM-NEXT:    li a1, 2
; ASM-NEXT:    csw a1, 0(ca0)
; ASM-NEXT:    clw a0, 12(csp)
; ASM-NEXT:    cincoffset csp, csp, 16
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@stack_int_inlined
; CHECK-SAME: () local_unnamed_addr addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:    [[VALUE:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast i32 addrspace(200)* [[VALUE]] to i8 addrspace(200)*
; CHECK-NEXT:    call void @llvm.lifetime.start.p200i8(i64 4, i8 addrspace(200)* nonnull [[TMP1]])
; CHECK-NEXT:    store i32 1, i32 addrspace(200)* [[VALUE]], align 4
; CHECK-NEXT:    [[TMP2:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull [[TMP1]], i64 4)
; CHECK-NEXT:    [[ADDRESS_WITH_BOUNDS:%.*]] = bitcast i8 addrspace(200)* [[TMP2]] to i32 addrspace(200)*
; CHECK-NEXT:    store i32 2, i32 addrspace(200)* [[ADDRESS_WITH_BOUNDS]], align 4
; CHECK-NEXT:    [[TMP3:%.*]] = load i32, i32 addrspace(200)* [[VALUE]], align 4
; CHECK-NEXT:    call void @llvm.lifetime.end.p200i8(i64 4, i8 addrspace(200)* nonnull [[TMP1]])
; CHECK-NEXT:    ret i32 [[TMP3]]
;
  %value = alloca i32, align 4, addrspace(200)
  %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
  call void @llvm.lifetime.start.p200i8(i64 4, i8 addrspace(200)* nonnull %1)
  store i32 1, i32 addrspace(200)* %value, align 4
  %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 4)
  %address.with.bounds = bitcast i8 addrspace(200)* %2 to i32 addrspace(200)*
  store i32 2, i32 addrspace(200)* %address.with.bounds, align 4
  %3 = load i32, i32 addrspace(200)* %value, align 4
  call void @llvm.lifetime.end.p200i8(i64 4, i8 addrspace(200)* nonnull %1)
  ret i32 %3
}

; DBG-LABEL: Checking function stack_int_inlined
; DBG-NEXT: cheri-bound-allocas:  -Checking if load/store needs bounds (GEP offset is 0):   %3 = load i32, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:   -Load/store size=4, alloca size=4, current GEP offset=0 for i32
; DBG-NEXT: cheri-bound-allocas:   -Load/store is in bounds -> can reuse $csp for   %3 = load i32, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:  -Checking if load/store needs bounds (GEP offset is 0):   store i32 1, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:   -Load/store size=4, alloca size=4, current GEP offset=0 for i32
; DBG-NEXT: cheri-bound-allocas:   -Load/store is in bounds -> can reuse $csp for   store i32 1, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:  -Checking if bitcast needs stack bounds:   %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for lifetime_{start,end}:   call void @llvm.lifetime.end.p200i8(i64 4, i8 addrspace(200)* nonnull %1)
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for use in setbounds with smaller or equal size: original size=4, setbounds size=4 current offset=0:  %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 4)
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for lifetime_{start,end}:   call void @llvm.lifetime.start.p200i8(i64 4, i8 addrspace(200)* nonnull %1)
; DBG-NEXT: cheri-bound-allocas:  -no bitcast users need bounds:   %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas: stack_int_inlined: 0 of 3 users need bounds for   %value = alloca i32, align 4, addrspace(200)
; DBG-NEXT: cheri-bound-allocas: No need to set bounds on stack alloca  %value = alloca i32, align 4, addrspace(200)
; DBG-EMPTY:

define signext i32 @out_of_bounds_setbounds() local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: out_of_bounds_setbounds:
; ASM:       # %bb.0:
; ASM-NEXT:    cincoffset csp, csp, -16
; ASM-NEXT:    cincoffset ca0, csp, 12
; ASM-NEXT:    csetbounds ca0, ca0, 4
; ASM-NEXT:    csetbounds ca0, ca0, 5
; ASM-NEXT:    li a1, 2
; ASM-NEXT:    csw a1, 0(ca0)
; ASM-NEXT:    clw a0, 12(csp)
; ASM-NEXT:    cincoffset csp, csp, 16
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@out_of_bounds_setbounds
; CHECK-SAME: () local_unnamed_addr addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:    [[VALUE:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast i32 addrspace(200)* [[VALUE]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP2:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP1]], i64 4)
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[TMP2]] to i32 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 addrspace(200)* [[TMP3]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull [[TMP4]], i64 5)
; CHECK-NEXT:    [[ADDRESS_WITH_BOUNDS:%.*]] = bitcast i8 addrspace(200)* [[TMP5]] to i32 addrspace(200)*
; CHECK-NEXT:    store i32 2, i32 addrspace(200)* [[ADDRESS_WITH_BOUNDS]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = load i32, i32 addrspace(200)* [[VALUE]], align 4
; CHECK-NEXT:    ret i32 [[TMP6]]
;
  %value = alloca i32, align 4, addrspace(200)
  ; TOO big, cannot elide the setbonds:
  %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
  %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 5)
  %address.with.bounds = bitcast i8 addrspace(200)* %2 to i32 addrspace(200)*
  store i32 2, i32 addrspace(200)* %address.with.bounds, align 4
  %3 = load i32, i32 addrspace(200)* %value, align 4
  ret i32 %3
}

; DBG-NEXT: Checking function out_of_bounds_setbounds
; DBG-NEXT: cheri-bound-allocas:  -Checking if load/store needs bounds (GEP offset is 0):   %3 = load i32, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:   -Load/store size=4, alloca size=4, current GEP offset=0 for i32
; DBG-NEXT: cheri-bound-allocas:   -Load/store is in bounds -> can reuse $csp for   %3 = load i32, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:  -Checking if bitcast needs stack bounds:   %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas:   -out_of_bounds_setbounds: setbounds use offset OUT OF BOUNDS and will trap -> adding csetbounds:   %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 5)
; DBG-NEXT: cheri-bound-allocas:  -Adding stack bounds since bitcast user needs bounds:   %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 5)
; DBG-NEXT: cheri-bound-allocas: Found alloca use that needs bounds:   %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas: out_of_bounds_setbounds: 1 of 2 users need bounds for   %value = alloca i32, align 4, addrspace(200)
; DBG-NEXT: out_of_bounds_setbounds: setting bounds on stack alloca to 4  %value = alloca i32, align 4, addrspace(200)
; DBG-EMPTY:

define signext i32 @setbounds_escapes() local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: setbounds_escapes:
; ASM:       # %bb.0:
; ASM-NEXT:    cincoffset csp, csp, -32
; ASM-NEXT:    csc cra, 16(csp) # 16-byte Folded Spill
; ASM-NEXT:    cincoffset ca0, csp, 12
; ASM-NEXT:    csetbounds ca0, ca0, 4
; ASM-NEXT:    li a1, 2
; ASM-NEXT:    csw a1, 0(ca0)
; ASM-NEXT:    ccall use
; ASM-NEXT:    clw a0, 12(csp)
; ASM-NEXT:    clc cra, 16(csp) # 16-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 32
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@setbounds_escapes
; CHECK-SAME: () local_unnamed_addr addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:    [[VALUE:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast i32 addrspace(200)* [[VALUE]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP2:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull [[TMP1]], i64 4)
; CHECK-NEXT:    [[ADDRESS_WITH_BOUNDS:%.*]] = bitcast i8 addrspace(200)* [[TMP2]] to i32 addrspace(200)*
; CHECK-NEXT:    store i32 2, i32 addrspace(200)* [[ADDRESS_WITH_BOUNDS]], align 4
; CHECK-NEXT:    call void @use(i8 addrspace(200)* [[TMP2]])
; CHECK-NEXT:    [[TMP3:%.*]] = load i32, i32 addrspace(200)* [[VALUE]], align 4
; CHECK-NEXT:    ret i32 [[TMP3]]
;
  %value = alloca i32, align 4, addrspace(200)
  ; Too big, cannot elide the setbonds:
  %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
  %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 4)
  %address.with.bounds = bitcast i8 addrspace(200)* %2 to i32 addrspace(200)*
  store i32 2, i32 addrspace(200)* %address.with.bounds, align 4
  call void @use(i8 addrspace(200)* %2)
  %3 = load i32, i32 addrspace(200)* %value, align 4
  ret i32 %3
}

; DBG-NEXT: Checking function setbounds_escapes
; DBG-NEXT: cheri-bound-allocas:  -Checking if load/store needs bounds (GEP offset is 0):   %3 = load i32, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:   -Load/store size=4, alloca size=4, current GEP offset=0 for i32
; DBG-NEXT: cheri-bound-allocas:   -Load/store is in bounds -> can reuse $csp for   %3 = load i32, i32 addrspace(200)* %value, align 4
; DBG-NEXT: cheri-bound-allocas:  -Checking if bitcast needs stack bounds:   %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas:   -No need for stack bounds for use in setbounds with smaller or equal size: original size=4, setbounds size=4 current offset=0:  %2 = call i8 addrspace(200)* @llvm.cheri.cap.bounds.set.i64(i8 addrspace(200)* nonnull %1, i64 4)
; DBG-NEXT: cheri-bound-allocas:  -no bitcast users need bounds:   %1 = bitcast i32 addrspace(200)* %value to i8 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas: setbounds_escapes: 0 of 2 users need bounds for   %value = alloca i32, align 4, addrspace(200)
; DBG-NEXT: cheri-bound-allocas: No need to set bounds on stack alloca  %value = alloca i32, align 4, addrspace(200)
; DBG-EMPTY:

; llvm.assume() should not add bounds:
define void @assume_aligned() local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: assume_aligned:
; ASM:       # %bb.0:
; ASM-NEXT:    cincoffset csp, csp, -16
; ASM-NEXT:    li a0, 1
; ASM-NEXT:    csw a0, 12(csp)
; ASM-NEXT:    cincoffset csp, csp, 16
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@assume_aligned
; CHECK-SAME: () local_unnamed_addr addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:    [[TMP1:%.*]] = alloca [4 x i8], align 4, addrspace(200)
; CHECK-NEXT:    call void @llvm.assume(i1 true) [ "align"([4 x i8] addrspace(200)* [[TMP1]], i64 4) ]
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast [4 x i8] addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; CHECK-NEXT:    store i32 1, i32 addrspace(200)* [[TMP2]], align 4
; CHECK-NEXT:    ret void
;
  %1 = alloca [4 x i8], align 4, addrspace(200)
  call void @llvm.assume(i1 true) [ "align"([4 x i8] addrspace(200)* %1, i64 4) ]
  %2 = bitcast [4 x i8] addrspace(200)* %1 to i32 addrspace(200)*
  store i32 1, i32 addrspace(200)* %2
  ret void
}

; DBG-NEXT: Checking function assume_aligned
; DBG-NEXT: cheri-bound-allocas:  -Checking if bitcast needs stack bounds:   %2 = bitcast [4 x i8] addrspace(200)* %1 to i32 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas:   -Checking if load/store needs bounds (GEP offset is 0):   store i32 1, i32 addrspace(200)* %2, align 4
; DBG-NEXT: cheri-bound-allocas:    -Load/store size=4, alloca size=4, current GEP offset=0 for i32
; DBG-NEXT: cheri-bound-allocas:    -Load/store is in bounds -> can reuse $csp for   store i32 1, i32 addrspace(200)* %2, align 4
; DBG-NEXT: cheri-bound-allocas:  -no bitcast users need bounds:   %2 = bitcast [4 x i8] addrspace(200)* %1 to i32 addrspace(200)*
; DBG-NEXT: cheri-bound-allocas:  -No need for stack bounds for assume:   call void @llvm.assume(i1 true) [ "align"([4 x i8] addrspace(200)* %1, i64 4) ]
; DBG-NEXT: cheri-bound-allocas: assume_aligned: 0 of 2 users need bounds for   %1 = alloca [4 x i8], align 4, addrspace(200)
; DBG-NEXT: cheri-bound-allocas: No need to set bounds on stack alloca  %1 = alloca [4 x i8], align 4, addrspace(200)
; DBG-EMPTY:

declare void @llvm.assume(i1) addrspace(200)
