; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/stack-bounds-pass-phi.ll
; REQUIRES: asserts
; RUN: opt -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -cheri-bound-allocas %s -o - -S -cheri-stack-bounds=if-needed \
; RUN:    -cheri-stack-bounds-single-intrinsic-threshold=10 -debug-only=cheri-bound-allocas 2>%t.dbg | FileCheck %s
; RUN: llc -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -cheri-stack-bounds=if-needed -O2 -cheri-stack-bounds-single-intrinsic-threshold=10 < %s | %cheri_FileCheck %s -check-prefix ASM
; RUN: FileCheck %s -check-prefix DBG -input-file=%t.dbg
target datalayout = "e-m:e-pf200:128:128:128:64-p:64:64-i64:64-i128:128-n64-S128-A200-P200-G200"

declare void @foo(i32 addrspace(200)*) addrspace(200)

; Check that we don't attempt to insert stack bounds intrinisics before the PHI at the start of a basic block:
define void @test_phi(i1 %cond) addrspace(200) nounwind {
; ASM-LABEL: test_phi:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    cincoffset csp, csp, -48
; ASM-NEXT:    csc cra, 32(csp) # 16-byte Folded Spill
; ASM-NEXT:    csc cs0, 16(csp) # 16-byte Folded Spill
; ASM-NEXT:    andi a0, a0, 1
; ASM-NEXT:    beqz a0, .LBB0_2
; ASM-NEXT:  # %bb.1: # %block1
; ASM-NEXT:    cmove ca0, cnull
; ASM-NEXT:    li a1, 1
; ASM-NEXT:    csw a1, 12(csp)
; ASM-NEXT:    li a1, 2
; ASM-NEXT:    csw a1, 8(csp)
; ASM-NEXT:    li a1, 3
; ASM-NEXT:    csw a1, 4(csp)
; ASM-NEXT:    cincoffset ca1, csp, 8
; ASM-NEXT:    j .LBB0_3
; ASM-NEXT:  .LBB0_2: # %block2
; ASM-NEXT:    li a0, 4
; ASM-NEXT:    csw a0, 12(csp)
; ASM-NEXT:    li a0, 5
; ASM-NEXT:    csw a0, 8(csp)
; ASM-NEXT:    li a0, 6
; ASM-NEXT:    csw a0, 4(csp)
; ASM-NEXT:    cincoffset ca0, csp, 12
; ASM-NEXT:    csetbounds ca0, ca0, 4
; ASM-NEXT:    cincoffset ca1, csp, 4
; ASM-NEXT:  .LBB0_3: # %phi_block
; ASM-NEXT:    csetbounds cs0, ca1, 4
; ASM-NEXT:    ccall foo
; ASM-NEXT:    cmove ca0, cs0
; ASM-NEXT:    ccall foo
; ASM-NEXT:    clc cra, 32(csp) # 16-byte Folded Reload
; ASM-NEXT:    clc cs0, 16(csp) # 16-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 48
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@test_phi
; CHECK-SAME: (i1 [[COND:%.*]]) addrspace(200) #[[ATTR1:[0-9]+]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA1:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA2:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA3:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    br i1 [[COND]], label [[BLOCK1:%.*]], label [[BLOCK2:%.*]]
; CHECK:       block1:
; CHECK-NEXT:    store i32 1, i32 addrspace(200)* [[ALLOCA1]], align 4
; CHECK-NEXT:    store i32 2, i32 addrspace(200)* [[ALLOCA2]], align 4
; CHECK-NEXT:    store i32 3, i32 addrspace(200)* [[ALLOCA3]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA2]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK:%.*]]
; CHECK:       block2:
; CHECK-NEXT:    store i32 4, i32 addrspace(200)* [[ALLOCA1]], align 4
; CHECK-NEXT:    store i32 5, i32 addrspace(200)* [[ALLOCA2]], align 4
; CHECK-NEXT:    store i32 6, i32 addrspace(200)* [[ALLOCA3]], align 4
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA1]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; CHECK-NEXT:    [[TMP6:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA3]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP7:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP6]], i64 4)
; CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 addrspace(200)* [[TMP7]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK]]
; CHECK:       phi_block:
; CHECK-NEXT:    [[VAL1:%.*]] = phi i32 addrspace(200)* [ null, [[BLOCK1]] ], [ [[TMP5]], [[BLOCK2]] ]
; CHECK-NEXT:    [[VAL2:%.*]] = phi i32 addrspace(200)* [ [[TMP2]], [[BLOCK1]] ], [ [[TMP8]], [[BLOCK2]] ]
; CHECK-NEXT:    call void @foo(i32 addrspace(200)* [[VAL1]])
; CHECK-NEXT:    call void @foo(i32 addrspace(200)* [[VAL2]])
; CHECK-NEXT:    ret void
;
entry:
  %alloca1 = alloca i32, align 4, addrspace(200)
  %alloca2 = alloca i32, align 4, addrspace(200)
  %alloca3 = alloca i32, align 4, addrspace(200)
  br i1 %cond, label %block1, label %block2

block1:
  store i32 1, i32 addrspace(200)* %alloca1, align 4
  store i32 2, i32 addrspace(200)* %alloca2, align 4
  store i32 3, i32 addrspace(200)* %alloca3, align 4
  br label %phi_block

block2:
  store i32 4, i32 addrspace(200)* %alloca1, align 4
  store i32 5, i32 addrspace(200)* %alloca2, align 4
  store i32 6, i32 addrspace(200)* %alloca3, align 4
  br label %phi_block

phi_block:
  %val1 = phi i32 addrspace(200)* [ null, %block1 ], [ %alloca1, %block2 ]
  %val2 = phi i32 addrspace(200)* [ %alloca2, %block1 ], [ %alloca3, %block2 ]
  call void @foo(i32 addrspace(200)* %val1)
  call void @foo(i32 addrspace(200)* %val2)
  ret void
}

; Check that we don't place all bounded allocas in the entry block, instead only do it in the predecessor
define void @test_only_created_in_predecessor_block(i1 %cond) addrspace(200) nounwind {
; ASM-LABEL: test_only_created_in_predecessor_block:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    cincoffset csp, csp, -32
; ASM-NEXT:    csc cra, 16(csp) # 16-byte Folded Spill
; ASM-NEXT:    andi a0, a0, 1
; ASM-NEXT:    beqz a0, .LBB1_2
; ASM-NEXT:  # %bb.1: # %block1
; ASM-NEXT:    li a0, 1
; ASM-NEXT:    csw a0, 12(csp)
; ASM-NEXT:    cincoffset ca0, csp, 12
; ASM-NEXT:    j .LBB1_3
; ASM-NEXT:  .LBB1_2: # %block2
; ASM-NEXT:    li a0, 5
; ASM-NEXT:    csw a0, 8(csp)
; ASM-NEXT:    cincoffset ca0, csp, 8
; ASM-NEXT:  .LBB1_3: # %phi_block
; ASM-NEXT:    csetbounds ca0, ca0, 4
; ASM-NEXT:    ccall foo
; ASM-NEXT:    clc cra, 16(csp) # 16-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 32
; ASM-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@test_only_created_in_predecessor_block
; CHECK-SAME: (i1 [[COND:%.*]]) addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA1:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA2:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    br i1 [[COND]], label [[BLOCK1:%.*]], label [[BLOCK2:%.*]]
; CHECK:       block1:
; CHECK-NEXT:    store i32 1, i32 addrspace(200)* [[ALLOCA1]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA1]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK:%.*]]
; CHECK:       block2:
; CHECK-NEXT:    store i32 5, i32 addrspace(200)* [[ALLOCA2]], align 4
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA2]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK]]
; CHECK:       phi_block:
; CHECK-NEXT:    [[VAL1:%.*]] = phi i32 addrspace(200)* [ [[TMP2]], [[BLOCK1]] ], [ [[TMP5]], [[BLOCK2]] ]
; CHECK-NEXT:    call void @foo(i32 addrspace(200)* [[VAL1]])
; CHECK-NEXT:    ret void
;
entry:
  %alloca1 = alloca i32, align 4, addrspace(200)
  %alloca2 = alloca i32, align 4, addrspace(200)
  br i1 %cond, label %block1, label %block2

block1:
  store i32 1, i32 addrspace(200)* %alloca1, align 4
  br label %phi_block

block2:
  store i32 5, i32 addrspace(200)* %alloca2, align 4
  br label %phi_block

phi_block:
  %val1 = phi i32 addrspace(200)* [ %alloca1, %block1 ], [ %alloca2, %block2 ]
  call void @foo(i32 addrspace(200)* %val1)
  ret void
}

; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(i32 addrspace(200)* %val1)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca1 = alloca i32, align 4, addrspace(200)
; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(i32 addrspace(200)* %val2)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca2 = alloca i32, align 4, addrspace(200)
; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(i32 addrspace(200)* %val2)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca3 = alloca i32, align 4, addrspace(200)
