; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --scrub-attributes --version 2
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/stack-bounds-pass-phi.ll
; REQUIRES: asserts
; RUN: opt -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -cheri-bound-allocas %s -o - -S -cheri-stack-bounds=if-needed \
; RUN:    -cheri-stack-bounds-single-intrinsic-threshold=10 -debug-only=cheri-bound-allocas 2>%t.dbg | FileCheck %s
; RUN: llc -mtriple=riscv64 --relocation-model=pic -target-abi l64pc128d -mattr=+xcheri,+cap-mode,+f,+d -cheri-stack-bounds=if-needed -O2 -cheri-stack-bounds-single-intrinsic-threshold=10 < %s | %cheri_FileCheck %s -check-prefix ASM
; RUN: FileCheck %s -check-prefix DBG -input-file=%t.dbg
target datalayout = "e-m:e-pf200:128:128:128:64-p:64:64-i64:64-i128:128-n64-S128-A200-P200-G200"

declare void @foo(ptr addrspace(200)) addrspace(200)

; Check that we don't attempt to insert stack bounds intrinisics before the PHI at the start of a basic block:
define void @test_phi(i1 %cond) addrspace(200) nounwind {
; ASM-LABEL: test_phi:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    cincoffset csp, csp, -48
; ASM-NEXT:    sc cra, 32(csp) # 16-byte Folded Spill
; ASM-NEXT:    sc cs0, 16(csp) # 16-byte Folded Spill
; ASM-NEXT:    andi a0, a0, 1
; ASM-NEXT:    beqz a0, .LBB0_2
; ASM-NEXT:  # %bb.1: # %block1
; ASM-NEXT:    cmove ca0, cnull
; ASM-NEXT:    li a1, 1
; ASM-NEXT:    sw a1, 12(csp)
; ASM-NEXT:    li a1, 2
; ASM-NEXT:    sw a1, 8(csp)
; ASM-NEXT:    li a1, 3
; ASM-NEXT:    sw a1, 4(csp)
; ASM-NEXT:    cincoffset ca1, csp, 8
; ASM-NEXT:    j .LBB0_3
; ASM-NEXT:  .LBB0_2: # %block2
; ASM-NEXT:    li a0, 4
; ASM-NEXT:    sw a0, 12(csp)
; ASM-NEXT:    li a0, 5
; ASM-NEXT:    sw a0, 8(csp)
; ASM-NEXT:    li a0, 6
; ASM-NEXT:    sw a0, 4(csp)
; ASM-NEXT:    cincoffset ca0, csp, 12
; ASM-NEXT:    csetbounds ca0, ca0, 4
; ASM-NEXT:    cincoffset ca1, csp, 4
; ASM-NEXT:  .LBB0_3: # %phi_block
; ASM-NEXT:    csetbounds cs0, ca1, 4
; ASM-NEXT:    ccall foo
; ASM-NEXT:    cmove ca0, cs0
; ASM-NEXT:    ccall foo
; ASM-NEXT:    lc cra, 32(csp) # 16-byte Folded Reload
; ASM-NEXT:    lc cs0, 16(csp) # 16-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 48
; ASM-NEXT:    ret
; CHECK-LABEL: define void @test_phi
; CHECK-SAME: (i1 [[COND:%.*]]) addrspace(200) #[[ATTR1:[0-9]+]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA1:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA2:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA3:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    br i1 [[COND]], label [[BLOCK1:%.*]], label [[BLOCK2:%.*]]
; CHECK:       block1:
; CHECK-NEXT:    store i32 1, ptr addrspace(200) [[ALLOCA1]], align 4
; CHECK-NEXT:    store i32 2, ptr addrspace(200) [[ALLOCA2]], align 4
; CHECK-NEXT:    store i32 3, ptr addrspace(200) [[ALLOCA3]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = call ptr addrspace(200) @llvm.cheri.bounded.stack.cap.i64(ptr addrspace(200) [[ALLOCA2]], i64 4)
; CHECK-NEXT:    br label [[PHI_BLOCK:%.*]]
; CHECK:       block2:
; CHECK-NEXT:    store i32 4, ptr addrspace(200) [[ALLOCA1]], align 4
; CHECK-NEXT:    store i32 5, ptr addrspace(200) [[ALLOCA2]], align 4
; CHECK-NEXT:    store i32 6, ptr addrspace(200) [[ALLOCA3]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = call ptr addrspace(200) @llvm.cheri.bounded.stack.cap.i64(ptr addrspace(200) [[ALLOCA1]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = call ptr addrspace(200) @llvm.cheri.bounded.stack.cap.i64(ptr addrspace(200) [[ALLOCA3]], i64 4)
; CHECK-NEXT:    br label [[PHI_BLOCK]]
; CHECK:       phi_block:
; CHECK-NEXT:    [[VAL1:%.*]] = phi ptr addrspace(200) [ null, [[BLOCK1]] ], [ [[TMP1]], [[BLOCK2]] ]
; CHECK-NEXT:    [[VAL2:%.*]] = phi ptr addrspace(200) [ [[TMP0]], [[BLOCK1]] ], [ [[TMP2]], [[BLOCK2]] ]
; CHECK-NEXT:    call void @foo(ptr addrspace(200) [[VAL1]])
; CHECK-NEXT:    call void @foo(ptr addrspace(200) [[VAL2]])
; CHECK-NEXT:    ret void
;
entry:
  %alloca1 = alloca i32, align 4, addrspace(200)
  %alloca2 = alloca i32, align 4, addrspace(200)
  %alloca3 = alloca i32, align 4, addrspace(200)
  br i1 %cond, label %block1, label %block2

block1:
  store i32 1, ptr addrspace(200) %alloca1, align 4
  store i32 2, ptr addrspace(200) %alloca2, align 4
  store i32 3, ptr addrspace(200) %alloca3, align 4
  br label %phi_block

block2:
  store i32 4, ptr addrspace(200) %alloca1, align 4
  store i32 5, ptr addrspace(200) %alloca2, align 4
  store i32 6, ptr addrspace(200) %alloca3, align 4
  br label %phi_block

phi_block:
  %val1 = phi ptr addrspace(200) [ null, %block1 ], [ %alloca1, %block2 ]
  %val2 = phi ptr addrspace(200) [ %alloca2, %block1 ], [ %alloca3, %block2 ]
  call void @foo(ptr addrspace(200) %val1)
  call void @foo(ptr addrspace(200) %val2)
  ret void
}

; Check that we don't place all bounded allocas in the entry block, instead only do it in the predecessor
define void @test_only_created_in_predecessor_block(i1 %cond) addrspace(200) nounwind {
; ASM-LABEL: test_only_created_in_predecessor_block:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    cincoffset csp, csp, -32
; ASM-NEXT:    sc cra, 16(csp) # 16-byte Folded Spill
; ASM-NEXT:    andi a0, a0, 1
; ASM-NEXT:    beqz a0, .LBB1_2
; ASM-NEXT:  # %bb.1: # %block1
; ASM-NEXT:    li a0, 1
; ASM-NEXT:    sw a0, 12(csp)
; ASM-NEXT:    cincoffset ca0, csp, 12
; ASM-NEXT:    j .LBB1_3
; ASM-NEXT:  .LBB1_2: # %block2
; ASM-NEXT:    li a0, 5
; ASM-NEXT:    sw a0, 8(csp)
; ASM-NEXT:    cincoffset ca0, csp, 8
; ASM-NEXT:  .LBB1_3: # %phi_block
; ASM-NEXT:    csetbounds ca0, ca0, 4
; ASM-NEXT:    ccall foo
; ASM-NEXT:    lc cra, 16(csp) # 16-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 32
; ASM-NEXT:    ret
; CHECK-LABEL: define void @test_only_created_in_predecessor_block
; CHECK-SAME: (i1 [[COND:%.*]]) addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA1:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA2:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    br i1 [[COND]], label [[BLOCK1:%.*]], label [[BLOCK2:%.*]]
; CHECK:       block1:
; CHECK-NEXT:    store i32 1, ptr addrspace(200) [[ALLOCA1]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = call ptr addrspace(200) @llvm.cheri.bounded.stack.cap.i64(ptr addrspace(200) [[ALLOCA1]], i64 4)
; CHECK-NEXT:    br label [[PHI_BLOCK:%.*]]
; CHECK:       block2:
; CHECK-NEXT:    store i32 5, ptr addrspace(200) [[ALLOCA2]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = call ptr addrspace(200) @llvm.cheri.bounded.stack.cap.i64(ptr addrspace(200) [[ALLOCA2]], i64 4)
; CHECK-NEXT:    br label [[PHI_BLOCK]]
; CHECK:       phi_block:
; CHECK-NEXT:    [[VAL1:%.*]] = phi ptr addrspace(200) [ [[TMP0]], [[BLOCK1]] ], [ [[TMP1]], [[BLOCK2]] ]
; CHECK-NEXT:    call void @foo(ptr addrspace(200) [[VAL1]])
; CHECK-NEXT:    ret void
;
entry:
  %alloca1 = alloca i32, align 4, addrspace(200)
  %alloca2 = alloca i32, align 4, addrspace(200)
  br i1 %cond, label %block1, label %block2

block1:
  store i32 1, ptr addrspace(200) %alloca1, align 4
  br label %phi_block

block2:
  store i32 5, ptr addrspace(200) %alloca2, align 4
  br label %phi_block

phi_block:
  %val1 = phi ptr addrspace(200) [ %alloca1, %block1 ], [ %alloca2, %block2 ]
  call void @foo(ptr addrspace(200) %val1)
  ret void
}

; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(ptr addrspace(200) %val1)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca1 = alloca i32, align 4, addrspace(200)
; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(ptr addrspace(200) %val2)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca2 = alloca i32, align 4, addrspace(200)
; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(ptr addrspace(200) %val2)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca3 = alloca i32, align 4, addrspace(200)
