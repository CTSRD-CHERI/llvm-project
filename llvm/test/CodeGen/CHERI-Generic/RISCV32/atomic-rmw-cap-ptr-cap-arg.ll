; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/atomic-rmw-cap-ptr-cap-arg.ll
; Check that we can generate sensible code for atomic operations using capability pointers on capabilities
; in both hybrid and purecap mode. For RISC-V this means expanding the instruction using the explicit
; addressing mode LR/SC instructions.
; See https://github.com/CTSRD-CHERI/llvm-project/issues/470
; RUN: %riscv32_cheri_purecap_llc %s -o - -mattr=+f,+a -verify-machineinstrs | FileCheck %s --check-prefixes=PURECAP,PURECAP-ATOMICS
; RUN: %riscv32_cheri_purecap_llc %s -o - -mattr=+f,-a -verify-machineinstrs | FileCheck %s --check-prefixes=PURECAP,PURECAP-LIBCALLS
; RUN: %riscv32_cheri_llc %s -o - -mattr=+f,+a -verify-machineinstrs | FileCheck %s --check-prefixes=HYBRID,HYBRID-ATOMICS
; RUN_FIXME_THIS_CRASHES: %riscv32_cheri_llc %s -o - -mattr=+f,-a -verify-machineinstrs | FileCheck %s --check-prefixes=HYBRID,HYBRID-LIBCALLS

define dso_local void @atomic_cap_ptr_xchg(i8 addrspace(200)* addrspace(200)* %ptr, i8 addrspace(200)* %val) nounwind {
; PURECAP-ATOMICS-LABEL: atomic_cap_ptr_xchg:
; PURECAP-ATOMICS:       # %bb.0: # %bb
; PURECAP-ATOMICS-NEXT:    camoswap.c.aqrl ca0, ca1, (ca0)
; PURECAP-ATOMICS-NEXT:    cret
;
; PURECAP-LIBCALLS-LABEL: atomic_cap_ptr_xchg:
; PURECAP-LIBCALLS:       # %bb.0: # %bb
; PURECAP-LIBCALLS-NEXT:    cincoffset csp, csp, -16
; PURECAP-LIBCALLS-NEXT:    csc cra, 8(csp)
; PURECAP-LIBCALLS-NEXT:  .LBB0_1: # %bb
; PURECAP-LIBCALLS-NEXT:    # Label of block must be emitted
; PURECAP-LIBCALLS-NEXT:    auipcc ca3, %captab_pcrel_hi(__atomic_exchange_cap)
; PURECAP-LIBCALLS-NEXT:    clc ca3, %pcrel_lo(.LBB0_1)(ca3)
; PURECAP-LIBCALLS-NEXT:    addi a2, zero, 5
; PURECAP-LIBCALLS-NEXT:    cjalr ca3
; PURECAP-LIBCALLS-NEXT:    clc cra, 8(csp)
; PURECAP-LIBCALLS-NEXT:    cincoffset csp, csp, 16
; PURECAP-LIBCALLS-NEXT:    cret
;
; HYBRID-LABEL: atomic_cap_ptr_xchg:
; HYBRID:       # %bb.0: # %bb
; HYBRID-NEXT:    fence rw, rw
; HYBRID-NEXT:    lc.cap ca2, (ca0)
; HYBRID-NEXT:  .LBB0_1: # %atomicrmw.start
; HYBRID-NEXT:    # =>This Loop Header: Depth=1
; HYBRID-NEXT:    # Child Loop BB0_3 Depth 2
; HYBRID-NEXT:    cmove ca3, ca1
; HYBRID-NEXT:  .LBB0_3: # %atomicrmw.start
; HYBRID-NEXT:    # Parent Loop BB0_1 Depth=1
; HYBRID-NEXT:    # => This Inner Loop Header: Depth=2
; HYBRID-NEXT:    lr.c.cap ca3, (ca0)
; HYBRID-NEXT:    bne a3, a2, .LBB0_5
; HYBRID-NEXT:  # %bb.4: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB0_3 Depth=2
; HYBRID-NEXT:    cmove ca4, ca3
; HYBRID-NEXT:    sc.c.cap ca4, (ca0)
; HYBRID-NEXT:    bnez a4, .LBB0_3
; HYBRID-NEXT:  .LBB0_5: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB0_1 Depth=1
; HYBRID-NEXT:    mv a4, a2
; HYBRID-NEXT:    cmove ca2, ca3
; HYBRID-NEXT:    bne a3, a4, .LBB0_1
; HYBRID-NEXT:  # %bb.2: # %atomicrmw.end
; HYBRID-NEXT:    fence r, rw
; HYBRID-NEXT:    ret
bb:
  %tmp = atomicrmw xchg i8 addrspace(200)* addrspace(200)* %ptr, i8 addrspace(200)* %val seq_cst
  ret void
}

; TODO: support all these:
; define dso_local void @atomic_cap_ptr_add(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw add i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
;
; define dso_local void @atomic_cap_ptr_sub(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw sub i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
;
; define dso_local void @atomic_cap_ptr_and(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw and i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
;
; define dso_local void @atomic_cap_ptr_nand(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw nand i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
;
; define dso_local void @atomic_cap_ptr_or(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw or i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
;
; define dso_local void @atomic_cap_ptr_xor(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw xor i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
;
; define dso_local void @atomic_cap_ptr_max(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw max i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
;
; define dso_local void @atomic_cap_ptr_min(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw min i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
;
; define dso_local void @atomic_cap_ptr_umax(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw umax i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
;
; define dso_local void @atomic_cap_ptr_umin(i8 addrspace(200)* addrspace(200)* %ptr, i32 %val) nounwind {
; bb:
;   %tmp = atomicrmw umin i8 addrspace(200)* addrspace(200)* %ptr, i32 %val seq_cst
;   ret void
; }
