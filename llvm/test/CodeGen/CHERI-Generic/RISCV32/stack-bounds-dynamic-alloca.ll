; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/stack-bounds-dynamic-alloca.ll
; RUN: opt -mtriple=riscv32 --relocation-model=pic -target-abi il32pc64f -mattr=+xcheri,+cap-mode,+f -cheri-bound-allocas -o - -S %s | FileCheck %s
; RUN: llc -mtriple=riscv32 --relocation-model=pic -target-abi il32pc64f -mattr=+xcheri,+cap-mode,+f -O0 %s -o - | FileCheck %s -check-prefix ASM
; RUN: llc -mtriple=riscv32 --relocation-model=pic -target-abi il32pc64f -mattr=+xcheri,+cap-mode,+f -O2 %s -o - | FileCheck %s -check-prefix ASM-OPT

; reduced C test case:
; __builtin_va_list a;
; char *b;
; void c() {
;   while (__builtin_va_arg(a, char))
;     b = __builtin_alloca(sizeof(b));
;   d(b);
; }
target datalayout = "e-m:e-pf200:64:64:64:32-p:32:32-i64:64-n32-S128-A200-P200-G200"

declare i32 @use_alloca(i8 addrspace(200)*) local_unnamed_addr addrspace(200)

define i32 @alloca_in_entry(i1 %arg) local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: alloca_in_entry:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    cincoffset csp, csp, -32
; ASM-NEXT:    csc cra, 24(csp) # 8-byte Folded Spill
; ASM-NEXT:    mv a1, a0
; ASM-NEXT:    andi a0, a1, 1
; ASM-NEXT:    li a1, 0
; ASM-NEXT:    beq a0, a1, .LBB0_4
; ASM-NEXT:    j .LBB0_1
; ASM-NEXT:  .LBB0_1: # %do_alloca
; ASM-NEXT:    j .LBB0_2
; ASM-NEXT:  .LBB0_2: # %use_alloca_no_bounds
; ASM-NEXT:    li a0, 0
; ASM-NEXT:    csw a0, 12(csp)
; ASM-NEXT:    li a0, 1234
; ASM-NEXT:    csw a0, 8(csp)
; ASM-NEXT:    j .LBB0_3
; ASM-NEXT:  .LBB0_3: # %use_alloca_need_bounds
; ASM-NEXT:    cincoffset ca0, csp, 0
; ASM-NEXT:    csetbounds ca0, ca0, 16
; ASM-NEXT:    ccall use_alloca
; ASM-NEXT:    j .LBB0_4
; ASM-NEXT:  .LBB0_4: # %exit
; ASM-NEXT:    li a0, 123
; ASM-NEXT:    clc cra, 24(csp) # 8-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 32
; ASM-NEXT:    cret
;
; ASM-OPT-LABEL: alloca_in_entry:
; ASM-OPT:       # %bb.0: # %entry
; ASM-OPT-NEXT:    andi a0, a0, 1
; ASM-OPT-NEXT:    beqz a0, .LBB0_2
; ASM-OPT-NEXT:  # %bb.1: # %do_alloca
; ASM-OPT-NEXT:    cincoffset csp, csp, -32
; ASM-OPT-NEXT:    csc cra, 24(csp) # 8-byte Folded Spill
; ASM-OPT-NEXT:    csw zero, 12(csp)
; ASM-OPT-NEXT:    li a0, 1234
; ASM-OPT-NEXT:    csw a0, 8(csp)
; ASM-OPT-NEXT:    cincoffset ca0, csp, 0
; ASM-OPT-NEXT:    csetbounds ca0, ca0, 16
; ASM-OPT-NEXT:    ccall use_alloca
; ASM-OPT-NEXT:    clc cra, 24(csp) # 8-byte Folded Reload
; ASM-OPT-NEXT:    cincoffset csp, csp, 32
; ASM-OPT-NEXT:  .LBB0_2: # %exit
; ASM-OPT-NEXT:    li a0, 123
; ASM-OPT-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@alloca_in_entry
; CHECK-SAME: (i1 [[ARG:%.*]]) local_unnamed_addr addrspace(200) #[[ATTR1:[0-9]+]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA:%.*]] = alloca [16 x i8], align 16, addrspace(200)
; CHECK-NEXT:    br i1 [[ARG]], label [[DO_ALLOCA:%.*]], label [[EXIT:%.*]]
; CHECK:       do_alloca:
; CHECK-NEXT:    br label [[USE_ALLOCA_NO_BOUNDS:%.*]]
; CHECK:       use_alloca_no_bounds:
; CHECK-NEXT:    [[PTR:%.*]] = bitcast [16 x i8] addrspace(200)* [[ALLOCA]] to i64 addrspace(200)*
; CHECK-NEXT:    [[PTR_PLUS_ONE:%.*]] = getelementptr i64, i64 addrspace(200)* [[PTR]], i64 1
; CHECK-NEXT:    store i64 1234, i64 addrspace(200)* [[PTR_PLUS_ONE]], align 8
; CHECK-NEXT:    br label [[USE_ALLOCA_NEED_BOUNDS:%.*]]
; CHECK:       use_alloca_need_bounds:
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast [16 x i8] addrspace(200)* [[ALLOCA]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i32(i8 addrspace(200)* [[TMP0]], i32 16)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to [16 x i8] addrspace(200)*
; CHECK-NEXT:    [[DOTSUB_LE:%.*]] = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* [[TMP2]], i64 0, i64 0
; CHECK-NEXT:    [[CALL:%.*]] = call signext i32 @use_alloca(i8 addrspace(200)* [[DOTSUB_LE]])
; CHECK-NEXT:    br label [[EXIT]]
; CHECK:       exit:
; CHECK-NEXT:    ret i32 123
;
entry:
  %alloca = alloca [16 x i8], align 16, addrspace(200)
  br i1 %arg, label %do_alloca, label %exit

do_alloca:
  br label %use_alloca_no_bounds

use_alloca_no_bounds:
  %ptr = bitcast [16 x i8] addrspace(200)* %alloca to i64 addrspace(200)*
  %ptr_plus_one = getelementptr i64, i64 addrspace(200)* %ptr, i64 1
  store i64 1234, i64 addrspace(200)* %ptr_plus_one, align 8
  br label %use_alloca_need_bounds

use_alloca_need_bounds:
  %.sub.le = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* %alloca, i64 0, i64 0
  %call = call signext i32 @use_alloca(i8 addrspace(200)* %.sub.le)
  br label %exit

exit:
  ret i32 123
}

define i32 @alloca_not_in_entry(i1 %arg) local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: alloca_not_in_entry:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    cincoffset csp, csp, -32
; ASM-NEXT:    csc cra, 24(csp) # 8-byte Folded Spill
; ASM-NEXT:    csc cs0, 16(csp) # 8-byte Folded Spill
; ASM-NEXT:    cincoffset cs0, csp, 32
; ASM-NEXT:    mv a1, a0
; ASM-NEXT:    andi a0, a1, 1
; ASM-NEXT:    li a1, 0
; ASM-NEXT:    beq a0, a1, .LBB1_4
; ASM-NEXT:    j .LBB1_1
; ASM-NEXT:  .LBB1_1: # %do_alloca
; ASM-NEXT:    cmove ca0, csp
; ASM-NEXT:    mv a1, a0
; ASM-NEXT:    addi a1, a1, -16
; ASM-NEXT:    csetaddr ca1, ca0, a1
; ASM-NEXT:    csetbounds ca0, ca1, 16
; ASM-NEXT:    csc ca0, -32(cs0) # 8-byte Folded Spill
; ASM-NEXT:    cmove csp, ca1
; ASM-NEXT:    csetbounds ca0, ca0, 16
; ASM-NEXT:    csc ca0, -24(cs0) # 8-byte Folded Spill
; ASM-NEXT:    j .LBB1_2
; ASM-NEXT:  .LBB1_2: # %use_alloca_no_bounds
; ASM-NEXT:    clc ca1, -32(cs0) # 8-byte Folded Reload
; ASM-NEXT:    li a0, 0
; ASM-NEXT:    csw a0, 12(ca1)
; ASM-NEXT:    li a0, 1234
; ASM-NEXT:    csw a0, 8(ca1)
; ASM-NEXT:    j .LBB1_3
; ASM-NEXT:  .LBB1_3: # %use_alloca_need_bounds
; ASM-NEXT:    clc ca0, -24(cs0) # 8-byte Folded Reload
; ASM-NEXT:    ccall use_alloca
; ASM-NEXT:    j .LBB1_4
; ASM-NEXT:  .LBB1_4: # %exit
; ASM-NEXT:    li a0, 123
; ASM-NEXT:    cincoffset csp, cs0, -32
; ASM-NEXT:    clc cra, 24(csp) # 8-byte Folded Reload
; ASM-NEXT:    clc cs0, 16(csp) # 8-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 32
; ASM-NEXT:    cret
;
; ASM-OPT-LABEL: alloca_not_in_entry:
; ASM-OPT:       # %bb.0: # %entry
; ASM-OPT-NEXT:    andi a0, a0, 1
; ASM-OPT-NEXT:    beqz a0, .LBB1_2
; ASM-OPT-NEXT:  # %bb.1: # %do_alloca
; ASM-OPT-NEXT:    cincoffset csp, csp, -16
; ASM-OPT-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; ASM-OPT-NEXT:    csc cs0, 0(csp) # 8-byte Folded Spill
; ASM-OPT-NEXT:    cincoffset cs0, csp, 16
; ASM-OPT-NEXT:    mv a0, sp
; ASM-OPT-NEXT:    addi a0, a0, -16
; ASM-OPT-NEXT:    csetaddr ca0, csp, a0
; ASM-OPT-NEXT:    csetbounds ca1, ca0, 16
; ASM-OPT-NEXT:    cmove csp, ca0
; ASM-OPT-NEXT:    csetbounds ca0, ca1, 16
; ASM-OPT-NEXT:    csw zero, 12(ca1)
; ASM-OPT-NEXT:    li a2, 1234
; ASM-OPT-NEXT:    csw a2, 8(ca1)
; ASM-OPT-NEXT:    ccall use_alloca
; ASM-OPT-NEXT:    cincoffset csp, cs0, -16
; ASM-OPT-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; ASM-OPT-NEXT:    clc cs0, 0(csp) # 8-byte Folded Reload
; ASM-OPT-NEXT:    cincoffset csp, csp, 16
; ASM-OPT-NEXT:  .LBB1_2: # %exit
; ASM-OPT-NEXT:    li a0, 123
; ASM-OPT-NEXT:    cret
; CHECK-LABEL: define {{[^@]+}}@alloca_not_in_entry
; CHECK-SAME: (i1 [[ARG:%.*]]) local_unnamed_addr addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 [[ARG]], label [[DO_ALLOCA:%.*]], label [[EXIT:%.*]]
; CHECK:       do_alloca:
; CHECK-NEXT:    [[ALLOCA:%.*]] = alloca [16 x i8], align 16, addrspace(200)
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast [16 x i8] addrspace(200)* [[ALLOCA]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.dynamic.i32(i8 addrspace(200)* [[TMP0]], i32 16)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to [16 x i8] addrspace(200)*
; CHECK-NEXT:    br label [[USE_ALLOCA_NO_BOUNDS:%.*]]
; CHECK:       use_alloca_no_bounds:
; CHECK-NEXT:    [[PTR:%.*]] = bitcast [16 x i8] addrspace(200)* [[ALLOCA]] to i64 addrspace(200)*
; CHECK-NEXT:    [[PTR_PLUS_ONE:%.*]] = getelementptr i64, i64 addrspace(200)* [[PTR]], i64 1
; CHECK-NEXT:    store i64 1234, i64 addrspace(200)* [[PTR_PLUS_ONE]], align 8
; CHECK-NEXT:    br label [[USE_ALLOCA_NEED_BOUNDS:%.*]]
; CHECK:       use_alloca_need_bounds:
; CHECK-NEXT:    [[DOTSUB_LE:%.*]] = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* [[TMP2]], i64 0, i64 0
; CHECK-NEXT:    [[CALL:%.*]] = call signext i32 @use_alloca(i8 addrspace(200)* [[DOTSUB_LE]])
; CHECK-NEXT:    br label [[EXIT]]
; CHECK:       exit:
; CHECK-NEXT:    ret i32 123
;
entry:
  br i1 %arg, label %do_alloca, label %exit

do_alloca:
  %alloca = alloca [16 x i8], align 16, addrspace(200)
  br label %use_alloca_no_bounds

use_alloca_no_bounds:
  %ptr = bitcast [16 x i8] addrspace(200)* %alloca to i64 addrspace(200)*
  %ptr_plus_one = getelementptr i64, i64 addrspace(200)* %ptr, i64 1
  store i64 1234, i64 addrspace(200)* %ptr_plus_one, align 8
  br label %use_alloca_need_bounds

use_alloca_need_bounds:
  %.sub.le = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* %alloca, i64 0, i64 0
  %call = call signext i32 @use_alloca(i8 addrspace(200)* %.sub.le)
  br label %exit

exit:
  ret i32 123
}

; The original reduced test case from libc/gen/exec.c
; We can't use llvm.cheri.bounded.stack.cap.i64 here, since that only works for static allocas:
define i32 @crash_reproducer(i1 %arg) local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: crash_reproducer:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    cincoffset csp, csp, -32
; ASM-NEXT:    csc cra, 24(csp) # 8-byte Folded Spill
; ASM-NEXT:    csc cs0, 16(csp) # 8-byte Folded Spill
; ASM-NEXT:    cincoffset cs0, csp, 32
; ASM-NEXT:    mv a1, a0
; ASM-NEXT:    andi a0, a1, 1
; ASM-NEXT:    li a1, 0
; ASM-NEXT:    beq a0, a1, .LBB2_2
; ASM-NEXT:    j .LBB2_1
; ASM-NEXT:  .LBB2_1: # %entry.while.end_crit_edge
; ASM-NEXT:  .LBB2_2: # %while.body
; ASM-NEXT:    cmove ca0, csp
; ASM-NEXT:    mv a1, a0
; ASM-NEXT:    addi a1, a1, -16
; ASM-NEXT:    csetaddr ca1, ca0, a1
; ASM-NEXT:    csetbounds ca0, ca1, 16
; ASM-NEXT:    cmove csp, ca1
; ASM-NEXT:    csetbounds ca0, ca0, 16
; ASM-NEXT:    csc ca0, -24(cs0) # 8-byte Folded Spill
; ASM-NEXT:    j .LBB2_3
; ASM-NEXT:  .LBB2_3: # %while.end.loopexit
; ASM-NEXT:    clc ca0, -24(cs0) # 8-byte Folded Reload
; ASM-NEXT:    csc ca0, -32(cs0) # 8-byte Folded Spill
; ASM-NEXT:    j .LBB2_4
; ASM-NEXT:  .LBB2_4: # %while.end
; ASM-NEXT:    clc ca0, -32(cs0) # 8-byte Folded Reload
; ASM-NEXT:    ccall use_alloca
; ASM-NEXT:    addi a0, a0, 1234
; ASM-NEXT:    cincoffset csp, cs0, -32
; ASM-NEXT:    clc cra, 24(csp) # 8-byte Folded Reload
; ASM-NEXT:    clc cs0, 16(csp) # 8-byte Folded Reload
; ASM-NEXT:    cincoffset csp, csp, 32
; ASM-NEXT:    cret
;
; ASM-OPT-LABEL: crash_reproducer:
; ASM-OPT:       # %bb.0: # %entry
; ASM-OPT-NEXT:    andi a0, a0, 1
; ASM-OPT-NEXT:    bnez a0, .LBB2_2
; ASM-OPT-NEXT:  # %bb.1: # %while.body
; ASM-OPT-NEXT:    cincoffset csp, csp, -16
; ASM-OPT-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; ASM-OPT-NEXT:    csc cs0, 0(csp) # 8-byte Folded Spill
; ASM-OPT-NEXT:    cincoffset cs0, csp, 16
; ASM-OPT-NEXT:    mv a0, sp
; ASM-OPT-NEXT:    addi a0, a0, -16
; ASM-OPT-NEXT:    csetaddr ca0, csp, a0
; ASM-OPT-NEXT:    csetbounds ca1, ca0, 16
; ASM-OPT-NEXT:    cmove csp, ca0
; ASM-OPT-NEXT:    csetbounds ca0, ca1, 16
; ASM-OPT-NEXT:    ccall use_alloca
; ASM-OPT-NEXT:    addi a0, a0, 1234
; ASM-OPT-NEXT:    cincoffset csp, cs0, -16
; ASM-OPT-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; ASM-OPT-NEXT:    clc cs0, 0(csp) # 8-byte Folded Reload
; ASM-OPT-NEXT:    cincoffset csp, csp, 16
; ASM-OPT-NEXT:    cret
; ASM-OPT-NEXT:  .LBB2_2: # %entry.while.end_crit_edge
; CHECK-LABEL: define {{[^@]+}}@crash_reproducer
; CHECK-SAME: (i1 [[ARG:%.*]]) local_unnamed_addr addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 [[ARG]], label [[ENTRY_WHILE_END_CRIT_EDGE:%.*]], label [[WHILE_BODY:%.*]]
; CHECK:       entry.while.end_crit_edge:
; CHECK-NEXT:    unreachable
; CHECK:       while.body:
; CHECK-NEXT:    [[TMP0:%.*]] = alloca [16 x i8], align 16, addrspace(200)
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast [16 x i8] addrspace(200)* [[TMP0]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP2:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.dynamic.i32(i8 addrspace(200)* [[TMP1]], i32 16)
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[TMP2]] to [16 x i8] addrspace(200)*
; CHECK-NEXT:    br label [[WHILE_END_LOOPEXIT:%.*]]
; CHECK:       while.end.loopexit:
; CHECK-NEXT:    [[DOTSUB_LE:%.*]] = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* [[TMP3]], i64 0, i64 0
; CHECK-NEXT:    br label [[WHILE_END:%.*]]
; CHECK:       while.end:
; CHECK-NEXT:    [[CALL:%.*]] = call signext i32 @use_alloca(i8 addrspace(200)* [[DOTSUB_LE]])
; CHECK-NEXT:    [[RESULT:%.*]] = add i32 [[CALL]], 1234
; CHECK-NEXT:    ret i32 [[RESULT]]
;
entry:
  br i1 %arg, label %entry.while.end_crit_edge, label %while.body

entry.while.end_crit_edge:
  unreachable

while.body:
  %0 = alloca [16 x i8], align 16, addrspace(200)
  br label %while.end.loopexit

while.end.loopexit:
  %.sub.le = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* %0, i64 0, i64 0
  br label %while.end

while.end:
  %call = call signext i32 @use_alloca(i8 addrspace(200)* %.sub.le)
  %result = add i32 %call, 1234
  ret i32 %result
}
