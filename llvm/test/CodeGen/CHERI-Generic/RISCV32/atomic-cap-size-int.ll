; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/atomic-cap-size-int.ll
;; Check that we can atomically update i128 (i64 for 32-bit systems)
;; For systems without double-width atomics (RISC-V, MIPS) we can use capability atomics
;; This is needed so we can report true for __atomic_always_lock_free(sizeof(uintptr_t), 0)
; RUN: opt -data-layout="e-m:e-pf200:64:64:64:32-p:32:32-i64:64-n32-S128-A200-P200-G200" -mtriple=riscv32 --relocation-model=pic -target-abi il32pc64f -mattr=+xcheri,+cap-mode,+f -atomic-expand -S -mattr=+a < %s | FileCheck %s --check-prefix=PURECAP-IR
; RUN: opt -data-layout="e-m:e-pf200:64:64:64:32-p:32:32-i64:64-n32-S128" -mtriple=riscv32 --relocation-model=pic -target-abi ilp32f -mattr=+xcheri,+f -atomic-expand -S -mattr=+a < %s | FileCheck %s --check-prefix=HYBRID-IR
; RUN: llc -mtriple=riscv32 --relocation-model=pic -target-abi il32pc64f -mattr=+xcheri,+cap-mode,+f -mattr=+a < %s | FileCheck %s --check-prefixes=PURECAP,PURECAP-ATOMICS --allow-unused-prefixes
; RUN: llc -mtriple=riscv32 --relocation-model=pic -target-abi il32pc64f -mattr=+xcheri,+cap-mode,+f -mattr=-a < %s | FileCheck %s --check-prefixes=PURECAP,PURECAP-LIBCALLS --allow-unused-prefixes
; RUN: sed 's/addrspace(200)/addrspace(0)/g' %s | llc -mtriple=riscv32 --relocation-model=pic -target-abi ilp32f -mattr=+xcheri,+f -mattr=+a | FileCheck %s --check-prefixes=HYBRID,HYBRID-ATOMICS --allow-unused-prefixes
; RUN: sed 's/addrspace(200)/addrspace(0)/g' %s | llc -mtriple=riscv32 --relocation-model=pic -target-abi ilp32f -mattr=+xcheri,+f -mattr=-a | FileCheck %s --check-prefixes=HYBRID,HYBRID-LIBCALLS --allow-unused-prefixes
; RUN: llc -mtriple=riscv32 --relocation-model=pic -target-abi ilp32f -mattr=+xcheri,+f -mattr=+a < %s | FileCheck %s --check-prefixes=HYBRID-CAP-PTR,HYBRID-CAP-PTR-ATOMICS --allow-unused-prefixes
; RUN: llc -mtriple=riscv32 --relocation-model=pic -target-abi ilp32f -mattr=+xcheri,+f -mattr=-a < %s | FileCheck %s --check-prefixes=HYBRID-CAP-PTR,HYBRID-CAP-PTR-LIBCALLS --allow-unused-prefixes

define i64 @store(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: store:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -32
; PURECAP-NEXT:    csc cra, 24(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 16(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    mv s0, a2
; PURECAP-NEXT:    mv s1, a1
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_store_8
; PURECAP-NEXT:    mv a0, s1
; PURECAP-NEXT:    mv a1, s0
; PURECAP-NEXT:    clc cra, 24(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 16(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 32
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: store:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s1, 4(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    mv s0, a2
; HYBRID-NEXT:    mv s1, a1
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_store_8@plt
; HYBRID-NEXT:    mv a0, s1
; HYBRID-NEXT:    mv a1, s0
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s1, 4(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: store:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s1, 4(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_store_8_c@plt
; HYBRID-CAP-PTR-NEXT:    mv a0, s1
; HYBRID-CAP-PTR-NEXT:    mv a1, s0
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s1, 4(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@store
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0:[0-9]+]] {
; PURECAP-IR-NEXT:    call void @__atomic_store_8(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i64 [[VAL]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@store
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0:[0-9]+]] {
; HYBRID-IR-NEXT:    call void @__atomic_store_8_c(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i64 [[VAL]]
;
  store atomic i64 %val, ptr addrspace(200) %ptr seq_cst, align 8
  ret i64 %val
}

define i64 @load(ptr addrspace(200) %ptr) nounwind {
; PURECAP-ATOMICS-LABEL: load:
; PURECAP-ATOMICS:       # %bb.0:
; PURECAP-ATOMICS-NEXT:    fence rw, rw
; PURECAP-ATOMICS-NEXT:    clc ca1, 0(ca0)
; PURECAP-ATOMICS-NEXT:    mv a0, a1
; PURECAP-ATOMICS-NEXT:    cgethigh a1, ca1
; PURECAP-ATOMICS-NEXT:    fence r, rw
; PURECAP-ATOMICS-NEXT:    cret
;
; PURECAP-LIBCALLS-LABEL: load:
; PURECAP-LIBCALLS:       # %bb.0:
; PURECAP-LIBCALLS-NEXT:    cincoffset csp, csp, -16
; PURECAP-LIBCALLS-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; PURECAP-LIBCALLS-NEXT:    li a1, 5
; PURECAP-LIBCALLS-NEXT:    ccall __atomic_load_8
; PURECAP-LIBCALLS-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; PURECAP-LIBCALLS-NEXT:    cincoffset csp, csp, 16
; PURECAP-LIBCALLS-NEXT:    cret
;
; HYBRID-ATOMICS-LABEL: load:
; HYBRID-ATOMICS:       # %bb.0:
; HYBRID-ATOMICS-NEXT:    fence rw, rw
; HYBRID-ATOMICS-NEXT:    lc ca1, 0(a0)
; HYBRID-ATOMICS-NEXT:    mv a0, a1
; HYBRID-ATOMICS-NEXT:    cgethigh a1, ca1
; HYBRID-ATOMICS-NEXT:    fence r, rw
; HYBRID-ATOMICS-NEXT:    ret
;
; HYBRID-LIBCALLS-LABEL: load:
; HYBRID-LIBCALLS:       # %bb.0:
; HYBRID-LIBCALLS-NEXT:    addi sp, sp, -16
; HYBRID-LIBCALLS-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-LIBCALLS-NEXT:    li a1, 5
; HYBRID-LIBCALLS-NEXT:    call __atomic_load_8@plt
; HYBRID-LIBCALLS-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-LIBCALLS-NEXT:    addi sp, sp, 16
; HYBRID-LIBCALLS-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: load:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a1, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_load_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@load
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    fence seq_cst
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = load atomic ptr addrspace(200), ptr addrspace(200) [[PTR]] monotonic, align 8
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = call i32 @llvm.cheri.cap.address.get.i32(ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = call i32 @llvm.cheri.cap.high.get.i32(ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = zext i32 [[TMP2]] to i64
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = zext i32 [[TMP3]] to i64
; PURECAP-IR-NEXT:    [[TMP6:%.*]] = shl i64 [[TMP5]], 32
; PURECAP-IR-NEXT:    [[TMP7:%.*]] = or i64 [[TMP4]], [[TMP6]]
; PURECAP-IR-NEXT:    fence acquire
; PURECAP-IR-NEXT:    ret i64 [[TMP7]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@load
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_load_8_c(ptr addrspace(200) [[PTR]], i32 5)
; HYBRID-IR-NEXT:    ret i64 [[TMP1]]
;
  %val = load atomic i64, ptr addrspace(200) %ptr seq_cst, align 8
  ret i64 %val
}

define i64 @atomic_xchg(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_xchg:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_exchange_8
; PURECAP-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_xchg:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_exchange_8@plt
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_xchg:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_exchange_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_xchg
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_exchange_8(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i64 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_xchg
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_exchange_8_c(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i64 [[TMP1]]
;
  %tmp = atomicrmw xchg ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_add(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_add:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_add_8
; PURECAP-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_add:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_add_8@plt
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_add:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_add_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_add
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_add_8(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i64 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_add
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_add_8_c(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i64 [[TMP1]]
;
  %tmp = atomicrmw add ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_sub(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_sub:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_sub_8
; PURECAP-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_sub:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_sub_8@plt
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_sub:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_sub_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_sub
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_sub_8(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i64 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_sub
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_sub_8_c(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i64 [[TMP1]]
;
  %tmp = atomicrmw sub ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_and(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_and:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_and_8
; PURECAP-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_and:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_and_8@plt
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_and:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_and_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_and
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_and_8(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i64 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_and
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_and_8_c(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i64 [[TMP1]]
;
  %tmp = atomicrmw and ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_nand(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_nand:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_nand_8
; PURECAP-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_nand:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_nand_8@plt
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_nand:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_nand_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_nand
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_nand_8(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i64 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_nand
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_nand_8_c(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i64 [[TMP1]]
;
  %tmp = atomicrmw nand ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_or(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_or:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_or_8
; PURECAP-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_or:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_or_8@plt
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_or:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_or_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_or
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_or_8(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i64 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_or
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_or_8_c(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i64 [[TMP1]]
;
  %tmp = atomicrmw or ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_xor(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_xor:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -16
; PURECAP-NEXT:    csc cra, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    li a3, 5
; PURECAP-NEXT:    ccall __atomic_fetch_xor_8
; PURECAP-NEXT:    clc cra, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 16
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_xor:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    li a3, 5
; HYBRID-NEXT:    call __atomic_fetch_xor_8@plt
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_xor:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    li a3, 5
; HYBRID-CAP-PTR-NEXT:    call __atomic_fetch_xor_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_xor
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_xor_8(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; PURECAP-IR-NEXT:    ret i64 [[TMP1]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_xor
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = call i64 @__atomic_fetch_xor_8_c(ptr addrspace(200) [[PTR]], i64 [[VAL]], i32 5)
; HYBRID-IR-NEXT:    ret i64 [[TMP1]]
;
  %tmp = atomicrmw xor ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_max(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_max:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -48
; PURECAP-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    cmove cs3, ca0
; PURECAP-NEXT:    clw a5, 4(ca0)
; PURECAP-NEXT:    clw a4, 0(ca0)
; PURECAP-NEXT:    mv s1, a2
; PURECAP-NEXT:    mv s2, a1
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds cs0, ca0, 8
; PURECAP-NEXT:    j .LBB9_2
; PURECAP-NEXT:  .LBB9_1: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    csw a4, 0(csp)
; PURECAP-NEXT:    csw a5, 4(csp)
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, cs3
; PURECAP-NEXT:    cmove ca1, cs0
; PURECAP-NEXT:    ccall __atomic_compare_exchange_8
; PURECAP-NEXT:    clw a5, 4(csp)
; PURECAP-NEXT:    clw a4, 0(csp)
; PURECAP-NEXT:    bnez a0, .LBB9_7
; PURECAP-NEXT:  .LBB9_2: # %atomicrmw.start
; PURECAP-NEXT:    # =>This Inner Loop Header: Depth=1
; PURECAP-NEXT:    beq a5, s1, .LBB9_4
; PURECAP-NEXT:  # %bb.3: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    slt a0, s1, a5
; PURECAP-NEXT:    j .LBB9_5
; PURECAP-NEXT:  .LBB9_4: # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    sltu a0, s2, a4
; PURECAP-NEXT:  .LBB9_5: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    mv a2, a4
; PURECAP-NEXT:    mv a3, a5
; PURECAP-NEXT:    bnez a0, .LBB9_1
; PURECAP-NEXT:  # %bb.6: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB9_2 Depth=1
; PURECAP-NEXT:    mv a2, s2
; PURECAP-NEXT:    mv a3, s1
; PURECAP-NEXT:    j .LBB9_1
; PURECAP-NEXT:  .LBB9_7: # %atomicrmw.end
; PURECAP-NEXT:    mv a0, a4
; PURECAP-NEXT:    mv a1, a5
; PURECAP-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 48
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_max:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -32
; HYBRID-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s2, 16(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    lw a5, 4(a0)
; HYBRID-NEXT:    lw a4, 0(a0)
; HYBRID-NEXT:    mv s1, a2
; HYBRID-NEXT:    mv s2, a1
; HYBRID-NEXT:    j .LBB9_2
; HYBRID-NEXT:  .LBB9_1: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    sw a4, 8(sp)
; HYBRID-NEXT:    sw a5, 12(sp)
; HYBRID-NEXT:    addi a1, sp, 8
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, s0
; HYBRID-NEXT:    call __atomic_compare_exchange_8@plt
; HYBRID-NEXT:    lw a5, 12(sp)
; HYBRID-NEXT:    lw a4, 8(sp)
; HYBRID-NEXT:    bnez a0, .LBB9_7
; HYBRID-NEXT:  .LBB9_2: # %atomicrmw.start
; HYBRID-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-NEXT:    beq a5, s1, .LBB9_4
; HYBRID-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    slt a0, s1, a5
; HYBRID-NEXT:    j .LBB9_5
; HYBRID-NEXT:  .LBB9_4: # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    sltu a0, s2, a4
; HYBRID-NEXT:  .LBB9_5: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    mv a2, a4
; HYBRID-NEXT:    mv a3, a5
; HYBRID-NEXT:    bnez a0, .LBB9_1
; HYBRID-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-NEXT:    mv a2, s2
; HYBRID-NEXT:    mv a3, s1
; HYBRID-NEXT:    j .LBB9_1
; HYBRID-NEXT:  .LBB9_7: # %atomicrmw.end
; HYBRID-NEXT:    mv a0, a4
; HYBRID-NEXT:    mv a1, a5
; HYBRID-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s2, 16(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 32
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_max:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -32
; HYBRID-CAP-PTR-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    lw.cap a4, (ca0)
; HYBRID-CAP-PTR-NEXT:    sc ca0, 0(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    cincoffset ca0, ca0, 4
; HYBRID-CAP-PTR-NEXT:    lw.cap a5, (ca0)
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    j .LBB9_2
; HYBRID-CAP-PTR-NEXT:  .LBB9_1: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sw a4, 8(sp)
; HYBRID-CAP-PTR-NEXT:    sw a5, 12(sp)
; HYBRID-CAP-PTR-NEXT:    addi a1, sp, 8
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    lc ca0, 0(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw a5, 12(sp)
; HYBRID-CAP-PTR-NEXT:    lw a4, 8(sp)
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB9_7
; HYBRID-CAP-PTR-NEXT:  .LBB9_2: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-CAP-PTR-NEXT:    beq a5, s0, .LBB9_4
; HYBRID-CAP-PTR-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    slt a0, s0, a5
; HYBRID-CAP-PTR-NEXT:    j .LBB9_5
; HYBRID-CAP-PTR-NEXT:  .LBB9_4: # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s1, a4
; HYBRID-CAP-PTR-NEXT:  .LBB9_5: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, a4
; HYBRID-CAP-PTR-NEXT:    mv a3, a5
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB9_1
; HYBRID-CAP-PTR-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB9_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, s1
; HYBRID-CAP-PTR-NEXT:    mv a3, s0
; HYBRID-CAP-PTR-NEXT:    j .LBB9_1
; HYBRID-CAP-PTR-NEXT:  .LBB9_7: # %atomicrmw.end
; HYBRID-CAP-PTR-NEXT:    mv a0, a4
; HYBRID-CAP-PTR-NEXT:    mv a1, a5
; HYBRID-CAP-PTR-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 32
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_max
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8, addrspace(200)
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = load i64, ptr addrspace(200) [[PTR]], align 8
; PURECAP-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; PURECAP-IR:       atomicrmw.start:
; PURECAP-IR-NEXT:    [[LOADED:%.*]] = phi i64 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = icmp sgt i64 [[LOADED]], [[VAL]]
; PURECAP-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i64 [[LOADED]], i64 [[VAL]]
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i64 [[LOADED]], ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_8(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = load i64, ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP5]], 0
; PURECAP-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i64, i1 } [[TMP6]], i1 [[TMP4]], 1
; PURECAP-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i64, i1 } [[TMP7]], 1
; PURECAP-IR-NEXT:    [[NEWLOADED]] = extractvalue { i64, i1 } [[TMP7]], 0
; PURECAP-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; PURECAP-IR:       atomicrmw.end:
; PURECAP-IR-NEXT:    ret i64 [[NEWLOADED]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_max
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = load i64, ptr addrspace(200) [[PTR]], align 8
; HYBRID-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; HYBRID-IR:       atomicrmw.start:
; HYBRID-IR-NEXT:    [[LOADED:%.*]] = phi i64 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = icmp sgt i64 [[LOADED]], [[VAL]]
; HYBRID-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i64 [[LOADED]], i64 [[VAL]]
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i64 [[LOADED]], ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_8_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = load i64, ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP5]], 0
; HYBRID-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i64, i1 } [[TMP6]], i1 [[TMP4]], 1
; HYBRID-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i64, i1 } [[TMP7]], 1
; HYBRID-IR-NEXT:    [[NEWLOADED]] = extractvalue { i64, i1 } [[TMP7]], 0
; HYBRID-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; HYBRID-IR:       atomicrmw.end:
; HYBRID-IR-NEXT:    ret i64 [[NEWLOADED]]
;
  %tmp = atomicrmw max ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_min(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_min:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -48
; PURECAP-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    cmove cs3, ca0
; PURECAP-NEXT:    clw a5, 4(ca0)
; PURECAP-NEXT:    clw a4, 0(ca0)
; PURECAP-NEXT:    mv s1, a2
; PURECAP-NEXT:    mv s2, a1
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds cs0, ca0, 8
; PURECAP-NEXT:    j .LBB10_2
; PURECAP-NEXT:  .LBB10_1: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    csw a4, 0(csp)
; PURECAP-NEXT:    csw a5, 4(csp)
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, cs3
; PURECAP-NEXT:    cmove ca1, cs0
; PURECAP-NEXT:    ccall __atomic_compare_exchange_8
; PURECAP-NEXT:    clw a5, 4(csp)
; PURECAP-NEXT:    clw a4, 0(csp)
; PURECAP-NEXT:    bnez a0, .LBB10_7
; PURECAP-NEXT:  .LBB10_2: # %atomicrmw.start
; PURECAP-NEXT:    # =>This Inner Loop Header: Depth=1
; PURECAP-NEXT:    beq a5, s1, .LBB10_4
; PURECAP-NEXT:  # %bb.3: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    slt a0, s1, a5
; PURECAP-NEXT:    j .LBB10_5
; PURECAP-NEXT:  .LBB10_4: # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    sltu a0, s2, a4
; PURECAP-NEXT:  .LBB10_5: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    xori a0, a0, 1
; PURECAP-NEXT:    mv a2, a4
; PURECAP-NEXT:    mv a3, a5
; PURECAP-NEXT:    bnez a0, .LBB10_1
; PURECAP-NEXT:  # %bb.6: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB10_2 Depth=1
; PURECAP-NEXT:    mv a2, s2
; PURECAP-NEXT:    mv a3, s1
; PURECAP-NEXT:    j .LBB10_1
; PURECAP-NEXT:  .LBB10_7: # %atomicrmw.end
; PURECAP-NEXT:    mv a0, a4
; PURECAP-NEXT:    mv a1, a5
; PURECAP-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 48
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_min:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -32
; HYBRID-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s2, 16(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    lw a5, 4(a0)
; HYBRID-NEXT:    lw a4, 0(a0)
; HYBRID-NEXT:    mv s1, a2
; HYBRID-NEXT:    mv s2, a1
; HYBRID-NEXT:    j .LBB10_2
; HYBRID-NEXT:  .LBB10_1: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    sw a4, 8(sp)
; HYBRID-NEXT:    sw a5, 12(sp)
; HYBRID-NEXT:    addi a1, sp, 8
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, s0
; HYBRID-NEXT:    call __atomic_compare_exchange_8@plt
; HYBRID-NEXT:    lw a5, 12(sp)
; HYBRID-NEXT:    lw a4, 8(sp)
; HYBRID-NEXT:    bnez a0, .LBB10_7
; HYBRID-NEXT:  .LBB10_2: # %atomicrmw.start
; HYBRID-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-NEXT:    beq a5, s1, .LBB10_4
; HYBRID-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    slt a0, s1, a5
; HYBRID-NEXT:    j .LBB10_5
; HYBRID-NEXT:  .LBB10_4: # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    sltu a0, s2, a4
; HYBRID-NEXT:  .LBB10_5: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    xori a0, a0, 1
; HYBRID-NEXT:    mv a2, a4
; HYBRID-NEXT:    mv a3, a5
; HYBRID-NEXT:    bnez a0, .LBB10_1
; HYBRID-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-NEXT:    mv a2, s2
; HYBRID-NEXT:    mv a3, s1
; HYBRID-NEXT:    j .LBB10_1
; HYBRID-NEXT:  .LBB10_7: # %atomicrmw.end
; HYBRID-NEXT:    mv a0, a4
; HYBRID-NEXT:    mv a1, a5
; HYBRID-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s2, 16(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 32
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_min:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -32
; HYBRID-CAP-PTR-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    lw.cap a4, (ca0)
; HYBRID-CAP-PTR-NEXT:    sc ca0, 0(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    cincoffset ca0, ca0, 4
; HYBRID-CAP-PTR-NEXT:    lw.cap a5, (ca0)
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    j .LBB10_2
; HYBRID-CAP-PTR-NEXT:  .LBB10_1: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sw a4, 8(sp)
; HYBRID-CAP-PTR-NEXT:    sw a5, 12(sp)
; HYBRID-CAP-PTR-NEXT:    addi a1, sp, 8
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    lc ca0, 0(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw a5, 12(sp)
; HYBRID-CAP-PTR-NEXT:    lw a4, 8(sp)
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB10_7
; HYBRID-CAP-PTR-NEXT:  .LBB10_2: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-CAP-PTR-NEXT:    beq a5, s0, .LBB10_4
; HYBRID-CAP-PTR-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    slt a0, s0, a5
; HYBRID-CAP-PTR-NEXT:    j .LBB10_5
; HYBRID-CAP-PTR-NEXT:  .LBB10_4: # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s1, a4
; HYBRID-CAP-PTR-NEXT:  .LBB10_5: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    xori a0, a0, 1
; HYBRID-CAP-PTR-NEXT:    mv a2, a4
; HYBRID-CAP-PTR-NEXT:    mv a3, a5
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB10_1
; HYBRID-CAP-PTR-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB10_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, s1
; HYBRID-CAP-PTR-NEXT:    mv a3, s0
; HYBRID-CAP-PTR-NEXT:    j .LBB10_1
; HYBRID-CAP-PTR-NEXT:  .LBB10_7: # %atomicrmw.end
; HYBRID-CAP-PTR-NEXT:    mv a0, a4
; HYBRID-CAP-PTR-NEXT:    mv a1, a5
; HYBRID-CAP-PTR-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 32
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_min
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8, addrspace(200)
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = load i64, ptr addrspace(200) [[PTR]], align 8
; PURECAP-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; PURECAP-IR:       atomicrmw.start:
; PURECAP-IR-NEXT:    [[LOADED:%.*]] = phi i64 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = icmp sle i64 [[LOADED]], [[VAL]]
; PURECAP-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i64 [[LOADED]], i64 [[VAL]]
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i64 [[LOADED]], ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_8(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = load i64, ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP5]], 0
; PURECAP-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i64, i1 } [[TMP6]], i1 [[TMP4]], 1
; PURECAP-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i64, i1 } [[TMP7]], 1
; PURECAP-IR-NEXT:    [[NEWLOADED]] = extractvalue { i64, i1 } [[TMP7]], 0
; PURECAP-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; PURECAP-IR:       atomicrmw.end:
; PURECAP-IR-NEXT:    ret i64 [[NEWLOADED]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_min
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = load i64, ptr addrspace(200) [[PTR]], align 8
; HYBRID-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; HYBRID-IR:       atomicrmw.start:
; HYBRID-IR-NEXT:    [[LOADED:%.*]] = phi i64 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = icmp sle i64 [[LOADED]], [[VAL]]
; HYBRID-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i64 [[LOADED]], i64 [[VAL]]
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i64 [[LOADED]], ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_8_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = load i64, ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP5]], 0
; HYBRID-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i64, i1 } [[TMP6]], i1 [[TMP4]], 1
; HYBRID-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i64, i1 } [[TMP7]], 1
; HYBRID-IR-NEXT:    [[NEWLOADED]] = extractvalue { i64, i1 } [[TMP7]], 0
; HYBRID-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; HYBRID-IR:       atomicrmw.end:
; HYBRID-IR-NEXT:    ret i64 [[NEWLOADED]]
;
  %tmp = atomicrmw min ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_umax(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_umax:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -48
; PURECAP-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    cmove cs3, ca0
; PURECAP-NEXT:    clw a5, 4(ca0)
; PURECAP-NEXT:    clw a4, 0(ca0)
; PURECAP-NEXT:    mv s1, a2
; PURECAP-NEXT:    mv s2, a1
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds cs0, ca0, 8
; PURECAP-NEXT:    j .LBB11_2
; PURECAP-NEXT:  .LBB11_1: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    csw a4, 0(csp)
; PURECAP-NEXT:    csw a5, 4(csp)
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, cs3
; PURECAP-NEXT:    cmove ca1, cs0
; PURECAP-NEXT:    ccall __atomic_compare_exchange_8
; PURECAP-NEXT:    clw a5, 4(csp)
; PURECAP-NEXT:    clw a4, 0(csp)
; PURECAP-NEXT:    bnez a0, .LBB11_7
; PURECAP-NEXT:  .LBB11_2: # %atomicrmw.start
; PURECAP-NEXT:    # =>This Inner Loop Header: Depth=1
; PURECAP-NEXT:    beq a5, s1, .LBB11_4
; PURECAP-NEXT:  # %bb.3: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    sltu a0, s1, a5
; PURECAP-NEXT:    j .LBB11_5
; PURECAP-NEXT:  .LBB11_4: # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    sltu a0, s2, a4
; PURECAP-NEXT:  .LBB11_5: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    mv a2, a4
; PURECAP-NEXT:    mv a3, a5
; PURECAP-NEXT:    bnez a0, .LBB11_1
; PURECAP-NEXT:  # %bb.6: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB11_2 Depth=1
; PURECAP-NEXT:    mv a2, s2
; PURECAP-NEXT:    mv a3, s1
; PURECAP-NEXT:    j .LBB11_1
; PURECAP-NEXT:  .LBB11_7: # %atomicrmw.end
; PURECAP-NEXT:    mv a0, a4
; PURECAP-NEXT:    mv a1, a5
; PURECAP-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 48
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_umax:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -32
; HYBRID-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s2, 16(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    lw a5, 4(a0)
; HYBRID-NEXT:    lw a4, 0(a0)
; HYBRID-NEXT:    mv s1, a2
; HYBRID-NEXT:    mv s2, a1
; HYBRID-NEXT:    j .LBB11_2
; HYBRID-NEXT:  .LBB11_1: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    sw a4, 8(sp)
; HYBRID-NEXT:    sw a5, 12(sp)
; HYBRID-NEXT:    addi a1, sp, 8
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, s0
; HYBRID-NEXT:    call __atomic_compare_exchange_8@plt
; HYBRID-NEXT:    lw a5, 12(sp)
; HYBRID-NEXT:    lw a4, 8(sp)
; HYBRID-NEXT:    bnez a0, .LBB11_7
; HYBRID-NEXT:  .LBB11_2: # %atomicrmw.start
; HYBRID-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-NEXT:    beq a5, s1, .LBB11_4
; HYBRID-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    sltu a0, s1, a5
; HYBRID-NEXT:    j .LBB11_5
; HYBRID-NEXT:  .LBB11_4: # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    sltu a0, s2, a4
; HYBRID-NEXT:  .LBB11_5: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    mv a2, a4
; HYBRID-NEXT:    mv a3, a5
; HYBRID-NEXT:    bnez a0, .LBB11_1
; HYBRID-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-NEXT:    mv a2, s2
; HYBRID-NEXT:    mv a3, s1
; HYBRID-NEXT:    j .LBB11_1
; HYBRID-NEXT:  .LBB11_7: # %atomicrmw.end
; HYBRID-NEXT:    mv a0, a4
; HYBRID-NEXT:    mv a1, a5
; HYBRID-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s2, 16(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 32
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_umax:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -32
; HYBRID-CAP-PTR-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    lw.cap a4, (ca0)
; HYBRID-CAP-PTR-NEXT:    sc ca0, 0(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    cincoffset ca0, ca0, 4
; HYBRID-CAP-PTR-NEXT:    lw.cap a5, (ca0)
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    j .LBB11_2
; HYBRID-CAP-PTR-NEXT:  .LBB11_1: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sw a4, 8(sp)
; HYBRID-CAP-PTR-NEXT:    sw a5, 12(sp)
; HYBRID-CAP-PTR-NEXT:    addi a1, sp, 8
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    lc ca0, 0(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw a5, 12(sp)
; HYBRID-CAP-PTR-NEXT:    lw a4, 8(sp)
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB11_7
; HYBRID-CAP-PTR-NEXT:  .LBB11_2: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-CAP-PTR-NEXT:    beq a5, s0, .LBB11_4
; HYBRID-CAP-PTR-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s0, a5
; HYBRID-CAP-PTR-NEXT:    j .LBB11_5
; HYBRID-CAP-PTR-NEXT:  .LBB11_4: # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s1, a4
; HYBRID-CAP-PTR-NEXT:  .LBB11_5: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, a4
; HYBRID-CAP-PTR-NEXT:    mv a3, a5
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB11_1
; HYBRID-CAP-PTR-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB11_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, s1
; HYBRID-CAP-PTR-NEXT:    mv a3, s0
; HYBRID-CAP-PTR-NEXT:    j .LBB11_1
; HYBRID-CAP-PTR-NEXT:  .LBB11_7: # %atomicrmw.end
; HYBRID-CAP-PTR-NEXT:    mv a0, a4
; HYBRID-CAP-PTR-NEXT:    mv a1, a5
; HYBRID-CAP-PTR-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 32
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_umax
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8, addrspace(200)
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = load i64, ptr addrspace(200) [[PTR]], align 8
; PURECAP-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; PURECAP-IR:       atomicrmw.start:
; PURECAP-IR-NEXT:    [[LOADED:%.*]] = phi i64 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = icmp ugt i64 [[LOADED]], [[VAL]]
; PURECAP-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i64 [[LOADED]], i64 [[VAL]]
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i64 [[LOADED]], ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_8(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = load i64, ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP5]], 0
; PURECAP-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i64, i1 } [[TMP6]], i1 [[TMP4]], 1
; PURECAP-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i64, i1 } [[TMP7]], 1
; PURECAP-IR-NEXT:    [[NEWLOADED]] = extractvalue { i64, i1 } [[TMP7]], 0
; PURECAP-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; PURECAP-IR:       atomicrmw.end:
; PURECAP-IR-NEXT:    ret i64 [[NEWLOADED]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_umax
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = load i64, ptr addrspace(200) [[PTR]], align 8
; HYBRID-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; HYBRID-IR:       atomicrmw.start:
; HYBRID-IR-NEXT:    [[LOADED:%.*]] = phi i64 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = icmp ugt i64 [[LOADED]], [[VAL]]
; HYBRID-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i64 [[LOADED]], i64 [[VAL]]
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i64 [[LOADED]], ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_8_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = load i64, ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP5]], 0
; HYBRID-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i64, i1 } [[TMP6]], i1 [[TMP4]], 1
; HYBRID-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i64, i1 } [[TMP7]], 1
; HYBRID-IR-NEXT:    [[NEWLOADED]] = extractvalue { i64, i1 } [[TMP7]], 0
; HYBRID-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; HYBRID-IR:       atomicrmw.end:
; HYBRID-IR-NEXT:    ret i64 [[NEWLOADED]]
;
  %tmp = atomicrmw umax ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define i64 @atomic_umin(ptr addrspace(200) %ptr, i64 %val) nounwind {
; PURECAP-LABEL: atomic_umin:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -48
; PURECAP-NEXT:    csc cra, 40(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 32(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs1, 24(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs2, 16(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs3, 8(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    cmove cs3, ca0
; PURECAP-NEXT:    clw a5, 4(ca0)
; PURECAP-NEXT:    clw a4, 0(ca0)
; PURECAP-NEXT:    mv s1, a2
; PURECAP-NEXT:    mv s2, a1
; PURECAP-NEXT:    cincoffset ca0, csp, 0
; PURECAP-NEXT:    csetbounds cs0, ca0, 8
; PURECAP-NEXT:    j .LBB12_2
; PURECAP-NEXT:  .LBB12_1: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    csw a4, 0(csp)
; PURECAP-NEXT:    csw a5, 4(csp)
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, cs3
; PURECAP-NEXT:    cmove ca1, cs0
; PURECAP-NEXT:    ccall __atomic_compare_exchange_8
; PURECAP-NEXT:    clw a5, 4(csp)
; PURECAP-NEXT:    clw a4, 0(csp)
; PURECAP-NEXT:    bnez a0, .LBB12_7
; PURECAP-NEXT:  .LBB12_2: # %atomicrmw.start
; PURECAP-NEXT:    # =>This Inner Loop Header: Depth=1
; PURECAP-NEXT:    beq a5, s1, .LBB12_4
; PURECAP-NEXT:  # %bb.3: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    sltu a0, s1, a5
; PURECAP-NEXT:    j .LBB12_5
; PURECAP-NEXT:  .LBB12_4: # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    sltu a0, s2, a4
; PURECAP-NEXT:  .LBB12_5: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    xori a0, a0, 1
; PURECAP-NEXT:    mv a2, a4
; PURECAP-NEXT:    mv a3, a5
; PURECAP-NEXT:    bnez a0, .LBB12_1
; PURECAP-NEXT:  # %bb.6: # %atomicrmw.start
; PURECAP-NEXT:    # in Loop: Header=BB12_2 Depth=1
; PURECAP-NEXT:    mv a2, s2
; PURECAP-NEXT:    mv a3, s1
; PURECAP-NEXT:    j .LBB12_1
; PURECAP-NEXT:  .LBB12_7: # %atomicrmw.end
; PURECAP-NEXT:    mv a0, a4
; PURECAP-NEXT:    mv a1, a5
; PURECAP-NEXT:    clc cra, 40(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 32(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs1, 24(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs2, 16(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs3, 8(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 48
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: atomic_umin:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -32
; HYBRID-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s2, 16(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    lw a5, 4(a0)
; HYBRID-NEXT:    lw a4, 0(a0)
; HYBRID-NEXT:    mv s1, a2
; HYBRID-NEXT:    mv s2, a1
; HYBRID-NEXT:    j .LBB12_2
; HYBRID-NEXT:  .LBB12_1: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    sw a4, 8(sp)
; HYBRID-NEXT:    sw a5, 12(sp)
; HYBRID-NEXT:    addi a1, sp, 8
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, s0
; HYBRID-NEXT:    call __atomic_compare_exchange_8@plt
; HYBRID-NEXT:    lw a5, 12(sp)
; HYBRID-NEXT:    lw a4, 8(sp)
; HYBRID-NEXT:    bnez a0, .LBB12_7
; HYBRID-NEXT:  .LBB12_2: # %atomicrmw.start
; HYBRID-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-NEXT:    beq a5, s1, .LBB12_4
; HYBRID-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    sltu a0, s1, a5
; HYBRID-NEXT:    j .LBB12_5
; HYBRID-NEXT:  .LBB12_4: # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    sltu a0, s2, a4
; HYBRID-NEXT:  .LBB12_5: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    xori a0, a0, 1
; HYBRID-NEXT:    mv a2, a4
; HYBRID-NEXT:    mv a3, a5
; HYBRID-NEXT:    bnez a0, .LBB12_1
; HYBRID-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-NEXT:    mv a2, s2
; HYBRID-NEXT:    mv a3, s1
; HYBRID-NEXT:    j .LBB12_1
; HYBRID-NEXT:  .LBB12_7: # %atomicrmw.end
; HYBRID-NEXT:    mv a0, a4
; HYBRID-NEXT:    mv a1, a5
; HYBRID-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s2, 16(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 32
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: atomic_umin:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -32
; HYBRID-CAP-PTR-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    lw.cap a4, (ca0)
; HYBRID-CAP-PTR-NEXT:    sc ca0, 0(sp) # 8-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    cincoffset ca0, ca0, 4
; HYBRID-CAP-PTR-NEXT:    lw.cap a5, (ca0)
; HYBRID-CAP-PTR-NEXT:    mv s0, a2
; HYBRID-CAP-PTR-NEXT:    mv s1, a1
; HYBRID-CAP-PTR-NEXT:    j .LBB12_2
; HYBRID-CAP-PTR-NEXT:  .LBB12_1: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sw a4, 8(sp)
; HYBRID-CAP-PTR-NEXT:    sw a5, 12(sp)
; HYBRID-CAP-PTR-NEXT:    addi a1, sp, 8
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    lc ca0, 0(sp) # 8-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw a5, 12(sp)
; HYBRID-CAP-PTR-NEXT:    lw a4, 8(sp)
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB12_7
; HYBRID-CAP-PTR-NEXT:  .LBB12_2: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # =>This Inner Loop Header: Depth=1
; HYBRID-CAP-PTR-NEXT:    beq a5, s0, .LBB12_4
; HYBRID-CAP-PTR-NEXT:  # %bb.3: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s0, a5
; HYBRID-CAP-PTR-NEXT:    j .LBB12_5
; HYBRID-CAP-PTR-NEXT:  .LBB12_4: # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    sltu a0, s1, a4
; HYBRID-CAP-PTR-NEXT:  .LBB12_5: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    xori a0, a0, 1
; HYBRID-CAP-PTR-NEXT:    mv a2, a4
; HYBRID-CAP-PTR-NEXT:    mv a3, a5
; HYBRID-CAP-PTR-NEXT:    bnez a0, .LBB12_1
; HYBRID-CAP-PTR-NEXT:  # %bb.6: # %atomicrmw.start
; HYBRID-CAP-PTR-NEXT:    # in Loop: Header=BB12_2 Depth=1
; HYBRID-CAP-PTR-NEXT:    mv a2, s1
; HYBRID-CAP-PTR-NEXT:    mv a3, s0
; HYBRID-CAP-PTR-NEXT:    j .LBB12_1
; HYBRID-CAP-PTR-NEXT:  .LBB12_7: # %atomicrmw.end
; HYBRID-CAP-PTR-NEXT:    mv a0, a4
; HYBRID-CAP-PTR-NEXT:    mv a1, a5
; HYBRID-CAP-PTR-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 32
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@atomic_umin
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8, addrspace(200)
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = load i64, ptr addrspace(200) [[PTR]], align 8
; PURECAP-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; PURECAP-IR:       atomicrmw.start:
; PURECAP-IR-NEXT:    [[LOADED:%.*]] = phi i64 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = icmp ule i64 [[LOADED]], [[VAL]]
; PURECAP-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i64 [[LOADED]], i64 [[VAL]]
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i64 [[LOADED]], ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_8(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = load i64, ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP5]], 0
; PURECAP-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i64, i1 } [[TMP6]], i1 [[TMP4]], 1
; PURECAP-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i64, i1 } [[TMP7]], 1
; PURECAP-IR-NEXT:    [[NEWLOADED]] = extractvalue { i64, i1 } [[TMP7]], 0
; PURECAP-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; PURECAP-IR:       atomicrmw.end:
; PURECAP-IR-NEXT:    ret i64 [[NEWLOADED]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@atomic_umin
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[VAL:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = load i64, ptr addrspace(200) [[PTR]], align 8
; HYBRID-IR-NEXT:    br label [[ATOMICRMW_START:%.*]]
; HYBRID-IR:       atomicrmw.start:
; HYBRID-IR-NEXT:    [[LOADED:%.*]] = phi i64 [ [[TMP2]], [[TMP0:%.*]] ], [ [[NEWLOADED:%.*]], [[ATOMICRMW_START]] ]
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = icmp ule i64 [[LOADED]], [[VAL]]
; HYBRID-IR-NEXT:    [[NEW:%.*]] = select i1 [[TMP3]], i64 [[LOADED]], i64 [[VAL]]
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i64 [[LOADED]], ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = call zeroext i1 @__atomic_compare_exchange_8_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = load i64, ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP6:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP5]], 0
; HYBRID-IR-NEXT:    [[TMP7:%.*]] = insertvalue { i64, i1 } [[TMP6]], i1 [[TMP4]], 1
; HYBRID-IR-NEXT:    [[SUCCESS:%.*]] = extractvalue { i64, i1 } [[TMP7]], 1
; HYBRID-IR-NEXT:    [[NEWLOADED]] = extractvalue { i64, i1 } [[TMP7]], 0
; HYBRID-IR-NEXT:    br i1 [[SUCCESS]], label [[ATOMICRMW_END:%.*]], label [[ATOMICRMW_START]]
; HYBRID-IR:       atomicrmw.end:
; HYBRID-IR-NEXT:    ret i64 [[NEWLOADED]]
;
  %tmp = atomicrmw umin ptr addrspace(200) %ptr, i64 %val seq_cst
  ret i64 %tmp
}

define { i64, i1 } @cmpxchg_weak(ptr addrspace(200) %ptr, i64 %exp, i64 %new) nounwind {
; PURECAP-LABEL: cmpxchg_weak:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -32
; PURECAP-NEXT:    csc cra, 24(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 16(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    mv a6, a5
; PURECAP-NEXT:    mv a7, a4
; PURECAP-NEXT:    cmove ct0, ca1
; PURECAP-NEXT:    cmove cs0, ca0
; PURECAP-NEXT:    csw a3, 12(csp)
; PURECAP-NEXT:    csw a2, 8(csp)
; PURECAP-NEXT:    cincoffset ca0, csp, 8
; PURECAP-NEXT:    csetbounds ca1, ca0, 8
; PURECAP-NEXT:    li a4, 4
; PURECAP-NEXT:    li a5, 2
; PURECAP-NEXT:    cmove ca0, ct0
; PURECAP-NEXT:    mv a2, a7
; PURECAP-NEXT:    mv a3, a6
; PURECAP-NEXT:    ccall __atomic_compare_exchange_8
; PURECAP-NEXT:    clw a1, 12(csp)
; PURECAP-NEXT:    clw a2, 8(csp)
; PURECAP-NEXT:    csw a1, 4(cs0)
; PURECAP-NEXT:    csw a2, 0(cs0)
; PURECAP-NEXT:    csb a0, 8(cs0)
; PURECAP-NEXT:    clc cra, 24(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 16(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 32
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: cmpxchg_weak:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    mv a6, a5
; HYBRID-NEXT:    mv a7, a4
; HYBRID-NEXT:    mv t0, a1
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    sw a3, 4(sp)
; HYBRID-NEXT:    sw a2, 0(sp)
; HYBRID-NEXT:    mv a1, sp
; HYBRID-NEXT:    li a4, 4
; HYBRID-NEXT:    li a5, 2
; HYBRID-NEXT:    mv a0, t0
; HYBRID-NEXT:    mv a2, a7
; HYBRID-NEXT:    mv a3, a6
; HYBRID-NEXT:    call __atomic_compare_exchange_8@plt
; HYBRID-NEXT:    lw a1, 4(sp)
; HYBRID-NEXT:    lw a2, 0(sp)
; HYBRID-NEXT:    sw a1, 4(s0)
; HYBRID-NEXT:    sw a2, 0(s0)
; HYBRID-NEXT:    sb a0, 8(s0)
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: cmpxchg_weak:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    mv a6, a5
; HYBRID-CAP-PTR-NEXT:    mv a7, a4
; HYBRID-CAP-PTR-NEXT:    cmove ct0, ca1
; HYBRID-CAP-PTR-NEXT:    mv s0, a0
; HYBRID-CAP-PTR-NEXT:    sw a3, 4(sp)
; HYBRID-CAP-PTR-NEXT:    sw a2, 0(sp)
; HYBRID-CAP-PTR-NEXT:    mv a1, sp
; HYBRID-CAP-PTR-NEXT:    li a4, 4
; HYBRID-CAP-PTR-NEXT:    li a5, 2
; HYBRID-CAP-PTR-NEXT:    cmove ca0, ct0
; HYBRID-CAP-PTR-NEXT:    mv a2, a7
; HYBRID-CAP-PTR-NEXT:    mv a3, a6
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw a1, 4(sp)
; HYBRID-CAP-PTR-NEXT:    lw a2, 0(sp)
; HYBRID-CAP-PTR-NEXT:    sw a1, 4(s0)
; HYBRID-CAP-PTR-NEXT:    sw a2, 0(s0)
; HYBRID-CAP-PTR-NEXT:    sb a0, 8(s0)
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@cmpxchg_weak
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[EXP:%.*]], i64 [[NEW:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8, addrspace(200)
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i64 [[EXP]], ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = call zeroext i1 @__atomic_compare_exchange_8(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i64 [[NEW]], i32 4, i32 2)
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = load i64, ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP3]], 0
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = insertvalue { i64, i1 } [[TMP4]], i1 [[TMP2]], 1
; PURECAP-IR-NEXT:    ret { i64, i1 } [[TMP5]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@cmpxchg_weak
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[EXP:%.*]], i64 [[NEW:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i64 [[EXP]], ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = call zeroext i1 @__atomic_compare_exchange_8_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i64 [[NEW]], i32 4, i32 2)
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = load i64, ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP3]], 0
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = insertvalue { i64, i1 } [[TMP4]], i1 [[TMP2]], 1
; HYBRID-IR-NEXT:    ret { i64, i1 } [[TMP5]]
;
  %1 = cmpxchg weak ptr addrspace(200) %ptr, i64 %exp, i64 %new acq_rel acquire
  ret { i64, i1 } %1
}

define { i64, i1 } @cmpxchg_strong(ptr addrspace(200) %ptr, i64 %exp, i64 %new) nounwind {
; PURECAP-LABEL: cmpxchg_strong:
; PURECAP:       # %bb.0:
; PURECAP-NEXT:    cincoffset csp, csp, -32
; PURECAP-NEXT:    csc cra, 24(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    csc cs0, 16(csp) # 8-byte Folded Spill
; PURECAP-NEXT:    mv a6, a5
; PURECAP-NEXT:    mv a7, a4
; PURECAP-NEXT:    cmove ct0, ca1
; PURECAP-NEXT:    cmove cs0, ca0
; PURECAP-NEXT:    csw a3, 12(csp)
; PURECAP-NEXT:    csw a2, 8(csp)
; PURECAP-NEXT:    cincoffset ca0, csp, 8
; PURECAP-NEXT:    csetbounds ca1, ca0, 8
; PURECAP-NEXT:    li a4, 5
; PURECAP-NEXT:    li a5, 5
; PURECAP-NEXT:    cmove ca0, ct0
; PURECAP-NEXT:    mv a2, a7
; PURECAP-NEXT:    mv a3, a6
; PURECAP-NEXT:    ccall __atomic_compare_exchange_8
; PURECAP-NEXT:    clw a1, 12(csp)
; PURECAP-NEXT:    clw a2, 8(csp)
; PURECAP-NEXT:    csw a1, 4(cs0)
; PURECAP-NEXT:    csw a2, 0(cs0)
; PURECAP-NEXT:    csb a0, 8(cs0)
; PURECAP-NEXT:    clc cra, 24(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    clc cs0, 16(csp) # 8-byte Folded Reload
; PURECAP-NEXT:    cincoffset csp, csp, 32
; PURECAP-NEXT:    cret
;
; HYBRID-LABEL: cmpxchg_strong:
; HYBRID:       # %bb.0:
; HYBRID-NEXT:    addi sp, sp, -16
; HYBRID-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; HYBRID-NEXT:    mv a6, a5
; HYBRID-NEXT:    mv a7, a4
; HYBRID-NEXT:    mv t0, a1
; HYBRID-NEXT:    mv s0, a0
; HYBRID-NEXT:    sw a3, 4(sp)
; HYBRID-NEXT:    sw a2, 0(sp)
; HYBRID-NEXT:    mv a1, sp
; HYBRID-NEXT:    li a4, 5
; HYBRID-NEXT:    li a5, 5
; HYBRID-NEXT:    mv a0, t0
; HYBRID-NEXT:    mv a2, a7
; HYBRID-NEXT:    mv a3, a6
; HYBRID-NEXT:    call __atomic_compare_exchange_8@plt
; HYBRID-NEXT:    lw a1, 4(sp)
; HYBRID-NEXT:    lw a2, 0(sp)
; HYBRID-NEXT:    sw a1, 4(s0)
; HYBRID-NEXT:    sw a2, 0(s0)
; HYBRID-NEXT:    sb a0, 8(s0)
; HYBRID-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; HYBRID-NEXT:    addi sp, sp, 16
; HYBRID-NEXT:    ret
;
; HYBRID-CAP-PTR-LABEL: cmpxchg_strong:
; HYBRID-CAP-PTR:       # %bb.0:
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, -16
; HYBRID-CAP-PTR-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; HYBRID-CAP-PTR-NEXT:    mv a6, a5
; HYBRID-CAP-PTR-NEXT:    mv a7, a4
; HYBRID-CAP-PTR-NEXT:    cmove ct0, ca1
; HYBRID-CAP-PTR-NEXT:    mv s0, a0
; HYBRID-CAP-PTR-NEXT:    sw a3, 4(sp)
; HYBRID-CAP-PTR-NEXT:    sw a2, 0(sp)
; HYBRID-CAP-PTR-NEXT:    mv a1, sp
; HYBRID-CAP-PTR-NEXT:    li a4, 5
; HYBRID-CAP-PTR-NEXT:    li a5, 5
; HYBRID-CAP-PTR-NEXT:    cmove ca0, ct0
; HYBRID-CAP-PTR-NEXT:    mv a2, a7
; HYBRID-CAP-PTR-NEXT:    mv a3, a6
; HYBRID-CAP-PTR-NEXT:    call __atomic_compare_exchange_8_c@plt
; HYBRID-CAP-PTR-NEXT:    lw a1, 4(sp)
; HYBRID-CAP-PTR-NEXT:    lw a2, 0(sp)
; HYBRID-CAP-PTR-NEXT:    sw a1, 4(s0)
; HYBRID-CAP-PTR-NEXT:    sw a2, 0(s0)
; HYBRID-CAP-PTR-NEXT:    sb a0, 8(s0)
; HYBRID-CAP-PTR-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; HYBRID-CAP-PTR-NEXT:    addi sp, sp, 16
; HYBRID-CAP-PTR-NEXT:    ret
; PURECAP-IR-LABEL: define {{[^@]+}}@cmpxchg_strong
; PURECAP-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[EXP:%.*]], i64 [[NEW:%.*]]) addrspace(200) #[[ATTR0]] {
; PURECAP-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8, addrspace(200)
; PURECAP-IR-NEXT:    call void @llvm.lifetime.start.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    store i64 [[EXP]], ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    [[TMP2:%.*]] = call zeroext i1 @__atomic_compare_exchange_8(ptr addrspace(200) [[PTR]], ptr addrspace(200) [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; PURECAP-IR-NEXT:    [[TMP3:%.*]] = load i64, ptr addrspace(200) [[TMP1]], align 8
; PURECAP-IR-NEXT:    call void @llvm.lifetime.end.p200(i64 8, ptr addrspace(200) [[TMP1]])
; PURECAP-IR-NEXT:    [[TMP4:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP3]], 0
; PURECAP-IR-NEXT:    [[TMP5:%.*]] = insertvalue { i64, i1 } [[TMP4]], i1 [[TMP2]], 1
; PURECAP-IR-NEXT:    ret { i64, i1 } [[TMP5]]
;
; HYBRID-IR-LABEL: define {{[^@]+}}@cmpxchg_strong
; HYBRID-IR-SAME: (ptr addrspace(200) [[PTR:%.*]], i64 [[EXP:%.*]], i64 [[NEW:%.*]]) #[[ATTR0]] {
; HYBRID-IR-NEXT:    [[TMP1:%.*]] = alloca i64, align 8
; HYBRID-IR-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    store i64 [[EXP]], ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    [[TMP2:%.*]] = call zeroext i1 @__atomic_compare_exchange_8_c(ptr addrspace(200) [[PTR]], ptr [[TMP1]], i64 [[NEW]], i32 5, i32 5)
; HYBRID-IR-NEXT:    [[TMP3:%.*]] = load i64, ptr [[TMP1]], align 8
; HYBRID-IR-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[TMP1]])
; HYBRID-IR-NEXT:    [[TMP4:%.*]] = insertvalue { i64, i1 } undef, i64 [[TMP3]], 0
; HYBRID-IR-NEXT:    [[TMP5:%.*]] = insertvalue { i64, i1 } [[TMP4]], i1 [[TMP2]], 1
; HYBRID-IR-NEXT:    ret { i64, i1 } [[TMP5]]
;
  %1 = cmpxchg ptr addrspace(200) %ptr, i64 %exp, i64 %new seq_cst seq_cst
  ret { i64, i1 } %1
}
