; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/cheri-intrinsics-folding-broken-module-regression.ll
; This used to create a broken function.
; FIXME: the getoffset+add sequence should be folded to an increment
; REQUIRES: mips-registered-target
; RUN: opt -mtriple=mips64 -mcpu=cheri128 -mattr=+cheri128 --relocation-model=pic -target-abi purecap -S -passes=instcombine %s -o - | FileCheck %s
; RUN: opt -mtriple=mips64 -mcpu=cheri128 -mattr=+cheri128 --relocation-model=pic -target-abi purecap -S '-passes=default<O3>' %s | llc -mtriple=mips64 -mcpu=cheri128 -mattr=+cheri128 --relocation-model=pic -target-abi purecap -O3 -o - | FileCheck %s --check-prefix ASM
target datalayout = "E-m:e-pf200:128:128:128:64-i8:8:32-i16:16:32-i64:64-n32:64-S128-A200-P200-G200"

@d = common addrspace(200) global i64 0, align 4
@e = common addrspace(200) global i8 addrspace(200)* null, align 32

; C Source code:
;int d;
;void* e;
;void g(int x, int y) {
;  e = (__uintcap_t)&d + x + y;
;}

define void @g(i64 %x, i64 %y) addrspace(200) nounwind {
; ASM-LABEL: g:
; ASM:       # %bb.0:
; ASM-NEXT:    lui $1, %pcrel_hi(_CHERI_CAPABILITY_TABLE_-8)
; ASM-NEXT:    daddiu $1, $1, %pcrel_lo(_CHERI_CAPABILITY_TABLE_-4)
; ASM-NEXT:    cgetpccincoffset $c1, $1
; ASM-NEXT:    daddu $2, $5, $4
; ASM-NEXT:    clcbi $c2, %captab20(d)($c1)
; ASM-NEXT:    clcbi $c1, %captab20(e)($c1)
; ASM-NEXT:    cgetoffset $1, $c2
; ASM-NEXT:    daddu $1, $2, $1
; ASM-NEXT:    csetoffset $c2, $c2, $1
; ASM-NEXT:    cjr $c17
; ASM-NEXT:    csc $c2, $zero, 0($c1)
; CHECK-LABEL: define {{[^@]+}}@g
; CHECK-SAME: (i64 [[X:%.*]], i64 [[Y:%.*]]) addrspace(200) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    [[TMP3:%.*]] = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* nonnull bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*))
; CHECK-NEXT:    [[ADD:%.*]] = add i64 [[TMP3]], [[X]]
; CHECK-NEXT:    [[ADD1:%.*]] = add i64 [[ADD]], [[Y]]
; CHECK-NEXT:    [[TMP11:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* nonnull bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*), i64 [[ADD1]])
; CHECK-NEXT:    store i8 addrspace(200)* [[TMP11]], i8 addrspace(200)* addrspace(200)* @e, align 32
; CHECK-NEXT:    ret void
;
  %x.addr = alloca i64, align 4, addrspace(200)
  %y.addr = alloca i64, align 4, addrspace(200)
  store i64 %x, i64 addrspace(200)* %x.addr, align 4
  store i64 %y, i64 addrspace(200)* %y.addr, align 4
  %tmp1 = load i64, i64 addrspace(200)* %x.addr, align 4
  %tmp2 = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* null, i64 %tmp1)
  %tmp3 = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*))
  %tmp4 = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* %tmp2)
  %add = add i64 %tmp3, %tmp4
  %tmp5 = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*), i64 %add)
  %tmp7 = load i64, i64 addrspace(200)* %y.addr, align 4
  %tmp8 = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* null, i64 %tmp7)
  %tmp9 = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* %tmp5)
  %tmp10 = call i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)* %tmp8)
  %add1 = add i64 %tmp9, %tmp10
  %tmp11 = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* %tmp5, i64 %add1)
  store i8 addrspace(200)* %tmp11, i8 addrspace(200)* addrspace(200)* @e, align 32
  ret void
}

; define void @g(i64 %x, i64 %y)  nounwind {
;   %tmp1 = tail call i8 addrspace(200)* @llvm.cheri.cap.offset.increment.i64(i8 addrspace(200)* bitcast (i64 addrspace(200)* @d to i8 addrspace(200)*), i64 %x)
;   %tmp3 = tail call i8 addrspace(200)* @llvm.cheri.cap.offset.increment.i64(i8 addrspace(200)* %tmp1, i64 %y)
;   store i8 addrspace(200)* %tmp3, i8 addrspace(200)* addrspace(200)* @e, align 32
;   ret void
; }
;
declare i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)*, i64) addrspace(200)
declare i8 addrspace(200)* @llvm.cheri.cap.offset.increment.i64(i8 addrspace(200)*, i64) addrspace(200)
declare i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)*) addrspace(200)
