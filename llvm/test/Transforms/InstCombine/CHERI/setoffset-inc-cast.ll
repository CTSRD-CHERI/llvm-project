; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --verbose
; RUN: opt -S -instcombine %s -o - | FileCheck %s
; RUNNOT: opt -S -instcombine -o - < %s | %riscv64_cheri_purecap_llc -o %t-ic.ll
; RUNNOT: %riscv64_cheri_purecap_llc < %s -o %t-no-ic.ll
; RUNNOT: diff -u %t-ic.ll %t-no-ic.ll
; Check that we handle folding of setoffset followed by bitcast+GEP sensibly
target datalayout = "E-m:e-pf200:128:128:128:64-i8:8:32-i16:16:32-i64:64-n32:64-S128-A200-P200-G200"

declare i64 @llvm.cheri.cap.offset.get.i64(i8 addrspace(200)*) nounwind
declare i64 @llvm.cheri.cap.address.get.i64(i8 addrspace(200)*) nounwind
declare i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)*, i64) nounwind
declare i8 addrspace(200)* @llvm.cheri.cap.address.set.i64(i8 addrspace(200)*, i64) nounwind

%struct.pluto = type { i8 addrspace(200)*, i8 addrspace(200)* }
; Function Attrs: noinline nounwind
@foo = addrspace(200) global %struct.pluto zeroinitializer

; Non-constant setoffset+GEP should not be folded:
define void @no_fold_nonconstant_offset_gep_zero(i8 addrspace(200)* %arg, i64 %offset) addrspace(200) nounwind {
; CHECK-LABEL: define {{[^@]+}}@no_fold_nonconstant_offset_gep_zero
; CHECK-SAME: (i8 addrspace(200)* [[ARG:%.*]], i64 [[OFFSET:%.*]]) addrspace(200)
; CHECK-NEXT:  bb:
; CHECK-NEXT:    [[TMP:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 [[OFFSET]])
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP]] to i8 addrspace(200)* addrspace(200)*
; CHECK-NEXT:    store i8 addrspace(200)* [[ARG]], i8 addrspace(200)* addrspace(200)* [[TMP5]], align 16
; CHECK-NEXT:    ret void
;
bb:
  %tmp = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 %offset)
  %tmp7 = bitcast i8 addrspace(200)* %tmp to %struct.pluto addrspace(200)*
  %tmp5 = getelementptr inbounds %struct.pluto, %struct.pluto addrspace(200)* %tmp7, i64 0, i32 0
  store i8 addrspace(200)* %arg, i8 addrspace(200)* addrspace(200)* %tmp5, align 16
  ret void
}

define void @no_fold_nonconstant_offset_gep_one(i8 addrspace(200)* %arg, i64 %offset) addrspace(200) nounwind {
; CHECK-LABEL: define {{[^@]+}}@no_fold_nonconstant_offset_gep_one
; CHECK-SAME: (i8 addrspace(200)* [[ARG:%.*]], i64 [[OFFSET:%.*]]) addrspace(200)
; CHECK-NEXT:  bb:
; CHECK-NEXT:    [[TMP:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 [[OFFSET]])
; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr inbounds i8, i8 addrspace(200)* [[TMP]], i64 16
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* [[TMP5]] to i8 addrspace(200)* addrspace(200)*
; CHECK-NEXT:    store i8 addrspace(200)* [[ARG]], i8 addrspace(200)* addrspace(200)* [[TMP0]], align 16
; CHECK-NEXT:    ret void
;
bb:
  %tmp = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 %offset)
  %tmp7 = bitcast i8 addrspace(200)* %tmp to %struct.pluto addrspace(200)*
  %tmp5 = getelementptr inbounds %struct.pluto, %struct.pluto addrspace(200)* %tmp7, i64 0, i32 1
  store i8 addrspace(200)* %arg, i8 addrspace(200)* addrspace(200)* %tmp5, align 16
  ret void
}

; Zero setoffset+GEP should not be folded (since the zero constant can use the NULL register and folding the offset is probably not beneficial)
define void @no_fold_zero_offset_gep_zero(i8 addrspace(200)* %arg) addrspace(200) nounwind {
; CHECK-LABEL: define {{[^@]+}}@no_fold_zero_offset_gep_zero
; CHECK-SAME: (i8 addrspace(200)* [[ARG:%.*]]) addrspace(200)
; CHECK-NEXT:  bb:
; CHECK-NEXT:    [[TMP:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 0)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP]] to i8 addrspace(200)* addrspace(200)*
; CHECK-NEXT:    store i8 addrspace(200)* [[ARG]], i8 addrspace(200)* addrspace(200)* [[TMP5]], align 16
; CHECK-NEXT:    ret void
;
bb:
  %tmp = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 0)
  %tmp7 = bitcast i8 addrspace(200)* %tmp to %struct.pluto addrspace(200)*
  %tmp5 = getelementptr inbounds %struct.pluto, %struct.pluto addrspace(200)* %tmp7, i64 0, i32 0
  store i8 addrspace(200)* %arg, i8 addrspace(200)* addrspace(200)* %tmp5, align 16
  ret void
}

define void @no_fold_zero_offset_gep_one(i8 addrspace(200)* %arg) addrspace(200) nounwind {
; CHECK-LABEL: define {{[^@]+}}@no_fold_zero_offset_gep_one
; CHECK-SAME: (i8 addrspace(200)* [[ARG:%.*]]) addrspace(200)
; CHECK-NEXT:  bb:
; CHECK-NEXT:    [[TMP:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 0)
; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr inbounds i8, i8 addrspace(200)* [[TMP]], i64 16
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i8 addrspace(200)* [[TMP5]] to i8 addrspace(200)* addrspace(200)*
; CHECK-NEXT:    store i8 addrspace(200)* [[ARG]], i8 addrspace(200)* addrspace(200)* [[TMP0]], align 16
; CHECK-NEXT:    ret void
;
bb:
  %tmp = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 0)
  %tmp7 = bitcast i8 addrspace(200)* %tmp to %struct.pluto addrspace(200)*
  %tmp5 = getelementptr inbounds %struct.pluto, %struct.pluto addrspace(200)* %tmp7, i64 0, i32 1
  store i8 addrspace(200)* %arg, i8 addrspace(200)* addrspace(200)* %tmp5, align 16
  ret void
}

; However non-zero constant setoffset + GEP should be folded since this could save one instruction
define void @fold_nonzero_offset_gep_zero(i8 addrspace(200)* %arg) addrspace(200) nounwind {
; CHECK-LABEL: define {{[^@]+}}@fold_nonzero_offset_gep_zero
; CHECK-SAME: (i8 addrspace(200)* [[ARG:%.*]]) addrspace(200)
; CHECK-NEXT:  bb:
; CHECK-NEXT:    [[TMP:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 16)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP]] to i8 addrspace(200)* addrspace(200)*
; CHECK-NEXT:    store i8 addrspace(200)* [[ARG]], i8 addrspace(200)* addrspace(200)* [[TMP5]], align 16
; CHECK-NEXT:    ret void
;
bb:
  %tmp = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 16)
  %tmp7 = bitcast i8 addrspace(200)* %tmp to %struct.pluto addrspace(200)*
  %tmp5 = getelementptr inbounds %struct.pluto, %struct.pluto addrspace(200)* %tmp7, i64 0, i32 0
  store i8 addrspace(200)* %arg, i8 addrspace(200)* addrspace(200)* %tmp5, align 16
  ret void
}

define void @fold_nonzero_offset_gep_one(i8 addrspace(200)* %arg) addrspace(200) nounwind {
; CHECK-LABEL: define {{[^@]+}}@fold_nonzero_offset_gep_one
; CHECK-SAME: (i8 addrspace(200)* [[ARG:%.*]]) addrspace(200)
; CHECK-NEXT:  bb:
; CHECK-NEXT:    [[TMP:%.*]] = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 32)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP]] to i8 addrspace(200)* addrspace(200)*
; CHECK-NEXT:    store i8 addrspace(200)* [[ARG]], i8 addrspace(200)* addrspace(200)* [[TMP5]], align 16
; CHECK-NEXT:    ret void
;
bb:
  %tmp = call i8 addrspace(200)* @llvm.cheri.cap.offset.set.i64(i8 addrspace(200)* bitcast (%struct.pluto addrspace(200)* @foo to i8 addrspace(200)*), i64 16)
  %tmp7 = bitcast i8 addrspace(200)* %tmp to %struct.pluto addrspace(200)*
  %tmp5 = getelementptr inbounds %struct.pluto, %struct.pluto addrspace(200)* %tmp7, i64 0, i32 1
  store i8 addrspace(200)* %arg, i8 addrspace(200)* addrspace(200)* %tmp5, align 16
  ret void
}

