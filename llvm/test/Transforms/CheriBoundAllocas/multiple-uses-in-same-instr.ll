; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes
; This IR (reduced from compiling libkscreen) resulted in an invalid PHI node being generated
; in the CheriBoundAllocas pass. We now ensure that multiple uses within the same instruction
; reuse the same intrinsic call to avoid this problem.
; RUN: opt -cheri-bound-allocas -S < %s | FileCheck %s
target datalayout = "e-m:e-pf200:128:128:128:64-p:64:64-i64:64-i128:128-n64-S128-A200-P200-G200"
target triple = "riscv64-unknown-freebsd13"

%class.QString = type { %struct.QTypedArrayData addrspace(200)* }
%struct.QTypedArrayData = type { %struct.QArrayData }
%struct.QArrayData = type { i32, i32, i32, i8 addrspace(200)* }

declare void @use_QString(%class.QString addrspace(200)* %arg)

define hidden void @multiple_uses_phi(i32 %arg) {
; CHECK-LABEL: define {{[^@]+}}@multiple_uses_phi
; CHECK-SAME: (i32 [[ARG:%.*]]) addrspace(200) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[REF_TMP107:%.*]] = alloca [[CLASS_QSTRING:%.*]], align 16, addrspace(200)
; CHECK-NEXT:    br label [[COND_TRUE:%.*]]
; CHECK:       cond.true:
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast [[CLASS_QSTRING]] addrspace(200)* [[REF_TMP107]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 16)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to [[CLASS_QSTRING]] addrspace(200)*
; CHECK-NEXT:    switch i32 [[ARG]], label [[COND_END_SINK_SPLIT:%.*]] [
; CHECK-NEXT:    i32 -1, label [[COND_END:%.*]]
; CHECK-NEXT:    i32 0, label [[COND_END]]
; CHECK-NEXT:    ]
; CHECK:       cond.end.sink.split:
; CHECK-NEXT:    br label [[COND_END]]
; CHECK:       cond.end:
; CHECK-NEXT:    [[REF_TMP109_SINK:%.*]] = phi [[CLASS_QSTRING]] addrspace(200)* [ [[TMP2]], [[COND_TRUE]] ], [ [[TMP2]], [[COND_TRUE]] ], [ null, [[COND_END_SINK_SPLIT]] ]
; CHECK-NEXT:    call void @use_QString([[CLASS_QSTRING]] addrspace(200)* [[REF_TMP109_SINK]])
; CHECK-NEXT:    ret void
;
entry:
  %ref.tmp107 = alloca %class.QString, align 16, addrspace(200)
  br label %cond.true

cond.true:
  switch i32 %arg, label %cond.end.sink.split [
  i32 -1, label %cond.end
  i32 0, label %cond.end
  ]

cond.end.sink.split:
  br label %cond.end

cond.end:
  %ref.tmp109.sink = phi %class.QString addrspace(200)* [ %ref.tmp107, %cond.true ], [ %ref.tmp107, %cond.true ], [ null, %cond.end.sink.split ]
  call void @use_QString(%class.QString addrspace(200)* %ref.tmp109.sink)
  ret void
}

declare void @use_two_i64s(i64 addrspace(200)* %arg1, i64 addrspace(200)* %arg2)

define hidden void @multiple_uses_call() {
; CHECK-LABEL: define {{[^@]+}}@multiple_uses_call() addrspace(200) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[FOO:%.*]] = alloca i64, align 8, addrspace(200)
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)* [[FOO]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 8)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i64 addrspace(200)*
; CHECK-NEXT:    call void @use_two_i64s(i64 addrspace(200)* [[TMP2]], i64 addrspace(200)* [[TMP2]])
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* [[FOO]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 8)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i64 addrspace(200)*
; CHECK-NEXT:    call void @use_two_i64s(i64 addrspace(200)* [[TMP5]], i64 addrspace(200)* [[TMP5]])
; CHECK-NEXT:    ret void
;
entry:
  %foo = alloca i64, align 8, addrspace(200)
  call void @use_two_i64s(i64 addrspace(200)* %foo, i64 addrspace(200)* %foo)
  ; This second call should insert another intrinsic call:
  call void @use_two_i64s(i64 addrspace(200)* %foo, i64 addrspace(200)* %foo)
  ret void
}

declare void @use_i32(i32 addrspace(200)*)

define hidden void @multiple_uses_different_blocks_phi(i32 %arg) {
; CHECK-LABEL: define {{[^@]+}}@multiple_uses_different_blocks_phi
; CHECK-SAME: (i32 [[ARG:%.*]]) addrspace(200) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[FOO:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    switch i32 [[ARG]], label [[COND_END:%.*]] [
; CHECK-NEXT:    i32 0, label [[COND_0:%.*]]
; CHECK-NEXT:    i32 1, label [[COND_1:%.*]]
; CHECK-NEXT:    ]
; CHECK:       cond.0:
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[FOO]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[COND_END]]
; CHECK:       cond.1:
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[FOO]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[COND_END]]
; CHECK:       cond.end:
; CHECK-NEXT:    [[PHI:%.*]] = phi i32 addrspace(200)* [ null, [[ENTRY:%.*]] ], [ [[TMP2]], [[COND_0]] ], [ [[TMP5]], [[COND_1]] ]
; CHECK-NEXT:    call void @use_i32(i32 addrspace(200)* [[PHI]])
; CHECK-NEXT:    ret void
;
entry:
  %foo = alloca i32, align 4, addrspace(200)
  switch i32 %arg, label %cond.end [
  i32 0, label %cond.0
  i32 1, label %cond.1
  ]

cond.0:
  br label %cond.end

cond.1:
  br label %cond.end

cond.end:
  ; Repeated uses for different incoming blocks must not share a use from one
  ; of them as it may not dominate the other (alternatively we could insert it
  ; in the dominator, but in general that could be expensive).
  %phi = phi i32 addrspace(200)* [ null, %entry ], [ %foo, %cond.0 ], [ %foo, %cond.1 ]
  call void @use_i32(i32 addrspace(200)* %phi)
  ret void
}
