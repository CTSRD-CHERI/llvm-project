; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes
; This IR (reduced from compiling libkscreen) resulted in an invalid PHI node being generated
; in the CheriBoundAllocas pass.
; PHI node has multiple entries for the same basic block with different incoming values!
; FIXME: remove -disable-verify+not --crash once fixed
; REQUIRES: asserts
; RUN: opt -cheri-bound-allocas -S -disable-verify < %s | FileCheck %s
; RUN: not --crash opt -cheri-bound-allocas -S < %s
target datalayout = "e-m:e-pf200:128:128:128:64-p:64:64-i64:64-i128:128-n64-S128-A200-P200-G200"
target triple = "riscv64-unknown-freebsd13"

%class.QString = type { %struct.QTypedArrayData addrspace(200)* }
%struct.QTypedArrayData = type { %struct.QArrayData }
%struct.QArrayData = type { i32, i32, i32, i8 addrspace(200)* }

declare void @use_QString(%class.QString addrspace(200)* %arg)

define hidden void @multiple_uses_phi(i32 %arg) {
; CHECK-LABEL: define {{[^@]+}}@multiple_uses_phi
; CHECK-SAME: (i32 [[ARG:%.*]]) addrspace(200) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[REF_TMP107:%.*]] = alloca [[CLASS_QSTRING:%.*]], align 16, addrspace(200)
; CHECK-NEXT:    br label [[COND_TRUE:%.*]]
; CHECK:       cond.true:
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast [[CLASS_QSTRING]] addrspace(200)* [[REF_TMP107]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 16)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to [[CLASS_QSTRING]] addrspace(200)*
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast [[CLASS_QSTRING]] addrspace(200)* [[REF_TMP107]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 16)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to [[CLASS_QSTRING]] addrspace(200)*
; CHECK-NEXT:    switch i32 [[ARG]], label [[COND_END_SINK_SPLIT:%.*]] [
; CHECK-NEXT:    i32 -1, label [[COND_END:%.*]]
; CHECK-NEXT:    i32 0, label [[COND_END]]
; CHECK-NEXT:    ]
; CHECK:       cond.end.sink.split:
; CHECK-NEXT:    br label [[COND_END]]
; CHECK:       cond.end:
; CHECK-NEXT:    [[REF_TMP109_SINK:%.*]] = phi [[CLASS_QSTRING]] addrspace(200)* [ [[TMP5]], [[COND_TRUE]] ], [ [[TMP2]], [[COND_TRUE]] ], [ null, [[COND_END_SINK_SPLIT]] ]
; CHECK-NEXT:    call void @use_QString([[CLASS_QSTRING]] addrspace(200)* [[REF_TMP109_SINK]])
; CHECK-NEXT:    ret void
;
entry:
  %ref.tmp107 = alloca %class.QString, align 16, addrspace(200)
  br label %cond.true

cond.true:
  switch i32 %arg, label %cond.end.sink.split [
  i32 -1, label %cond.end
  i32 0, label %cond.end
  ]

cond.end.sink.split:
  br label %cond.end

cond.end:
  %ref.tmp109.sink = phi %class.QString addrspace(200)* [ %ref.tmp107, %cond.true ], [ %ref.tmp107, %cond.true ], [ null, %cond.end.sink.split ]
  call void @use_QString(%class.QString addrspace(200)* %ref.tmp109.sink)
  ret void
}

declare void @use_two_i64s(i64 addrspace(200)* %arg1, i64 addrspace(200)* %arg2)

define hidden void @multiple_uses_call() {
; CHECK-LABEL: define {{[^@]+}}@multiple_uses_call() addrspace(200) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[FOO:%.*]] = alloca i64, align 8, addrspace(200)
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 addrspace(200)* [[FOO]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 8)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i64 addrspace(200)*
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i64 addrspace(200)* [[FOO]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 8)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i64 addrspace(200)*
; CHECK-NEXT:    call void @use_two_i64s(i64 addrspace(200)* [[TMP5]], i64 addrspace(200)* [[TMP2]])
; CHECK-NEXT:    [[TMP6:%.*]] = bitcast i64 addrspace(200)* [[FOO]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP7:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP6]], i64 8)
; CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 addrspace(200)* [[TMP7]] to i64 addrspace(200)*
; CHECK-NEXT:    [[TMP9:%.*]] = bitcast i64 addrspace(200)* [[FOO]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP10:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP9]], i64 8)
; CHECK-NEXT:    [[TMP11:%.*]] = bitcast i8 addrspace(200)* [[TMP10]] to i64 addrspace(200)*
; CHECK-NEXT:    call void @use_two_i64s(i64 addrspace(200)* [[TMP11]], i64 addrspace(200)* [[TMP8]])
; CHECK-NEXT:    ret void
;
entry:
  %foo = alloca i64, align 8, addrspace(200)
  call void @use_two_i64s(i64 addrspace(200)* %foo, i64 addrspace(200)* %foo)
  ; This second call should insert another intrinsic call:
  call void @use_two_i64s(i64 addrspace(200)* %foo, i64 addrspace(200)* %foo)
  ret void
}
