; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; The new CheriBoundedStackPseudo instruction lets us pretend that the incoffset+csetbounds
; is a single trivially rematerizable instruction so it can freely move it around to avoid stack spills.
; we were moving the allocation of the register that is only used later to the beginning

; REQUIRES: asserts
; RUN: %cheri_purecap_opt -cheri-bound-allocas %s -o - -S | FileCheck %s -check-prefix DEFAULT
; RUN: %cheri_purecap_opt -cheri-bound-allocas %s -o - -S -cheri-stack-bounds-single-intrinsic-threshold=0 -cheri-stack-bounds=if-needed | FileCheck %s -check-prefix IF-NEEDED-SINGLE
; RUN: %cheri_purecap_opt -cheri-bound-allocas %s -o - -S -cheri-stack-bounds-single-intrinsic-threshold=10 -cheri-stack-bounds=if-needed | FileCheck %s -check-prefix IF-NEEDED-PER-USE
; RUN: %cheri_purecap_opt -cheri-bound-allocas %s -o - -S -cheri-stack-bounds-single-intrinsic-threshold=10 -cheri-stack-bounds=all-or-none | FileCheck %s -check-prefix ALL-OR-NONE-PER-USE

target datalayout = "Eme-pf200:128:128:128:64-A200-P200-G200"

declare void @foo() addrspace(200)

declare void @bar(i32 addrspace(200)*) addrspace(200)

define void @foobar() addrspace(200) nounwind {
; DEFAULT-LABEL: @foobar(
; DEFAULT-NEXT:  entry:
; DEFAULT-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; DEFAULT-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; DEFAULT-NEXT:    call void @foo()
; DEFAULT-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; DEFAULT-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; DEFAULT-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; DEFAULT-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; DEFAULT-NEXT:    ret void
;
; IF-NEEDED-SINGLE-LABEL: @foobar(
; IF-NEEDED-SINGLE-NEXT:  entry:
; IF-NEEDED-SINGLE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-SINGLE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; IF-NEEDED-SINGLE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-SINGLE-NEXT:    call void @foo()
; IF-NEEDED-SINGLE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; IF-NEEDED-SINGLE-NEXT:    ret void
;
; IF-NEEDED-PER-USE-LABEL: @foobar(
; IF-NEEDED-PER-USE-NEXT:  entry:
; IF-NEEDED-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-PER-USE-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-PER-USE-NEXT:    call void @foo()
; IF-NEEDED-PER-USE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; IF-NEEDED-PER-USE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; IF-NEEDED-PER-USE-NEXT:    ret void
;
; ALL-OR-NONE-PER-USE-LABEL: @foobar(
; ALL-OR-NONE-PER-USE-NEXT:  entry:
; ALL-OR-NONE-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    store i32 123, i32 addrspace(200)* [[TMP2]], align 4
; ALL-OR-NONE-PER-USE-NEXT:    call void @foo()
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP5]])
; ALL-OR-NONE-PER-USE-NEXT:    ret void
;
; FOR-ALL-OR-NONE-LABEL: @foobar(
; FOR-ALL-OR-NONE-NEXT:  entry:
; FOR-ALL-OR-NONE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; FOR-ALL-OR-NONE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; FOR-ALL-OR-NONE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    store i32 123, i32 addrspace(200)* [[TMP2]], align 4
; FOR-ALL-OR-NONE-NEXT:    call void @foo()
; FOR-ALL-OR-NONE-NEXT:    [[TMP3:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; FOR-ALL-OR-NONE-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* [[TMP3]] to i32 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP4]])
; FOR-ALL-OR-NONE-NEXT:    ret void
entry:
  %x = alloca i32, align 4, addrspace(200)
  store i32 123, i32 addrspace(200)* %x, align 4
  call void @foo()
  call void @bar(i32 addrspace(200)* %x)
  ret void
}

define i32 @only_load_store() addrspace(200) nounwind {
; DEFAULT-LABEL: @only_load_store(
; DEFAULT-NEXT:  entry:
; DEFAULT-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; DEFAULT-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; DEFAULT-NEXT:    call void @foo()
; DEFAULT-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[X]], align 4
; DEFAULT-NEXT:    ret i32 [[RET]]
;
; IF-NEEDED-SINGLE-LABEL: @only_load_store(
; IF-NEEDED-SINGLE-NEXT:  entry:
; IF-NEEDED-SINGLE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-SINGLE-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-SINGLE-NEXT:    call void @foo()
; IF-NEEDED-SINGLE-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-SINGLE-NEXT:    ret i32 [[RET]]
;
; IF-NEEDED-PER-USE-LABEL: @only_load_store(
; IF-NEEDED-PER-USE-NEXT:  entry:
; IF-NEEDED-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-PER-USE-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-PER-USE-NEXT:    call void @foo()
; IF-NEEDED-PER-USE-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-PER-USE-NEXT:    ret i32 [[RET]]
;
; ALL-OR-NONE-PER-USE-LABEL: @only_load_store(
; ALL-OR-NONE-PER-USE-NEXT:  entry:
; ALL-OR-NONE-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; ALL-OR-NONE-PER-USE-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; ALL-OR-NONE-PER-USE-NEXT:    call void @foo()
; ALL-OR-NONE-PER-USE-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[X]], align 4
; ALL-OR-NONE-PER-USE-NEXT:    ret i32 [[RET]]
;
; FOR-ALL-OR-NONE-LABEL: @only_load_store(
; FOR-ALL-OR-NONE-NEXT:  entry:
; FOR-ALL-OR-NONE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; FOR-ALL-OR-NONE-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; FOR-ALL-OR-NONE-NEXT:    call void @foo()
; FOR-ALL-OR-NONE-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[X]], align 4
; FOR-ALL-OR-NONE-NEXT:    ret i32 [[RET]]
entry:
  %x = alloca i32, align 4, addrspace(200)
  store i32 123, i32 addrspace(200)* %x, align 4
  call void @foo()
  %ret = load i32, i32 addrspace(200)* %x, align 4
  ret i32 %ret
}

define i32 @load_store_and_call() addrspace(200) nounwind {
; DEFAULT-LABEL: @load_store_and_call(
; DEFAULT-NEXT:  entry:
; DEFAULT-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; DEFAULT-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; DEFAULT-NEXT:    call void @foo()
; DEFAULT-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; DEFAULT-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; DEFAULT-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; DEFAULT-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; DEFAULT-NEXT:    call void @foo()
; DEFAULT-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[X]], align 4
; DEFAULT-NEXT:    ret i32 [[RET]]
;
; IF-NEEDED-SINGLE-LABEL: @load_store_and_call(
; IF-NEEDED-SINGLE-NEXT:  entry:
; IF-NEEDED-SINGLE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-SINGLE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; IF-NEEDED-SINGLE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-SINGLE-NEXT:    call void @foo()
; IF-NEEDED-SINGLE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; IF-NEEDED-SINGLE-NEXT:    call void @foo()
; IF-NEEDED-SINGLE-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-SINGLE-NEXT:    ret i32 [[RET]]
;
; IF-NEEDED-PER-USE-LABEL: @load_store_and_call(
; IF-NEEDED-PER-USE-NEXT:  entry:
; IF-NEEDED-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-PER-USE-NEXT:    store i32 123, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-PER-USE-NEXT:    call void @foo()
; IF-NEEDED-PER-USE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; IF-NEEDED-PER-USE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; IF-NEEDED-PER-USE-NEXT:    call void @foo()
; IF-NEEDED-PER-USE-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[X]], align 4
; IF-NEEDED-PER-USE-NEXT:    ret i32 [[RET]]
;
; ALL-OR-NONE-PER-USE-LABEL: @load_store_and_call(
; ALL-OR-NONE-PER-USE-NEXT:  entry:
; ALL-OR-NONE-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    store i32 123, i32 addrspace(200)* [[TMP2]], align 4
; ALL-OR-NONE-PER-USE-NEXT:    call void @foo()
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP5]])
; ALL-OR-NONE-PER-USE-NEXT:    call void @foo()
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP6:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP7:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP6]], i64 4)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP8:%.*]] = bitcast i8 addrspace(200)* [[TMP7]] to i32 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[TMP8]], align 4
; ALL-OR-NONE-PER-USE-NEXT:    ret i32 [[RET]]
;
; FOR-ALL-OR-NONE-LABEL: @load_store_and_call(
; FOR-ALL-OR-NONE-NEXT:  entry:
; FOR-ALL-OR-NONE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; FOR-ALL-OR-NONE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; FOR-ALL-OR-NONE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    store i32 123, i32 addrspace(200)* [[TMP2]], align 4
; FOR-ALL-OR-NONE-NEXT:    call void @foo()
; FOR-ALL-OR-NONE-NEXT:    [[TMP3:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; FOR-ALL-OR-NONE-NEXT:    [[TMP4:%.*]] = bitcast i8 addrspace(200)* [[TMP3]] to i32 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP4]])
; FOR-ALL-OR-NONE-NEXT:    call void @foo()
; FOR-ALL-OR-NONE-NEXT:    [[TMP5:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; FOR-ALL-OR-NONE-NEXT:    [[TMP6:%.*]] = bitcast i8 addrspace(200)* [[TMP5]] to i32 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    [[RET:%.*]] = load i32, i32 addrspace(200)* [[TMP6]], align 4
; FOR-ALL-OR-NONE-NEXT:    ret i32 [[RET]]
entry:
  %x = alloca i32, align 4, addrspace(200)
  store i32 123, i32 addrspace(200)* %x, align 4
  call void @foo()
  call void @bar(i32 addrspace(200)* %x)
  call void @foo()
  %ret = load i32, i32 addrspace(200)* %x, align 4
  ret i32 %ret
}


define void @foobar_without_store() addrspace(200) nounwind {
; DEFAULT-LABEL: @foobar_without_store(
; DEFAULT-NEXT:  entry:
; DEFAULT-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; DEFAULT-NEXT:    [[Y:%.*]] = alloca i32, align 4, addrspace(200)
; DEFAULT-NEXT:    call void @foo()
; DEFAULT-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; DEFAULT-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; DEFAULT-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; DEFAULT-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; DEFAULT-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[Y]] to i8 addrspace(200)*
; DEFAULT-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; DEFAULT-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; DEFAULT-NEXT:    call void @bar(i32 addrspace(200)* [[TMP5]])
; DEFAULT-NEXT:    ret void
;
; IF-NEEDED-SINGLE-LABEL: @foobar_without_store(
; IF-NEEDED-SINGLE-NEXT:  entry:
; IF-NEEDED-SINGLE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-SINGLE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; IF-NEEDED-SINGLE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    [[Y:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-SINGLE-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[Y]] to i8 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; IF-NEEDED-SINGLE-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    call void @foo()
; IF-NEEDED-SINGLE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; IF-NEEDED-SINGLE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP5]])
; IF-NEEDED-SINGLE-NEXT:    ret void
;
; IF-NEEDED-PER-USE-LABEL: @foobar_without_store(
; IF-NEEDED-PER-USE-NEXT:  entry:
; IF-NEEDED-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-PER-USE-NEXT:    [[Y:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-PER-USE-NEXT:    call void @foo()
; IF-NEEDED-PER-USE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; IF-NEEDED-PER-USE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; IF-NEEDED-PER-USE-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[Y]] to i8 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; IF-NEEDED-PER-USE-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP5]])
; IF-NEEDED-PER-USE-NEXT:    ret void
;
; ALL-OR-NONE-PER-USE-LABEL: @foobar_without_store(
; ALL-OR-NONE-PER-USE-NEXT:  entry:
; ALL-OR-NONE-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; ALL-OR-NONE-PER-USE-NEXT:    [[Y:%.*]] = alloca i32, align 4, addrspace(200)
; ALL-OR-NONE-PER-USE-NEXT:    call void @foo()
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP2]])
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[Y]] to i8 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP5]])
; ALL-OR-NONE-PER-USE-NEXT:    ret void
;
; FOR-ALL-OR-NONE-LABEL: @foobar_without_store(
; FOR-ALL-OR-NONE-NEXT:  entry:
; FOR-ALL-OR-NONE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; FOR-ALL-OR-NONE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    [[Y:%.*]] = alloca i32, align 4, addrspace(200)
; FOR-ALL-OR-NONE-NEXT:    [[TMP1:%.*]] = bitcast i32 addrspace(200)* [[Y]] to i8 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    call void @foo()
; FOR-ALL-OR-NONE-NEXT:    [[TMP2:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; FOR-ALL-OR-NONE-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[TMP2]] to i32 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP3]])
; FOR-ALL-OR-NONE-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP1]], i64 4)
; FOR-ALL-OR-NONE-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    call void @bar(i32 addrspace(200)* [[TMP5]])
; FOR-ALL-OR-NONE-NEXT:    ret void
entry:
  %x = alloca i32, align 4, addrspace(200)
  %y = alloca i32, align 4, addrspace(200)
  call void @foo()
  call void @bar(i32 addrspace(200)* %x)
  call void @bar(i32 addrspace(200)* %y)
  ret void
}

define i8 addrspace(200)* @return_alloca() addrspace(200) nounwind {
; DEFAULT-LABEL: @return_alloca(
; DEFAULT-NEXT:  entry:
; DEFAULT-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; DEFAULT-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; DEFAULT-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; DEFAULT-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; DEFAULT-NEXT:    [[I8:%.*]] = bitcast i32 addrspace(200)* [[TMP2]] to i8 addrspace(200)*
; DEFAULT-NEXT:    ret i8 addrspace(200)* [[I8]]
;
; IF-NEEDED-SINGLE-LABEL: @return_alloca(
; IF-NEEDED-SINGLE-NEXT:  entry:
; IF-NEEDED-SINGLE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-SINGLE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; IF-NEEDED-SINGLE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    [[I8:%.*]] = bitcast i32 addrspace(200)* [[TMP2]] to i8 addrspace(200)*
; IF-NEEDED-SINGLE-NEXT:    ret i8 addrspace(200)* [[I8]]
;
; IF-NEEDED-PER-USE-LABEL: @return_alloca(
; IF-NEEDED-PER-USE-NEXT:  entry:
; IF-NEEDED-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; IF-NEEDED-PER-USE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; IF-NEEDED-PER-USE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    [[I8:%.*]] = bitcast i32 addrspace(200)* [[TMP2]] to i8 addrspace(200)*
; IF-NEEDED-PER-USE-NEXT:    ret i8 addrspace(200)* [[I8]]
;
; ALL-OR-NONE-PER-USE-LABEL: @return_alloca(
; ALL-OR-NONE-PER-USE-NEXT:  entry:
; ALL-OR-NONE-PER-USE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; ALL-OR-NONE-PER-USE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    [[I8:%.*]] = bitcast i32 addrspace(200)* [[TMP2]] to i8 addrspace(200)*
; ALL-OR-NONE-PER-USE-NEXT:    ret i8 addrspace(200)* [[I8]]
;
; FOR-ALL-OR-NONE-LABEL: @return_alloca(
; FOR-ALL-OR-NONE-NEXT:  entry:
; FOR-ALL-OR-NONE-NEXT:    [[X:%.*]] = alloca i32, align 4, addrspace(200)
; FOR-ALL-OR-NONE-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[X]] to i8 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; FOR-ALL-OR-NONE-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    [[I8:%.*]] = bitcast i32 addrspace(200)* [[TMP2]] to i8 addrspace(200)*
; FOR-ALL-OR-NONE-NEXT:    ret i8 addrspace(200)* [[I8]]
entry:
  %x = alloca i32, align 4, addrspace(200)
  %i8 = bitcast i32 addrspace(200)* %x to i8 addrspace(200)*
  ret i8 addrspace(200)* %i8
}
